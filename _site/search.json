[
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html",
    "title": "Take-Home Exercise 01",
    "section": "",
    "text": "City of Engagement, with a total population of 50,000, is a small city located at Country of Nowhere. The city serves as a service centre of an agriculture region surrounding the city. The main agriculture of the region is fruit farms and vineyards. The local council of the city is in the process of preparing the Local Plan 2023. A sample survey of 1000 representative residents had been conducted to collect data related to their household demographic and spending patterns, among other things. The city aims to use the data to assist with their major community revitalization efforts, including how to allocate a very large city renewal grant they have recently received.\n\n\nIn this Take-Home Exercise, we will reveal the demographic and financial characteristics of the city of Engagement by using appropriate static and interactive statistical graphics methods, to help the city managers and planners to explore the complex data in an engaging way and reveal hidden patterns.\n\n\n\nTwo (2) datasets will be used, as inputs:\n\nParticipants.csv – information about the residents of City of Engagement who agreed to participate in this study.\nFinancialJournal.csv – information about the financial transactions of the participants from March 2022 to February 2023."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#load-participants.csv-and-and-perform-data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#load-participants.csv-and-and-perform-data-wrangling",
    "title": "Take-Home Exercise 01",
    "section": "4.1. Load Participants.csv and and Perform Data Wrangling",
    "text": "4.1. Load Participants.csv and and Perform Data Wrangling\nAs part of data wrangling, we will check the data set for duplicate records and remove them (if there is any), then we will convert the data into the respective data types (string factor, integer, boolean, double), for easier analysis.\n\n\nCode\n# Load the Participants.csv into the environment\nparticipants_raw &lt;- read_csv(\"data/Participants.csv\")\n\n# Check if 'participants' contains any duplicate rows\nhas_duplicates &lt;- any(duplicated(participants_raw))\n\nif (has_duplicates) {\n  #print(\"The participants data frame contains duplicate rows.\")\n  \n  # extracting only the unique participants records\n  participants &lt;- distinct(participants_raw)\n} else {\n  #print(\"The participants data frame does not contain duplicate rows.\")\n  # if there are no duplicate rows, we will just replicate participants data for easy merging later\n  participants &lt;- participants_raw\n}\n\n# Convert participantId, educationLevel columns into string factor\nparticipants$participantId &lt;- as.factor(as.character(participants$participantId))\nparticipants$educationLevel &lt;- as.factor(as.character(participants$educationLevel))\n\n# Convert householdSize and age into whole numbers, i.e. integers\nparticipants$householdSize &lt;- as.integer(participants$householdSize)\nparticipants$age &lt;- as.integer(participants$age)\n\n# Bin the age column into 5 groupings \nparticipants &lt;- participants %&gt;% mutate(age_bin = cut(age, breaks=c(0, 30, 40, 50, 60)))\n\n# Convert haveKids column into a logical (boolean) data type\nparticipants$haveKids &lt;- as.logical(participants$haveKids)\n\n# Convert joviality column into a double data type\nparticipants$joviality &lt;- as.double(participants$joviality)\n\n# Bin the joviality column into 3 broad categories\nparticipants &lt;- participants %&gt;% mutate(jov_bin = cut(joviality, breaks=c(0, 0.2, 0.4, 0.6, 0.8, 1.0)))\n\n\n\n4.2. Load FinancialJournal.csv and Perform Data Wrangling\nAs part of data wrangling, we will check the data set for duplicate records and remove them (if there is any), then we will convert the data into the respective data types (string factor, integer, boolean, double), for easier analysis.\nWe will also be extracting the year and month from the timestamp column and transpose the values in the category column into individual columns.\n\n\nCode\n# Load the FinancialJournal.csv into the environment\nfinjournal_raw &lt;- read_csv(\"data/FinancialJournal.csv\")\n\n# Check if 'finjournal' contains any duplicate rows\nhas_duplicates &lt;- any(duplicated(finjournal_raw))\n\nif (has_duplicates) {\n  print(\"The finjournal_raw data frame contains duplicate rows.\")\n  \n  # Remove duplicate records from the 'finjournal_raw' data frame\n  finjournal &lt;- distinct(finjournal_raw)\n} else {\n  print(\"The finjournal_raw data frame does not contain duplicate rows.\")\n  # if there are no duplicate rows, we will just replicate finjournal_raw data for easy merging \n  finjournal &lt;- finjournal_raw\n}\n\n\n[1] \"The finjournal_raw data frame contains duplicate rows.\"\n\n\nCode\n# Convert participantId, category columns into string factor \nfinjournal$participantId &lt;- as.factor(as.character(finjournal$participantId))\nfinjournal$category &lt;- as.factor(as.character(finjournal$category))\n\n# Extract the date component from timestamp column\nfinjournal$date &lt;- as.Date(finjournal$timestamp)\n\n# Extract the month and year from the date component\nfinjournal$mthYear &lt;- format(finjournal$date, \"%m-%Y\")\n\n# Convert the negative amount values to absolute values and round to 2 decimal places\nfinjournal$amount &lt;- round(abs(finjournal$amount), 2)\n\n\n\n4.2.1. Checking the quality of financial journal data\nThere are often dirty data that we will need to cleanse before performing any data analysis. This portion will check on the quality of the financial journal data.\n\n4.2.1.1. Identifying any possible outliers by retrieving the total number of transactions for each participant\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\n\nfinjournal_grp &lt;- finjournal %&gt;%\n  group_by(participantId) %&gt;%\n  summarise(transaction_cnt = n()) %&gt;%\n  arrange(transaction_cnt)\n\n# Plotting a histogram with the data extracted for better visualisation\nplot &lt;- ggplot(data = finjournal_grp, \n               aes(x = transaction_cnt)) +\n  geom_histogram(color=\"gray30\", fill=\"deepskyblue3\") + \n  ylim(0, 150) +\n  ggtitle(\"Distribution of Transactions by Participants\") + \n  xlab(\"Transaction Count\") +\n  ylab(\"Number of Participants\") + \n  theme_light()\n\nplot &lt;- ggplotly(plot, tooltip = c(\"y\"))\n\nplot\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs you noticed from the chart above, there is a number of participants (131 of them in total), who have a significantly low number of transaction records, which is far off from the main distribution. As such, we will treat these as outliers and remove them from our analysis.\n\n\n\n\n\n4.2.1.2. Removing the outliers\nWe will proceed to remove the 131 participants who have significantly low number of transaction records, compared to the rest.\n\n\nCode\n# Get the transaction count for the last row of the 131 outliers \ntxn_count &lt;- finjournal_grp[131, \"transaction_cnt\"]\n\n# Extract the participants without the 131 participants \nfinjournal_grp_filtered &lt;- finjournal_grp %&gt;%\n  filter(transaction_cnt &gt; as.integer(txn_count))\n\n# Remove the 131 participants from the finjournal\nfinjournal_filtered &lt;- merge(finjournal, finjournal_grp_filtered, by = \"participantId\")\n\n\n\n\n4.2.1.3. Transposing the category of transaction into indvidual columns for easy analysis\nIn this section, we will be performing a series of data wrangling activities for each participant:\n\nsum up all the daily transactions amount based on the respective categories\ntranspose the transaction categories into multiple columns (Education, Food, Recreation, Shelter, Wage, RentAdjustment)\nreplace all the NA values to 0\nderive total monthly expenses and total monthly earnings\nderive monthly savings (earnings - expenses)\n\n\n\nCode\n# sum up all the daily transactions amount based on the respective categories\ntemp_finjournal &lt;- finjournal_filtered %&gt;% \n  group_by(participantId, category, mthYear) %&gt;% \n  summarise(Total = sum(amount))\n\n# Transpose the categories to columns\ntemp_finjournal &lt;- pivot_wider(temp_finjournal, \n                               names_from = category, \n                               values_from = Total)\n\n# Replace all the NA values to 0\ntemp_finjournal[is.na(temp_finjournal)] &lt;- 0.0\n\n# Derive the total monthly expenses and total monthly earnings for each participant \ntemp_finjournal_sum &lt;- temp_finjournal %&gt;%\n  group_by(participantId, mthYear) %&gt;%\n  summarize(sum_expense = sum(Education, Food,\n                              Recreation, Shelter),\n            sum_earning = sum(Wage, RentAdjustment))\n\n# Derive the monthly savings for each participant\n\ntemp_finjournal_sum$saving &lt;- temp_finjournal_sum$sum_earning -\n                              temp_finjournal_sum$sum_expense\n\n\nfinal_finjournal &lt;- merge(temp_finjournal, temp_finjournal_sum, by = \"participantId\")\n\n# Removing the repeated \"mthYear.y\" column and rename \"mthYear.x\" column to \"mthYear\"\nfinal_finjournal &lt;- select(final_finjournal, -c(mthYear.y))\n\nnames(final_finjournal)[2] &lt;- \"mthYear\"\n\n\n\n\nCode\n### Not sure if want to do this way for analysis. KIV KIV KIV\n### KIV \n\n#final_finjournal_wide &lt;- final_finjournal %&gt;%\n#  pivot_wider(names_from = mthYear, \n#              values_from = c(Education, Food, Recreation, Shelter, Wage, RentAdjustment))\n\n# Replace all the 'NA' values to 0.0\n#final_finjournal_wide[is.na(final_finjournal_wide)] &lt;- 0.0\n\n\n\n\n\n4.3 Combine the 2 data sets\nWe will combine the finalised data sets of Participants and FinancialJournal into 1 single data frame for ease of analysis.\n\n\nCode\n# Merge the two files based on the 'participantId' column\n#merged_data &lt;- merge(participants, final_finjournal_wide, by = \"participantId\")   #will need this we decide to revise the earlier part with final_finjournal_wide\nmerged_data &lt;- merge(participants, final_finjournal, by = \"participantId\")\n\n# Write the merged data to a new CSV file -&gt; This might not be necessary once we have checked the data\n#write_csv(merged_data, \"data/combined.csv\", na=\"\", append=FALSE, col_names=TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 02",
    "section": "",
    "text": "In this exercise, beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\n\nCode\npacman::p_load(tidyverse, ggrepel, patchwork, ggthemes, hrbrthemes)\n\n\n\n\n\nIt consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nThere are a total of seven attributes in the exam_data tibble data frame. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE.\n\n\n\nCode\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "title": "Hands-on Exercise 02",
    "section": "",
    "text": "In this exercise, beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\n\nCode\npacman::p_load(tidyverse, ggrepel, patchwork, ggthemes, hrbrthemes)\n\n\n\n\n\nIt consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nThere are a total of seven attributes in the exam_data tibble data frame. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE.\n\n\n\nCode\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "title": "Hands-on Exercise 02",
    "section": "2. Beyond ggplot2 Annotation: ggrepel",
    "text": "2. Beyond ggplot2 Annotation: ggrepel\nOne of the challenge in plotting statistical graph is annotation, especially with large number of data points.\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method=lm, \n              linewidth=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n2.1. Working with ggrepel\nggrepel is an extension of ggplot2 package which provides geoms for ggplot2 to repel overlapping text as in the above example.\nTo do so, we simply replace geom_text() by geom_text_repel() and geom_label() by geom_label_repel.\nNote: We will get the warning message: “ggrepel: 318 unlabeled data points (too many overlaps). Consider increasing max.overlaps” when we run the code snippet. However, in order to eliminate the warning message, we will need to set max.overlaps=Inf. This, however, makes the entire chart cluttered with labels (which seemed worse that the original chart above, without using ggrepel).\nAlternatively, we can just set warning=FALSE to suppress the warning message and let ggrepel does its work of suggesting the ‘best’ number of labels.\nTo show the comparison, the left tab code snippet will present the warning message, while the right tab code snippet will show the chart is cluttered with labels when we set max.overlaps=Inf.\n\nOriginal Codes (without setting max.overlaps=Inf)Revised Codes (with max.overlaps=Inf)\n\n\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method=lm, \n              linewidth=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method=lm, \n              linewidth=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\", max.overlaps=Inf) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "title": "Hands-on Exercise 02",
    "section": "3. Beyond ggplot2 Themes",
    "text": "3. Beyond ggplot2 Themes\nggplot2 comes with eight built-in themes, they are: theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\nRefer to this link to learn more about ggplot2 Themes\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n3.1. Working with ggtheme package\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\nIn the example below, The Economist theme is used.\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\nIt also provides some extra geoms and scales for ‘ggplot2’. Consult this vignette to learn more.\n\n\n3.2. Working with hrbthemes package\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\n\nCode\n#pacman::p_load(extrafont)\n#font_import()\n#loadfonts(device = \"win\")\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\nThe second goal centers around productivity for a production workflow. In fact, this “production workflow” is the context for where the elements of hrbrthemes should be used. Consult this vignette to learn more.\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\nFrom the example above, we learnt that\n\naxis_title_size argument is used to increase the font size of the axis title to 18\nbase_size argument is used to increase the default axis label to 15, and\ngrid argument is used to remove the x-axis grid lines."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-single-graph",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-single-graph",
    "title": "Hands-on Exercise 02",
    "section": "4. Beyond Single Graph",
    "text": "4. Beyond Single Graph\nIt is not unusual that multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions provide functions to compose figure with multiple graphs. In this section, we will create composite plot by combining multiple graphs. First, let us create three statistical graphics by using the code chunk below.\nFirstly, the histogram for MATHS scores\n\n\nCode\n p1 &lt;- ggplot(data=exam_data,\n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\np1\n\n\n\n\n\nNext, the histogram for ENGLISH scores\n\n\nCode\np2 &lt;- ggplot(data=exam_data,\n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np2\n\n\n\n\n\nLastly, we will draw a scatterplot for English score versus Maths score by as shown below\n\n\nCode\np3 &lt;- ggplot(data=exam_data,\n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method=lm, \n              linewidth=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\np3\n\n\n\n\n\n\n4.1. Creating Composite Graphics: pathwork methods\nThere are several ggplot2 extension’s functions support the needs to prepare composite figure by combining several graphs such as grid.arrange() of gridExtra package and plot_grid() of cowplot package.\nIn this section, We will use a ggplot2 extension called patchwork which is specially designed for combining separate ggplot2 graphs into a single figure.\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\n\n\n4.2. Combining 2 ggplot2 graphs\nFigure below shows a composite of two histograms created using patchwork. Note how simple the syntax used to create the plot!\n\n\nCode\np1 + p2\n\n\n\n\n\n\n\n4.3. Combining 3 ggplot2 graphs\nWe can plot more complex composite by using appropriate operators. For example, the composite figure below is plotted by using:\n\n“|” operator to stack two ggplot2 graphs,\n“/” operator to place the plots beside each other,\n“()” operator the define the sequence of the plotting.\n\n\n\nCode\n(p1 / p2) | p3\n\n\n\n\n\nTo learn more about, refer to Plot Assembly.\n\n\n4.4. Creating a composite figure with tag\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown in the figure below. For tagging,\n\n‘1’ for Arabic numerals,\n‘A’ for uppercase Latin letters,\n‘a’ for lowercase Latin letters,\n‘I’ for uppercase Roman numerals, and\n‘i’ for lowercase Roman numerals.\n\n\n\nCode\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\n4.5. Creating figure with insert\nBeside providing functions to place plots next to each other based on the provided layout. With inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\n\nCode\np12 &lt;- p1|p2\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\n4.6. Creating a composite figure by using patchwork and ggtheme\nFigure below is created by combining patchwork and theme_economist() of ggthemes package discussed earlier.\n\n\nCode\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html",
    "title": "Hands-on Exercise 03a - Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "In this exercise, beside tidyverse, four R packages will be used. They are:\n\nggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\npatchwork for combining multiple ggplot2 graphs into one figure.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\n\nCode\npacman::p_load(ggiraph, plotly, DT, patchwork, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#getting-started",
    "title": "Hands-on Exercise 03a - Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "In this exercise, beside tidyverse, four R packages will be used. They are:\n\nggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\npatchwork for combining multiple ggplot2 graphs into one figure.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\n\nCode\npacman::p_load(ggiraph, plotly, DT, patchwork, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#importing-data",
    "title": "Hands-on Exercise 03a - Programming Interactive Data Visualisation with R",
    "section": "2. Importing Data",
    "text": "2. Importing Data\nIn this section, Exam_data.csv provided will be used. Using read_csv() of readr package, import Exam_data.csv into R.\n\n\nCode\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#interactive-data-visualisation---ggiraph-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#interactive-data-visualisation---ggiraph-methods",
    "title": "Hands-on Exercise 03a - Programming Interactive Data Visualisation with R",
    "section": "3. Interactive Data Visualisation - ggiraph methods",
    "text": "3. Interactive Data Visualisation - ggiraph methods\nggiraph is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\n\nTooltip: a column of data-sets that contain tool tips to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements.\n\nIf it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detail explanation.\n\n3.1. Tooltip effect with tooltip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package.\nNotice that the code chunk consists of two parts.\n\nFirst, an ggplot object will be created.\nNext, girafe() of ggiraph will be used to create an interactive svg object.\n\n\n\nCode\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\n\nNotice that two steps are involved.\n\nFirst, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph.\nThen, girafe() will be used to generate an svg object to be displayed on an html page.\n\nNOTE : By hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\n\n\n3.2. Displaying multiple information on tooltip\nWe can also customise the content of the tooltip by including a list object as shown in the code chunk below.\n\n\nCode\n# The first three lines of codes in the code chunk create a new field called tooltip.\n#  At the same time, it populates text in ID and CLASS fields into the newly created field. \nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\n# Next, this newly created field is used as tooltip field.\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\n\nNOTE : By hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed.\n\n\n3.3. Customising Tooltip Style\nExample below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\n\nCode\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)\n\n\n\n\n\n\nNotice that the background colour of the tooltip is black and the font colour is white and bold.\nRefer to Customizing girafe objects to learn more about how to customise ggiraph objects.\n\n\n3.4. Displaying statistics on tooltip\nIn this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip. Code chunk below shows an advanced way to customise tooltip.\n\n\nCode\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, linewidth = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\n3.5. Hover effect with data_id aesthetic\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\n\n\nCode\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)\n\n\n\n\n\n\nNOTE :\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\nNote that the default value of the hover css is hover_css = “fill:orange;”.\n\n\n\n3.6. Styling hover effect\nIn the example below, css codes are used to change the highlighting effect.\n\n\nCode\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)\n\n\n\n\n\n\nNOTE:\n\nElements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\nDifferent from previous example, in this example the ccs customisation request are encoded directly.\n\n\n\n3.7. Combining tooltip and hover effect\nThere are times that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\n\nCode\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)\n\n\n\n\n\n\nNOTE : Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\n\n3.8. Click effect with onclick\nAn example of onclick argument of ggiraph provides hotlink interactivity on the web.\n\n\nCode\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)\n\n\n\n\n\n\nNOTE :\n\nWeb document link with a data object will be displayed on the web browser upon mouse click.\nWARNING Note that click actions must be a string column in the dataset containing valid javascript instructions.\n\n\n\n3.9. Coordinated Multiple Views with ggiraph\nExample below shows coordinated multiple views methods has been implemented in the data visualisation below.\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point.\n\n\nCode\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n       )\n      )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#interactive-data-visualisation---plotly-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#interactive-data-visualisation---plotly-methods",
    "title": "Hands-on Exercise 03a - Programming Interactive Data Visualisation with R",
    "section": "4. Interactive Data Visualisation - plotly methods!",
    "text": "4. Interactive Data Visualisation - plotly methods!\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\n\n\n\n\n\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\n4.1. Creating an interactive scatter plot: plot_ly() method\nThe example below shows a basic interactive plot created by using plot_ly().\n\n\nCode\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n4.2. Working with visual variable: plot_ly() method\nThe example below uses the color argument to map to a qualitative visual variable (i.e. RACE).\nNOTE : Click on the colour symbol at the legend for filtering of the data by respective RACE.\n\n\nCode\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE,\n        type=\"scatter\")\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n4.3. Creating an interactive scatter plot: ggplotly() method\nThe example below plots an interactive scatter plot by using ggplotly().\nNOTE : Notice that the only extra line you need to include in the code chunk is ggplotly().\n\n\nCode\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\n\n4.4. Coordinated Multiple Views with plotly\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\nNOTE : Click on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\n\n\nCode\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\n\n\nThing to learn from the code chunk:\n\nhighlight_key() simply creates an object of class crosstalk::SharedData.\nVisit this link to learn more about crosstalk,"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#interactive-data-visualisation---crosstalk-methods",
    "title": "Hands-on Exercise 03a - Programming Interactive Data Visualisation with R",
    "section": "5. Interactive Data Visualisation - crosstalk methods!",
    "text": "5. Interactive Data Visualisation - crosstalk methods!\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n5.1. Interactive Data Table: DT package\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\n\nCode\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\n\n5.2. Linked brushing: crosstalk method\nExample below is used to implement the coordinated brushing shown above.\n\n\nCode\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)\n\n\nSetting the `off` event (i.e., 'plotly_deselect') to match the `on` event (i.e., 'plotly_selected'). You can change this default via the `highlight()` function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html",
    "title": "Hands-on Exercise 03b - Programming Animated Statistical Graphics with R",
    "section": "",
    "text": "When telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. In this hands-on exercise, you will learn how to create animated data visualisation by using gganimate and plotly r packages. At the same time, you will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\n\n\n\n\n\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#basic-concepts-of-animation",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#basic-concepts-of-animation",
    "title": "Hands-on Exercise 03b - Programming Animated Statistical Graphics with R",
    "section": "",
    "text": "When creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#terminology",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#terminology",
    "title": "Hands-on Exercise 03b - Programming Animated Statistical Graphics with R",
    "section": "",
    "text": "Before we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#loading-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#loading-the-r-packages",
    "title": "Hands-on Exercise 03b - Programming Animated Statistical Graphics with R",
    "section": "2.1. Loading the R packages",
    "text": "2.1. Loading the R packages\nIn this exercise, beside tidyverse, four R packages will be used. They are:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\n\nCode\npacman::p_load(readxl, gifski, gapminder, plotly, gganimate, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#importing-the-data",
    "title": "Hands-on Exercise 03b - Programming Animated Statistical Graphics with R",
    "section": "2.2. Importing the data",
    "text": "2.2. Importing the data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nImporting Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\n\nCode\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\nThings to learn :\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_each_() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#building-a-static-population-bubble-plot",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#building-a-static-population-bubble-plot",
    "title": "Hands-on Exercise 03b - Programming Animated Statistical Graphics with R",
    "section": "3.1. Building a static population bubble plot",
    "text": "3.1. Building a static population bubble plot\nIn the example below, the basic ggplot2 functions are used to create a static bubble plot.\n\n\nCode\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#building-the-animated-bubble-plot",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#building-the-animated-bubble-plot",
    "title": "Hands-on Exercise 03b - Programming Animated Statistical Graphics with R",
    "section": "3.2. Building the animated bubble plot",
    "text": "3.2. Building the animated bubble plot\nIn the example below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\n\nCode\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#building-an-animated-bubble-plot-ggplotly-method",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#building-an-animated-bubble-plot-ggplotly-method",
    "title": "Hands-on Exercise 03b - Programming Animated Statistical Graphics with R",
    "section": "4.1. Building an animated bubble plot: ggplotly() method",
    "text": "4.1. Building an animated bubble plot: ggplotly() method\nIn this example, we will create an animated bubble plot by using ggplotly() method.\n\n\nCode\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\nThings to learn :\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#building-an-animated-bubble-plot-plot_ly-method",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#building-an-animated-bubble-plot-plot_ly-method",
    "title": "Hands-on Exercise 03b - Programming Animated Statistical Graphics with R",
    "section": "4.2. Building an animated bubble plot: plot_ly() method",
    "text": "4.2. Building an animated bubble plot: plot_ly() method\nIn this example, we will create an animated bubble plot by using plot_ly() method.\n\n\nCode\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent, \n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          )\nbp"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html",
    "title": "Hands-on Exercise 04a - Visual Statistical Analysis",
    "section": "",
    "text": "In this exercise using Exam_data, we will be using ggstatsplot and tidyverse. rstantools and PMCMRplus are also be required for plotting the ONEWAY ANOVA graph.\n\n\nCode\npacman::p_load(ggstatsplot, tidyverse, rstantools)\n\n\n\n\n\nFor this exercise, Exam_data.csv provided will be imported into R by using read_csv() of readr package.\n\n\nCode\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores, with default information: - statistical details - Bayes Factor - sample sizes - distribution summary.\n\n\nCode\nset.seed(1234)\n\ngghistostats(\n  data = exam_data,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\n\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\n\n\n\n\n\n\n\nThe code chunk below shows ggbetweenstats() being used to build a visual for two-sample mean test of Maths scores by gender.Default information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\nCode\nggbetweenstats(\n  data = exam_data,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nThe code chunk below shows ggbetweenstats() being used to build a visual for One-way ANOVA test on English score by race.\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\nCode\nggbetweenstats(\n  data = exam_data,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below shows ggscatterstats() being used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\n\nCode\nggscatterstats(\n  data = exam_data,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\n\nCode\nexam1 &lt;- exam_data %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\n\nWe then use ggbarstats() to build a visual for Significant Test of Association, as shown in the code chunk below.\n\n\nCode\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#getting-started",
    "title": "Hands-on Exercise 04a - Visual Statistical Analysis",
    "section": "",
    "text": "In this exercise using Exam_data, we will be using ggstatsplot and tidyverse. rstantools and PMCMRplus are also be required for plotting the ONEWAY ANOVA graph.\n\n\nCode\npacman::p_load(ggstatsplot, tidyverse, rstantools)\n\n\n\n\n\nFor this exercise, Exam_data.csv provided will be imported into R by using read_csv() of readr package.\n\n\nCode\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores, with default information: - statistical details - Bayes Factor - sample sizes - distribution summary.\n\n\nCode\nset.seed(1234)\n\ngghistostats(\n  data = exam_data,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\n\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\n\n\n\n\n\n\n\nThe code chunk below shows ggbetweenstats() being used to build a visual for two-sample mean test of Maths scores by gender.Default information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\nCode\nggbetweenstats(\n  data = exam_data,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nThe code chunk below shows ggbetweenstats() being used to build a visual for One-way ANOVA test on English score by race.\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\nCode\nggbetweenstats(\n  data = exam_data,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below shows ggscatterstats() being used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\n\nCode\nggscatterstats(\n  data = exam_data,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\n\nCode\nexam1 &lt;- exam_data %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\n\nWe then use ggbarstats() to build a visual for Significant Test of Association, as shown in the code chunk below.\n\n\nCode\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#visualising-models",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#visualising-models",
    "title": "Hands-on Exercise 04a - Visual Statistical Analysis",
    "section": "2. Visualising Models",
    "text": "2. Visualising Models\nIn this section, we will visualise model diagnostic and model parameters by using parameters package.\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n2.1. Getting Started - Installing and Loading the required R Packages\n\n\nCode\npacman::p_load(readxl, performance, parameters, see)\n\n\n\n\n2.2. Importing Data (ToyotaCorolla.xls)\nFor this exercise, ToyotaCorolla.xls provided will be imported into R by using read_xls() of readxl package.\n\n\nCode\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n\n\n2.3. Multiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\n\nCode\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n2.4. Model Diagnostic: checking for multicolinearity\nUsing check_collinearity() method from the performance package\n\n\nCode\ncheck_collinearity(model)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\n\nCode\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n2.5. Model Diagnostic: checking normality assumption\nUsing check_normality() method from the performance package\n\n\nCode\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\ncheck_n &lt;- check_normality(model1)\n\nplot(check_n)\n\n\n\n\n\n\n\n2.6. Model Diagnostic: Check model for homogeneity of variances\nUsing check_heteroscedasticity() method from the performance package\n\n\nCode\ncheck_h &lt;- check_heteroscedasticity(model1)\n\nplot(check_h)\n\n\n\n\n\n\n\n2.7. Model Diagnostic: Complete check\nWe can also perform the complete by using check_model().\n\n\nCode\ncheck_model(model1)\n\n\n\n\n\n\n\n2.8. Visualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package are used to visualise the parameters of a regression model.\n\n\nCode\nplot(parameters(model1))\n\n\n\n\n\n\n\n2.9. Visualising Regression Parameters: ggcoefstats() method\nThis example uses ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\n\nCode\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html",
    "title": "Hands-on Exercise 04b - Visualising Uncertainty",
    "section": "",
    "text": "A point estimate is a single number, such as a mean.\nUncertainty is expressed as standard error, confidence interval, or credible interval\nImportant:\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualizing-the-uncertainty-of-point-estimates",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualizing-the-uncertainty-of-point-estimates",
    "title": "Hands-on Exercise 04b - Visualising Uncertainty",
    "section": "",
    "text": "A point estimate is a single number, such as a mean.\nUncertainty is expressed as standard error, confidence interval, or credible interval\nImportant:\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#getting-started",
    "title": "Hands-on Exercise 04b - Visualising Uncertainty",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n2.1. Installing and Loading the required R Packages\nIn this exercise using Exam_data, we will be using tidyverse, plotly, crosstalk, DT, ggdist and gganimate.\n\n\nCode\npacman::p_load(tidyverse, plotly, crosstalk, DT, ggdist, gganimate)\n\n\n\n\n2.2. Importing Data (Exam_data)\n\n\nCode\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n2.3. Visualizing the uncertainty of point estimates: ggplot2 methods\nThe code chunk below performs the followings:\n\ngroup the observation by RACE,\ncomputes the count of observations, mean, standard deviation and standard error of Maths by RACE, and\nsave the output as a tibble data table called my_sum.\n\n\n\nCode\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\nmy_sum\n\n\n# A tibble: 4 × 5\n  RACE        n  mean    sd    se\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Chinese   193  76.5  15.7  1.13\n2 Indian     12  60.7  23.4  7.04\n3 Malay     108  57.4  21.1  2.04\n4 Others      9  69.7  10.7  3.79\n\n\nNext, the code chunk below will\n\n\nCode\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n2.4. Visualizing the uncertainty of point estimates: ggplot2 methods\nThe code chunk below is used to reveal the standard error of mean maths score by race.\n\n\nCode\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by race\")\n\n\n\n\n\n\n\n2.5. Visualizing the uncertainty of point estimates: ggplot2 methods\nPlotting the 95% confidence interval of mean maths score by race. The error bars are sorted by the average maths scores.\n\n\nCode\nmy_sum2 &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1)) %&gt;%\n  mutate(ci95= qt(c(0.05, 0.95), length(n) - 1) * se) %&gt;%\n  mutate(ci99= qt(c(0.01, 0.99), length(n) - 1) * se)\n\nmy_sum2$RACE = with(my_sum2, reorder(RACE, -mean))\n\nggplot(my_sum2) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-ci95, \n        ymax=mean+ci95), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"95% Confidence Interval of mean maths score by race\")\n\n\n\n\n\n\n\n2.6. Visualizing the uncertainty of point estimates with interactive error bars\nInteractive error bars for the 99% confidence interval of mean maths score by race.\n\n\nCode\ncolnames(my_sum) &lt;- c('Race', 'No. of pupils','Avg Scores','Std Dev','Std Error')\ncolnames(my_sum2) &lt;- c('Race', 'No. of pupils','Avg Scores','Std Dev','Std Error', '95% CI', '99% CI')\n\nDT::datatable(my_sum, class= \"compact\")\n\n\n\n\n\n\n\nCode\nd &lt;- highlight_key(my_sum)\n\np &lt;- ggplot(my_sum2) +\n      geom_errorbar(\n        aes(x=Race, \n            ymin=`Avg Scores`-`99% CI`, \n            ymax=`Avg Scores`+`99% CI`), \n        width=0.2, \n        colour=\"black\", \n        alpha=0.9, \n        linewidth=0.5) +\n      geom_point(aes\n               (x=Race, \n                y=`Avg Scores`, \n                text=paste(\"N=\",`No. of pupils`,\"&lt;br&gt;99% CI=\",`99% CI`)), \n               stat=\"identity\", \n               color=\"red\",\n               size = 1.5,\n               alpha=1) +\n      ggtitle(\"99% Confidence Interval of \\n mean maths score by race\")\n\n\ngg &lt;- highlight(ggplotly(p), tooltip=\"text\")\n#                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualising-uncertainty-ggdist-package",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualising-uncertainty-ggdist-package",
    "title": "Hands-on Exercise 04b - Visualising Uncertainty",
    "section": "3. Visualising Uncertainty: ggdist package",
    "text": "3. Visualising Uncertainty: ggdist package\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\n\n\n\n\n3.1. Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\nNOTE: This function comes with many arguments, refer to the syntax reference here for more detail.\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n3.2. Visualizing the uncertainty of point estimates: ggdist methods\nShowing the plots with 95% and 99% confidence intervals.\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n3.3. Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\nNOTE: This function comes with many arguments, refer to the syntax reference here for more detail.\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 04b - Visualising Uncertainty",
    "section": "4. Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "4. Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\nStep 1: Installing ungeviz package (only need to perform this step once1)\n\n\nCode\n# devtools::install_github(\"wilkelab/ungeviz\")\n\n\nStep 2: Launch the application in R\n\n\nCode\nlibrary(ungeviz)\n\n\n\n\nCode\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops-1",
    "title": "Hands-on Exercise 04b - Visualising Uncertainty",
    "section": "5. Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "5. Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\n\n\nCode\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html",
    "title": "Hands-on Exercise 04c - Building Funnel Plot with R",
    "section": "",
    "text": "In this exercise, we will be using\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\n\nCode\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\n\nFor this exercise, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal.\nFor this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\n\nCode\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#getting-started",
    "title": "Hands-on Exercise 04c - Building Funnel Plot with R",
    "section": "",
    "text": "In this exercise, we will be using\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\n\nCode\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\n\nFor this exercise, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal.\nFor this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\n\nCode\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#funnelplotr-methods",
    "title": "Hands-on Exercise 04c - Building Funnel Plot with R",
    "section": "1.2. FunnelPlotR methods",
    "text": "1.2. FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nx_range and y_range: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n1.2.1. FunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot.\n\n\nCode\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_type argument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n1.2.2. FunnelPlotR methods: Makeover 1\n\n\nCode\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  x_range = c(0, 6500),  #&lt;&lt;\n  y_range = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\ndata_type argument is used to change from default “SR” to “PR” (i.e. proportions).\nx_range and y_range are used to set the range of x-axis and y-axis\n\n\n\n1.2.3. FunnelPlotR methods: Makeover 2\n\n\nCode\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  x_range = c(0, 6500),  \n  y_range = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by \\nCumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Exercise 04c - Building Funnel Plot with R",
    "section": "1.3. Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "1.3. Funnel Plot for Fair Visual Comparison: ggplot2 methods\nBuilding funnel plots step-by-step by using ggplot2, allows customising speciallised data visualisation like funnel plot.\n\n1.3.1. Computing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\n\nCode\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\n\nNext, the fit.mean is computed by using the code chunk below.\n\n\nCode\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\n1.3.2. Calculate lower and upper limits for 95% and 99.9% CI\nThe codes below is used to compute the lower and upper limits for 95% confidence interval.\n\n\nCode\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\n1.3.3. Plotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\n\nCode\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n1.3.4. Interactive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\n\nCode\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html",
    "title": "In-Class Exercise 1",
    "section": "",
    "text": "Code\npacman::p_load(tidyverse)\n\n\n\n\n\n\n\nCode\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#getting-started",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#getting-started",
    "title": "In-Class Exercise 1",
    "section": "",
    "text": "Code\npacman::p_load(tidyverse)\n\n\n\n\n\n\n\nCode\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#working-with-theme",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#working-with-theme",
    "title": "In-Class Exercise 1",
    "section": "Working with theme",
    "text": "Working with theme\nPlot a horizontal bar chart\n\nChange the colours of plot panel background of theme_minimal() to light blue and the color of gridlines to white\n\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal() + \n  theme(\n    panel.background = element_rect(fill = \"lightblue\", colour = \"lightblue\", \n                                    linewidth = 0.5, linetype = \"solid\"),\n    panel.grid.major = element_line(linewidth = 0.5, linetype = 'solid', colour = \"white\"), \n    panel.grid.minor = element_line(linewidth = 0.25, linetype = 'solid', colour = \"white\"))"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#designing-data-drive-graphics-for-analysis-i",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#designing-data-drive-graphics-for-analysis-i",
    "title": "In-Class Exercise 1",
    "section": "Designing Data-drive Graphics for Analysis I",
    "text": "Designing Data-drive Graphics for Analysis I\nIn this example, we will revise the vertical bar chart to provide additional information.\nThe left tab panel shows the original vertical bar chart, while the right panel shows the revised version of the vertical bar chart.\n\nOriginalRevised\n\n\nA simple vertical bar chart for frequency analysis.\nCritics:\n\ny-axis label is not clear (i.e. count)\nTo support effective comparison, the bars should be sorted by their respective frequencies.\nFor static graph, frequency values should be added to provide addition information.\n\n\n\nCode\nggplot(data=exam_data,\n       aes(x = RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\nThe revised vertical bar chart will show:\n\nThe y-axis will be changed to the Total Number of Pupils\nIn order to support effective comparison, we will sort the bars by their respective frequencies\nSince this is a static graph, we will also include the frequency values to provide additional information.\n\n\n\nCode\nggplot(data = exam_data,\n       aes(x = reorder(RACE, RACE,\n                       function(x)-length(x)))) +\n  geom_bar() + \n  ylim(0,220) +\n  geom_text(stat=\"count\", \n      aes(label=paste0(after_stat(count), \", \", \n      round(after_stat(count)/sum(after_stat(count))*100, 1), \"%\")),\n      vjust=-1) +\n  xlab(\"RACE\") +\n  ylab(\"NO. OF\\nPUPILS\") +\n  theme(axis.title.y=element_text(angle = 0))"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#designing-data-drive-graphics-for-analysis-ii",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#designing-data-drive-graphics-for-analysis-ii",
    "title": "In-Class Exercise 1",
    "section": "Designing Data-drive Graphics for Analysis II",
    "text": "Designing Data-drive Graphics for Analysis II\nIn this example, we will improve the aesthetics of the histogram.\nThe left tab panel shows the original version of the histogram plotted, while the right panel shows the revised version, with added information of the mean and median lines.\n\nOriginalRevised\n\n\nA simple histogram.\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS)) +\n  geom_histogram(bins=30)\n\n\n\n\n\n\n\nWe will\n\nChange the fill color to light blue and line color to black\nInclude the mean and median lines onto the histogram plot\n\n\n\nCode\nggplot(data = exam_data,\n       aes(x=MATHS)) +\n  geom_histogram(bins=20,\n                 fill=\"light blue\",\n                 color=\"black\") +\n  geom_vline(aes(xintercept=mean(MATHS, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1) +\n  geom_vline(aes(xintercept=median(MATHS, na.rm=T)),\n             color=\"grey30\",\n             linetype=\"dashed\", \n             linewidth=1)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#designing-data-drive-graphics-for-analysis-iii",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#designing-data-drive-graphics-for-analysis-iii",
    "title": "In-Class Exercise 1",
    "section": "Designing Data-drive Graphics for Analysis III",
    "text": "Designing Data-drive Graphics for Analysis III\nThe original histograms on the left tab (Original) are elegantly designed but not informative. This is because they only reveal the distribution of English scores by gender but without context such as all pupils.\nIn this example, on the right tab (Revised), we will show the distribution of English scores by gender and also include the histogram of all pupils at the background.\n\nOriginalRevised\n\n\n\n\nCode\nggplot(data=exam_data, aes(x = ENGLISH)) + \n  geom_histogram(bins=30) +\n    facet_grid(~ GENDER)\n\n\n\n\n\n\n\n\n\nCode\nbackgd_data &lt;- exam_data   \nd_bg &lt;- backgd_data[, -3]\n\nggplot(backgd_data, aes(x = ENGLISH, fill = GENDER)) +\n  geom_histogram(data = d_bg, fill = \"grey\", alpha = .5, bins=30) +\n  geom_histogram(colour = \"black\", bins=30) +\n  facet_wrap(~ GENDER) +\n  guides(none) +  \n  theme_bw()"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#designing-data-drive-graphics-for-analysis-iv",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#designing-data-drive-graphics-for-analysis-iv",
    "title": "In-Class Exercise 1",
    "section": "Designing Data-drive Graphics for Analysis IV",
    "text": "Designing Data-drive Graphics for Analysis IV\nIn this example, we will beautify (makeover) the scatterplot from left tab (Original) to right tab (Revised)\n\nOriginalRevised\n\n\n\n\nCode\nggplot(data=exam_data, \n       aes(x=MATHS, y=ENGLISH)) +\n  geom_point() \n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n       aes(x=MATHS, y=ENGLISH)) +\n  geom_point() +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  geom_hline(yintercept=50,\n             linetype=\"dashed\",\n             color=\"grey60\",\n             linewidth=1) + \n  geom_vline(xintercept=50, \n             linetype=\"dashed\",\n             color=\"grey60\",\n             linewidth=1)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex04/In-Class_Ex04.html",
    "href": "In-Class_Ex/In-Class_Ex04/In-Class_Ex04.html",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "In this exercise using Exam_data, we will be using tidyverse, rstatix, gt and patchwork.\n\npacman::p_load(tidyverse, rstatix, gt, patchwork)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nA Q-Q plot (Quantile-Quantile plot) is used to assess whether a set of data points are normally distributed.\nif the data is normally distrbuted, the points in a Q-Q plot will lie on a straight diagonal line. Conversely, if the points deviate significantly from the straight diagonal line, then it’s less likely that the data is normally distributed.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data,\n       aes(sample=ENGLISH)) + \n  stat_qq() + \n  stat_qq_line()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the points deviate significantly form the straight diagnoal line. This is a clear indication that the set of data is not normally distributed.\n\n\n\n\n\npng, webshot2 packages will be required to run the following codes.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\nqq &lt;- ggplot(exam_data,\n             aes(sample=ENGLISH)) + \n  stat_qq() +\n  stat_qq_line()\n\n# running shapiro test and save into gt() format\nsw_t &lt;- exam_data %&gt;%\n  shapiro_test(ENGLISH) %&gt;%\n  gt()\n\n\n# converting the sw_t into an image file (png)\ntmp &lt;- tempfile(fileext = '.png')\ngtsave(sw_t, tmp)\ntable_png &lt;- png::readPNG(tmp,\n                          native = TRUE)\n\nqq + table_png"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex04/In-Class_Ex04.html#getting-started",
    "href": "In-Class_Ex/In-Class_Ex04/In-Class_Ex04.html#getting-started",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "In this exercise using Exam_data, we will be using tidyverse, rstatix, gt and patchwork.\n\npacman::p_load(tidyverse, rstatix, gt, patchwork)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nA Q-Q plot (Quantile-Quantile plot) is used to assess whether a set of data points are normally distributed.\nif the data is normally distrbuted, the points in a Q-Q plot will lie on a straight diagonal line. Conversely, if the points deviate significantly from the straight diagonal line, then it’s less likely that the data is normally distributed.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data,\n       aes(sample=ENGLISH)) + \n  stat_qq() + \n  stat_qq_line()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the points deviate significantly form the straight diagnoal line. This is a clear indication that the set of data is not normally distributed.\n\n\n\n\n\npng, webshot2 packages will be required to run the following codes.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\nqq &lt;- ggplot(exam_data,\n             aes(sample=ENGLISH)) + \n  stat_qq() +\n  stat_qq_line()\n\n# running shapiro test and save into gt() format\nsw_t &lt;- exam_data %&gt;%\n  shapiro_test(ENGLISH) %&gt;%\n  gt()\n\n\n# converting the sw_t into an image file (png)\ntmp &lt;- tempfile(fileext = '.png')\ngtsave(sw_t, tmp)\ntable_png &lt;- png::readPNG(tmp,\n                          native = TRUE)\n\nqq + table_png"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#the-task",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#the-task",
    "title": "Take-Home Exercise 01",
    "section": "",
    "text": "In this Take-Home Exercise, we will reveal the demographic and financial characteristics of the city of Engagement by using appropriate static and interactive statistical graphics methods, to help the city managers and planners to explore the complex data in an engaging way and reveal hidden patterns."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#data-source",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#data-source",
    "title": "Take-Home Exercise 01",
    "section": "",
    "text": "Two (2) datasets will be used, as inputs:\n\nParticipants.csv – information about the residents of City of Engagement who agreed to participate in this study.\nFinancialJournal.csv – information about the financial transactions of the participants from March 2022 to February 2023."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#load-participants.csv",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#load-participants.csv",
    "title": "Take-Home Exercise 01",
    "section": "3.1. Load Participants.csv",
    "text": "3.1. Load Participants.csv\nImport data from csv using readr::read_csv() and store it in variable participants_raw.\n\n\nShow the codes\n# Load the Participants.csv into the environment\nparticipants_raw &lt;- read_csv(\"data/Participants.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#perform-data-wrangling-on-participants_raw",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#perform-data-wrangling-on-participants_raw",
    "title": "Take-Home Exercise 01",
    "section": "3.2 Perform Data Wrangling on participants_raw",
    "text": "3.2 Perform Data Wrangling on participants_raw\nWe will performing the following activities as part of data wrangling.\n\ncheck the data set for duplicate records and remove them (if there is any)\nconvert the data into the respective data types (string factor, integer, boolean, double), for easier analysis.\n\n\n\nCode\n# Check if 'participants' contains any duplicate rows\nhas_duplicates &lt;- any(duplicated(participants_raw))\n\nif (has_duplicates) {\n  # contains duplicate rows\n  # extracting only the unique participants records\n  participants &lt;- distinct(participants_raw)\n} else {\n  # No duplicate rows\n  # since there are no duplicate rows, we will just replicate participants data for consistency\n  participants &lt;- participants_raw\n}\nparticipants\n\n\n# A tibble: 1,011 × 7\n   participantId householdSize haveKids   age educationLevel      interestGroup\n           &lt;dbl&gt;         &lt;dbl&gt; &lt;lgl&gt;    &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;        \n 1             0             3 TRUE        36 HighSchoolOrCollege H            \n 2             1             3 TRUE        25 HighSchoolOrCollege B            \n 3             2             3 TRUE        35 HighSchoolOrCollege A            \n 4             3             3 TRUE        21 HighSchoolOrCollege I            \n 5             4             3 TRUE        43 Bachelors           H            \n 6             5             3 TRUE        32 HighSchoolOrCollege D            \n 7             6             3 TRUE        26 HighSchoolOrCollege I            \n 8             7             3 TRUE        27 Bachelors           A            \n 9             8             3 TRUE        20 Bachelors           G            \n10             9             3 TRUE        35 Bachelors           D            \n# ℹ 1,001 more rows\n# ℹ 1 more variable: joviality &lt;dbl&gt;\n\n\nLooking at the participants data, we notice that there are a few problems that we need to resolve before we can perform analysis on them\n\nparticipantId is in &lt;dbl&gt; format. Since this is used to identify each participant uniquely, we will convert it to a string factor\neducationLevel is in &lt;chr&gt; format. We will convert it to a string factor\nhouseholdSize is in &lt;dbl&gt; format. We will convert it to &lt;int&gt; format since it should be a whole number.\nage is in &lt;dbl&gt; format. We will convert it to &lt;int&gt; format since it should be a whole number.\njoavlity values are too granular. We will round it to 2 decimal places.\nWe will also bin the age and jovality for ease of analysis\n\n\n\nCode\n# Convert participantId, educationLevel columns into string factor\nparticipants$participantId &lt;- as.factor(as.character(participants$participantId))\nparticipants$educationLevel &lt;- as.factor(as.character(participants$educationLevel))\n\n# Convert householdSize and age into whole numbers, i.e. integers\nparticipants$householdSize &lt;- as.integer(participants$householdSize)\nparticipants$age &lt;- as.integer(participants$age)\n\n# Bin the age column into 5 groupings \nparticipants &lt;- participants %&gt;% mutate(age_bin = cut(age, breaks=c(0, 30, 40, 50, 60)))\n\n# Convert joviality column into a double data type and round it to 2 decimal places\nparticipants$joviality &lt;- round(as.double(participants$joviality), 2)\n\n# Bin the joviality column into 3 broad categories\nparticipants &lt;- participants %&gt;% \n  mutate(jov_bin = cut(joviality, breaks=c(0, 0.2, 0.4, 0.6, 0.8, 1.0)))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#load-financialjournal.csv-and-perform-data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#load-financialjournal.csv-and-perform-data-wrangling",
    "title": "Take-Home Exercise 01",
    "section": "3.2. Load FinancialJournal.csv and Perform Data Wrangling",
    "text": "3.2. Load FinancialJournal.csv and Perform Data Wrangling\nAs part of data wrangling, we will check the data set for duplicate records and remove them (if there is any), then we will convert the data into the respective data types (string factor, integer, boolean, double), for easier analysis.\nWe will also be extracting the year and month from the timestamp column and transpose the values in the category column into individual columns.\n\n\nCode\n# Load the FinancialJournal.csv into the environment\nfinjournal_raw &lt;- read_csv(\"data/FinancialJournal.csv\")\n\n# Check if 'finjournal' contains any duplicate rows\nhas_duplicates &lt;- any(duplicated(finjournal_raw))\n\nif (has_duplicates) {\n  print(\"The finjournal_raw data frame contains duplicate rows.\")\n  \n  # Remove duplicate records from the 'finjournal_raw' data frame\n  finjournal &lt;- distinct(finjournal_raw)\n} else {\n  print(\"The finjournal_raw data frame does not contain duplicate rows.\")\n  # if there are no duplicate rows, we will just replicate finjournal_raw data for easy merging \n  finjournal &lt;- finjournal_raw\n}\n\n\n[1] \"The finjournal_raw data frame contains duplicate rows.\"\n\n\nCode\n# Convert participantId, category columns into string factor \nfinjournal$participantId &lt;- as.factor(as.character(finjournal$participantId))\nfinjournal$category &lt;- as.factor(as.character(finjournal$category))\n\n# Extract the date component from timestamp column\nfinjournal$date &lt;- as.Date(finjournal$timestamp)\n\n# Extract the month and year from the date component\nfinjournal$mthYear &lt;- format(finjournal$date, \"%m-%Y\")\n\n# Convert the negative amount values to absolute values and round to 2 decimal places\nfinjournal$amount &lt;- round(abs(finjournal$amount), 2)\n\n\n\n4.2.1. Checking the quality of financial journal data\nThere are often dirty data that we will need to cleanse before performing any data analysis. This portion will check on the quality of the financial journal data.\n\n4.2.1.1. Identifying any possible outliers by retrieving the total number of transactions for each participant\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\n\nfinjournal_grp &lt;- finjournal %&gt;%\n  group_by(participantId) %&gt;%\n  summarise(transaction_cnt = n()) %&gt;%\n  arrange(transaction_cnt)\n\n# Plotting a histogram with the data extracted for better visualisation\nplot &lt;- ggplot(data = finjournal_grp, \n               aes(x = transaction_cnt)) +\n  geom_histogram(color=\"gray30\", fill=\"deepskyblue3\") + \n  ylim(0, 150) +\n  ggtitle(\"Distribution of Transactions by Participants\") + \n  xlab(\"Transaction Count\") +\n  ylab(\"Number of Participants\") + \n  theme_light()\n\nplot &lt;- ggplotly(plot, tooltip = c(\"y\"))\n\nplot\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs you noticed from the chart above, there is a number of participants (131 of them in total), who have a significantly low number of transaction records, which is far off from the main distribution. As such, we will treat these as outliers and remove them from our analysis.\n\n\n\n\n\n4.2.1.2. Removing the outliers\nWe will proceed to remove the 131 participants who have significantly low number of transaction records, compared to the rest.\n\n\nCode\n# Get the transaction count for the last row of the 131 outliers \ntxn_count &lt;- finjournal_grp[131, \"transaction_cnt\"]\n\n# Extract the participants without the 131 participants \nfinjournal_grp_filtered &lt;- finjournal_grp %&gt;%\n  filter(transaction_cnt &gt; as.integer(txn_count))\n\n# Remove the 131 participants from the finjournal\nfinjournal_filtered &lt;- merge(finjournal, finjournal_grp_filtered, by = \"participantId\")\n\n\n\n\n4.2.1.3. Transposing the category of transaction into indvidual columns for easy analysis\nIn this section, we will be performing a series of data wrangling activities for each participant:\n\nsum up all the daily transactions amount based on the respective categories\ntranspose the transaction categories into multiple columns (Education, Food, Recreation, Shelter, Wage, RentAdjustment)\nreplace all the NA values to 0\nderive total monthly expenses and total monthly earnings\nderive monthly savings (earnings - expenses)\n\n\n\nCode\n# sum up all the daily transactions amount based on the respective categories\ntemp_finjournal &lt;- finjournal_filtered %&gt;% \n  group_by(participantId, category, mthYear) %&gt;% \n  summarise(Total = sum(amount))\n\n# Transpose the categories to columns\ntemp_finjournal &lt;- pivot_wider(temp_finjournal, \n                               names_from = category, \n                               values_from = Total)\n\n# Replace all the NA values to 0\ntemp_finjournal[is.na(temp_finjournal)] &lt;- 0.0\n\n# Derive the total monthly expenses and total monthly earnings for each participant \ntemp_finjournal_sum &lt;- temp_finjournal %&gt;%\n  group_by(participantId, mthYear) %&gt;%\n  summarize(sum_expense = sum(Education, Food,\n                              Recreation, Shelter),\n            sum_earning = sum(Wage, RentAdjustment))\n\n# Derive the monthly savings for each participant\n\ntemp_finjournal_sum$saving &lt;- temp_finjournal_sum$sum_earning -\n                              temp_finjournal_sum$sum_expense\n\n\nfinal_finjournal &lt;- merge(temp_finjournal, temp_finjournal_sum, by = \"participantId\")\n\n# Removing the repeated \"mthYear.y\" column and rename \"mthYear.x\" column to \"mthYear\"\nfinal_finjournal &lt;- select(final_finjournal, -c(mthYear.y))\n\nnames(final_finjournal)[2] &lt;- \"mthYear\"\n\n\n\n\nCode\n### Not sure if want to do this way for analysis. KIV KIV KIV\n### KIV \n\n#final_finjournal_wide &lt;- final_finjournal %&gt;%\n#  pivot_wider(names_from = mthYear, \n#              values_from = c(Education, Food, Recreation, Shelter, Wage, RentAdjustment))\n\n# Replace all the 'NA' values to 0.0\n#final_finjournal_wide[is.na(final_finjournal_wide)] &lt;- 0.0\n\n\n\n\n4.3 Combine the 2 data sets\nWe will combine the finalised data sets of Participants and FinancialJournal into 1 single data frame for ease of analysis.\n\n\nCode\n# Merge the two files based on the 'participantId' column\n#merged_data &lt;- merge(participants, final_finjournal_wide, by = \"participantId\")   #will need this we decide to revise the earlier part with final_finjournal_wide\nmerged_data &lt;- merge(participants, final_finjournal, by = \"participantId\")\n\n# Write the merged data to a new CSV file -&gt; This might not be necessary once we have checked the data\n#write_csv(merged_data, \"data/combined.csv\", na=\"\", append=FALSE, col_names=TRUE)\n\n\n\n\n4.4. Data Analysis"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#perform-data-wrangling-on-participants-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#perform-data-wrangling-on-participants-data",
    "title": "Take-Home Exercise 01",
    "section": "3.2 Perform Data Wrangling on participants data",
    "text": "3.2 Perform Data Wrangling on participants data\nAs part of the data wrangling process, we will also check the data set for duplicate records and remove them (if there is any).\n\n\nShow the codes\n# Check if 'participants' contains any duplicate rows\nhas_duplicates &lt;- any(duplicated(participants_raw))\n\nif (has_duplicates) {\n  # contains duplicate rows\n  # extracting only the unique participants records\n  participants &lt;- distinct(participants_raw)\n} else {\n  # No duplicate rows\n  # since there are no duplicate rows, we will just replicate participants data for consistency\n  participants &lt;- participants_raw\n}\nparticipants\n\n\n# A tibble: 1,011 × 7\n   participantId householdSize haveKids   age educationLevel      interestGroup\n           &lt;dbl&gt;         &lt;dbl&gt; &lt;lgl&gt;    &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;        \n 1             0             3 TRUE        36 HighSchoolOrCollege H            \n 2             1             3 TRUE        25 HighSchoolOrCollege B            \n 3             2             3 TRUE        35 HighSchoolOrCollege A            \n 4             3             3 TRUE        21 HighSchoolOrCollege I            \n 5             4             3 TRUE        43 Bachelors           H            \n 6             5             3 TRUE        32 HighSchoolOrCollege D            \n 7             6             3 TRUE        26 HighSchoolOrCollege I            \n 8             7             3 TRUE        27 Bachelors           A            \n 9             8             3 TRUE        20 Bachelors           G            \n10             9             3 TRUE        35 Bachelors           D            \n# ℹ 1,001 more rows\n# ℹ 1 more variable: joviality &lt;dbl&gt;\n\n\nLooking at the participants data set above, we notice that there are a few problems that we need to resolve before we can perform analysis on them\n\nparticipantId is in &lt;dbl&gt; format. Since this is used to identify each participant uniquely, we will convert it to a string factor\neducationLevel is in &lt;chr&gt; format. We will convert it to a string factor\nhouseholdSize is in &lt;dbl&gt; format. We will convert it to &lt;int&gt; format since it should be a whole number.\nhaveKids is in &lt;lgl&gt; format and has value of TRUE and FALSE, which are not very intuitive. We will change it to YES and NO instead.\nage is in &lt;dbl&gt; format. We will convert it to &lt;int&gt; format since it should be a whole number.\nWe will also bin the age and joviality for analysis purposes.\n\n\n\nShow the codes\n# Convert participantId, educationLevel columns into string factor\nparticipants$participantId &lt;- as.factor(as.character(participants$participantId))\nparticipants$educationLevel &lt;- as.factor(as.character(participants$educationLevel))\n\n# Convert householdSize and age into whole numbers, i.e. integers\nparticipants$householdSize &lt;- as.integer(participants$householdSize)\nparticipants$age &lt;- as.integer(participants$age)\n\n# Replace `TRUE` to `YES` and `FALSE` to `NO` for haveKids column\nparticipants$haveKids &lt;- ifelse(participants$haveKids == \"TRUE\", \"YES\",\n                                participants$haveKids)\n\nparticipants$haveKids &lt;- ifelse(participants$haveKids == \"FALSE\", \"NO\",\n                                participants$haveKids)\n\n\n# Convert joviality column into a double data type and round it to 2 decimal places\nparticipants$joviality &lt;- as.double(participants$joviality)\n\n# Bin the age column into groupings and save into another column, age_bin\nparticipants &lt;- participants %&gt;% mutate(age_bin = cut(age, breaks=c(17, 30, 40, 50, 60)))\n\n# Bin the joviality column into broad categories and save into another column, jov_bin\nparticipants &lt;- participants %&gt;% \n  mutate(jov_bin = cut(joviality, breaks=c(0, 0.2, 0.4, 0.6, 0.8, 1.0)))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#load-financialjournal.csv",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#load-financialjournal.csv",
    "title": "Take-Home Exercise 01",
    "section": "3.3. Load FinancialJournal.csv",
    "text": "3.3. Load FinancialJournal.csv\nImport data from csv using readr::read_csv() and store it in variable finjournal_raw.\n\n\nShow the codes\n# Load the FinancialJournal.csv into the environment\nfinjournal_raw &lt;- read_csv(\"data/FinancialJournal.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#perform-data-wrangling-on-finjournal-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#perform-data-wrangling-on-finjournal-data",
    "title": "Take-Home Exercise 01",
    "section": "3.4. Perform Data Wrangling on finjournal data",
    "text": "3.4. Perform Data Wrangling on finjournal data\nAs part of the data wrangling process, we will also check the data set for duplicate records and remove them (if there is any).\n\n\nCode\n# Load the FinancialJournal.csv into the environment\nfinjournal_raw &lt;- read_csv(\"data/FinancialJournal.csv\")\n\n# Check if 'finjournal' contains any duplicate rows\nhas_duplicates &lt;- any(duplicated(finjournal_raw))\n\nif (has_duplicates) {\n  # contains duplicate rows\n  \n  # Remove duplicate records from the 'finjournal_raw' data frame\n  finjournal &lt;- distinct(finjournal_raw)\n} else {\n  # No duplicate rows\n  # if there are no duplicate rows, we will just replicate finjournal_raw data for consistency\n  finjournal &lt;- finjournal_raw\n}\nfinjournal\n\n\n# A tibble: 1,512,523 × 4\n   participantId timestamp           category  amount\n           &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;      &lt;dbl&gt;\n 1             0 2022-03-01 00:00:00 Wage      2473. \n 2             0 2022-03-01 00:00:00 Shelter   -555. \n 3             0 2022-03-01 00:00:00 Education  -38.0\n 4             1 2022-03-01 00:00:00 Wage      2047. \n 5             1 2022-03-01 00:00:00 Shelter   -555. \n 6             1 2022-03-01 00:00:00 Education  -38.0\n 7             2 2022-03-01 00:00:00 Wage      2437. \n 8             2 2022-03-01 00:00:00 Shelter   -557. \n 9             2 2022-03-01 00:00:00 Education  -12.8\n10             3 2022-03-01 00:00:00 Wage      2367. \n# ℹ 1,512,513 more rows\n\n\nLooking at the finjournal data set above, we notice that there are a few problems that we need to resolve before we can perform analysis on them\n\nparticipantId is in &lt;dbl&gt; format. Since this is used to identify each participant uniquely, we will convert it to a string factor\ntimestamp is in &lt;POSIXct&gt; format. For the purpose of analysis, we will extract the month and year from this data item.\ncategory is in &lt;chr&gt; format. We will convert it to a string factor\namount values are too granular. We will round it to 2 decimal places. There are also negative values, which we will convert all to absolute values instead.\n\n\n\nCode\n# Convert participantId, category columns into string factor \nfinjournal$participantId &lt;- as.factor(as.character(finjournal$participantId))\nfinjournal$category &lt;- as.factor(as.character(finjournal$category))\n\n# Extract the date component from timestamp column\nfinjournal$date &lt;- as.Date(finjournal$timestamp)\n\n# Extract the month and year from the date component\nfinjournal$YearMonth &lt;- format(finjournal$date, \"%Y-%m\")\n\n# Convert the negative amount values to absolute values and round to 2 decimal places\nfinjournal$amount &lt;- round(abs(finjournal$amount), 2)\n\n\n\n3.4.1. Checking the quality of financial journal data\nThere are often dirty data that we will need to cleanse before performing any data analysis. This portion will check on the quality of the financial journal data.\n\n3.4.1.1. Identifying any possible outliers\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\n\nfinjournal_grp &lt;- finjournal %&gt;%\n  group_by(participantId) %&gt;%\n  summarise(transaction_cnt = n()) %&gt;%\n  arrange(transaction_cnt)\n\n# Plotting a histogram with the data extracted for better visualisation\nplot &lt;- ggplot(data = finjournal_grp, \n               aes(x = transaction_cnt)) +\n  geom_histogram(color=\"gray30\", fill=\"deepskyblue3\") + \n  ylim(0, 150) +\n  ggtitle(\"Distribution of Transactions by Participants\") + \n  xlab(\"Transaction Count\") +\n  ylab(\"Number of Participants\") + \n  theme_light()\n\nplot &lt;- ggplotly(plot, tooltip = c(\"y\"))\n\nplot\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs you noticed from the chart above, there is a number of participants (131 of them in total), who have a significantly low number of transaction records, which is far off from the main distribution. As such, we will treat these as outliers and remove them from our analysis.\n\n\n\n\n3.4.1.2. Removing the outliers\nWe will proceed to remove the 131 participants who have significantly low number of transaction records, compared to the rest.\n\n\nCode\n# Get the transaction count for the last row of the 131 outliers \ntxn_count &lt;- finjournal_grp[131, \"transaction_cnt\"]\n\n# Extract the participants without the 131 participants \nfinjournal_grp_filtered &lt;- finjournal_grp %&gt;%\n  filter(transaction_cnt &gt; as.integer(txn_count))\n\n# Remove the 131 participants from the finjournal\nfinjournal_filtered &lt;- merge(finjournal, finjournal_grp_filtered, by = \"participantId\")\n\n\n\n\n3.4.1.3. Transposing the category of transaction into indvidual columns\nIn this section, we will be performing a series of data wrangling activities for each participant:\n\nsum up all the daily transactions amount based on the respective categories\ntranspose the transaction categories into multiple columns (Education, Food, Recreation, Shelter, Wage, RentAdjustment)\nreplace all the NA values to 0\nderive total monthly expenses and total monthly earnings\nderive monthly savings (earnings - expenses)\n\n\n\nCode\n# sum up all the daily transactions amount based on the respective categories\nfinal_finjournal &lt;- finjournal_filtered %&gt;% \n  group_by(participantId, category, YearMonth) %&gt;% \n  summarise(Total = sum(amount))\n\n# Transpose the categories to columns\nfinal_finjournal_wide &lt;- pivot_wider(final_finjournal, \n                               names_from = category, \n                               values_from = Total)\n\n# Replace all the NA values to 0\nfinal_finjournal_wide[is.na(final_finjournal_wide)] &lt;- 0.0\n\n# Derive the total monthly expenses, total monthly earnings and savings for each participant \nfinal_finjournal_wide$sum_expense &lt;- final_finjournal_wide$Education + \n                                     final_finjournal_wide$Food +\n                                     final_finjournal_wide$Recreation + \n                                     final_finjournal_wide$Shelter\n\nfinal_finjournal_wide$sum_earning &lt;- final_finjournal_wide$Wage + \n                                     final_finjournal_wide$RentAdjustment\n\nfinal_finjournal_wide$saving &lt;- final_finjournal_wide$sum_earning -\n                                final_finjournal_wide$sum_expense\n\nfinal_finjournal_wide$YearMthDay &lt;- \n  as.Date(paste0(final_finjournal_wide$YearMonth,\"-01\"))\n\n\nWe will aggregate all the monthly transactions by participants, since final_finjournal will contain the detailed YearMonth for each participant.\n\n\nCode\n# To have 1 financial journal record per participant \n\nfinal_finjournal_single &lt;- final_finjournal_wide %&gt;% \n  group_by(participantId) %&gt;% \n  summarise(Total_Edu = sum(Education), \n            Total_Food = sum(Food),\n            Total_Rec = sum(Recreation),\n            Total_Shelter = sum(Shelter),\n            Total_Wage = sum(Wage),\n            Total_RentAdj = sum(RentAdjustment),\n            Total_sumExp = sum(sum_expense),\n            Total_sumEarn = sum(sum_earning),\n            Total_saving = sum(saving))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#data-visualisation-and-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#data-visualisation-and-analysis",
    "title": "Take-Home Exercise 01",
    "section": "4. Data Visualisation and Analysis",
    "text": "4. Data Visualisation and Analysis\n\n4.1. Exploratory Data Analysis\n\n4.1.1. Cost of Living over Time\n\n\nCode\nplot_ly(data = single_merged_data,\n        x = ~Total_sumEarn,\n        y = ~Total_saving,\n        color = ~educationLevel)\n\n\n\n\n\n\n\n\nCode\n#bubble plot - is it useful???\nbp &lt;- expanded_merge_data %&gt;%\n  plot_ly(x = ~sum_expense, \n          y = ~sum_earning, \n          size = ~householdSize, \n          color = ~educationLevel, \n          frame = ~YearMonth, \n          text = ~age, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          )\nbp\n\n\n\n\n\n\nTotal Wage vs Education Level\n\n\n\n4.2. One-sample test\n\n\nCode\nset.seed(1234)\n\ngghistostats(\n  data = single_merged_data,\n  x = Total_sumEarn,\n  type = \"bayes\",\n  test.value = 15000,\n  xlab = \"Total Expenses\"\n)\n\n\n\n\n\n\n\nCode\nset.seed(1234)\n\ngghistostats(\n  data = single_merged_data,\n  x = Total_sumExp,\n  type = \"bayes\",\n  test.value = 15000,\n  xlab = \"Total Expenses\"\n)\n\n\n\n\n\n\n\n4.3. The correlation between Wages, Expenditures and Age\nThe correlation charts show that there is faint negative relationship between the age and wage.\n\nAge and Wage CorrelationAge and Total Expenditure\n\n\n\nggscatterstats(data = single_merged_data,\n               x = age,\n               y = Total_Wage,\n               marginal = FALSE) +\n  theme_minimal() +\n  labs(title=\"Correlation of Total Wages and Age\",\n       x = \"Age\",\n       y = \"Total wage\") +\n  \n  theme(text = element_text(family = \"Arial\"),\n        plot.title = element_text(hjust = 0.2, size = 15, face = 'bold'),\n        plot.margin = margin(20, 20, 20, 20),\n        legend.position = \"bottom\",\n        axis.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\nggscatterstats(data = single_merged_data,\n               x = age,\n               y = Total_sumExp,\n               marginal = FALSE) +\n  theme_minimal() +\n  labs(title=\"Correlation of Total Expenditure and Age\",\n       x = \"Age\",\n       y = \"Total Expenditure\") +\n  \n  theme(text = element_text(family = \"Arial\"),\n        plot.title = element_text(hjust = 0.2, size = 15, face = 'bold'),\n        plot.margin = margin(20, 20, 20, 20),\n        legend.position = \"bottom\",\n        axis.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n4.4. The correlation between Total Earnings, Expenditures and Savings\nThe correlation charts show that there is weak negative relationship between the Total Earnings and Total Expenditure. In addition, there is a very strong relationship between the Total Earnings and Total Savings.\n\nTotal Earnings and Total SpendingTotal Earnings and Total Saving\n\n\n\nggscatterstats(data = single_merged_data,\n               x = Total_sumExp,\n               y = Total_sumEarn,\n               marginal = FALSE) +\n  theme_minimal() +\n  labs(title=\"Correlation of Total Expenditure and Total Earnings\",\n       x = \"Total Expenditure\",\n       y = \"Total Earnings\") +\n  \n  theme(text = element_text(family = \"Arial\"),\n        plot.title = element_text(hjust = 0.2, size = 15, face = 'bold'),\n        plot.margin = margin(20, 20, 20, 20),\n        legend.position = \"bottom\",\n        axis.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\nggscatterstats(data = single_merged_data,\n               x = Total_sumEarn,\n               y = Total_saving,\n               marginal = FALSE) +\n  theme_minimal() +\n  labs(title=\"Correlation of Total Earnings and Savings\",\n       x = \"Total Earnings\",\n       y = \"Total Savings\") +\n  \n  theme(text = element_text(family = \"Arial\"),\n        plot.title = element_text(hjust = 0.2, size = 15, face = 'bold'),\n        plot.margin = margin(20, 20, 20, 20),\n        legend.position = \"bottom\",\n        axis.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#exploratory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#exploratory-data-analysis",
    "title": "Take-Home Exercise 01",
    "section": "4.1. Exploratory Data Analysis",
    "text": "4.1. Exploratory Data Analysis\n\n4.1.1 Demography of the Representative Residents\nThe demography of the representative residents are as shown in the graphs below.\n\nProportion of Education LevelProportion of AgeHappiness Index\n\n\nDesign Consideration Pie charts are used to represent data as a proportion or percentage of a whole. They are useful when we want to show how different categories contribute to the overall total or when we want to compare the size of different categories.\nIn this case, we make use of pie chart to show the education level composition of the representative residents (in %). Different colours are also used to represent the different levels of education for easy reference. We also make use of callout extensions (through ggrepel to show the %). The same colour scheme is applied to the callout boxes for easy association.\n\n\nShow the codes\n# create a pie chart to show the proportion of education level across the participants\n\nedu_count  &lt;- single_merged_data %&gt;%\n  group_by(educationLevel) %&gt;%\n  summarize(count = n()) %&gt;% \n  mutate(edu_pie_pct = round(count/sum(count)*100)) %&gt;% \n  mutate(ypos_p = rev(cumsum(rev(edu_pie_pct))),\n         pos_p = edu_pie_pct/2 + lead(ypos_p,1),\n         pos_p = if_else(is.na(pos_p), edu_pie_pct/2, pos_p))\n\nggplot(edu_count, \n       aes(x = \"\" , y = edu_pie_pct, \n           fill = fct_inorder(educationLevel))) +\n  geom_col(width = 1, color = 1) +\n  coord_polar(theta = \"y\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  geom_label_repel(data = edu_count,\n                   aes(y = pos_p, label = paste0(edu_pie_pct, \"%\")),\n                   size = 4.5, nudge_x = 1, color = c(1, 1, 1, 1), \n                   show.legend = FALSE) +\n  guides(fill = guide_legend(title = \"Education Level\")) +\n  labs(title = \"Proportion of Education Level\")+\n  xlab(\"\") + ylab(\"\") + \n  theme(legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5))+\n  theme_void()\n\n\n\n\n\nExplanation Of the total sample size of 880 (after removing the outliers), 45% of them have a Bachelor degree and above education, while the remaining representative residents have lower education of High School/College and below.\n\n\nDesign Consideration We make use of bar chart to show the breakdown of the representative residents’ age instead of pie chart so that we can compare the different age groups side-by-side. Besides the % of composition, the number of representative residents belonging to the respective age groups, are also included.\nDifferent colours are also used to represent the different age groups for easy reference. We also make use of callout extensions above the vertical bars (through ggrepel to show the number of representative residents and the %). The same colour scheme is applied to the callout boxes for easy association. The label of the x-axis ticks were amended for better clarity. In addition, the legend box was excluded as label ticks on the x-axis would have indicated the different groups.\n\n\nShow the codes\n# create a bar chart to show the proportion of age across the representative residents\nage_count  &lt;- single_merged_data %&gt;%\n  group_by(age_bin) %&gt;%\n  summarize(count = n()) %&gt;% \n  mutate(age_pct = round(count/sum(count)*100)) %&gt;% \n  mutate(ypos_p = rev(cumsum(rev(age_pct))),\n         pos_p = age_pct/2 + lead(ypos_p,1),\n         pos_p = if_else(is.na(pos_p), age_pct/2, pos_p))\n\nggplot(age_count, \n       aes(x = fct_inorder(age_bin) , y = count, \n           fill = fct_inorder(age_bin))) +\n  geom_bar(stat = \"identity\", width = 0.5, color = \"black\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  geom_label_repel(data = age_count,\n                   aes(label = paste0(count, \"(\", age_pct, \"%)\")),\n                   size = 4, nudge_x = c(0.1, 0.1, 0.1, 0.1), \n                   nudge_y = c(0, 0.1, 0.1, 0.1), \n                   color = \"grey20\",\n                   show.legend = FALSE) +\n  guides(fill = FALSE) +\n  labs(title = \"Proportion of Representative Residents by Age Group\") +\n  xlab(\"Age Group\") + ylab(\"Count\") + \n  scale_x_discrete(labels = c(\"30 and below\", \"31-40\", \"41-50\", \"Over 50\")) +\n  theme(plot.title = element_text(hjust = 0.5)) \n\n\n\n\n\nExplanation Based on the information presented, majority (30%) of the representatives are 30 years old or younger while the rest of the age groups were rather evenly distributed.\n\n\nDesign Consideration We make use of bar chart to show the breakdown of the representative residents’ happiness index instead of pie chart to facilitate visual comparison of the different happiness index side-by-side. Besides the % of composition, the number of representative residents with the respective happiness index are also included.\nDifferent colours are also used to represent the different groups of happiness index for easy reference. We also make use of callout extensions above the vertical bars (through ggrepel to show the number of representative residents and the %). The same colour scheme is applied to the callout boxes for easy association. The label of the x-axis ticks were amended for better clarity. In addition, the legend box was excluded as label ticks on the x-axis would have indicated the different groups.\n\n\nShow the codes\njov_count  &lt;- single_merged_data %&gt;%\n  group_by(jov_bin) %&gt;%\n  summarize(count = n()) %&gt;% \n  mutate(jov_pct = round(count/sum(count)*100)) %&gt;% \n  mutate(ypos_p = rev(cumsum(rev(jov_pct))),\n         pos_p = jov_pct/2 + lead(ypos_p,1),\n         pos_p = if_else(is.na(pos_p), jov_pct/2, pos_p))\n\nggplot(jov_count, \n       aes(x = fct_inorder(jov_bin) , y = count, \n           fill = fct_inorder(jov_bin))) +\n  geom_bar(stat = \"identity\", width = 0.5, color = \"black\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  geom_label_repel(data = jov_count,\n                   aes(label = paste0(count, \"(\", jov_pct, \"%)\")),\n                   size = 4, nudge_x = c(0.1, 0.1, 0.1, 0.1, 0.1), \n                   nudge_y = c(0.1, 0.1, 0.1, 0.1, 0.1), color = \"grey20\", \n                   show.legend = FALSE) +\n  guides(fill = FALSE) +\n  labs(title = \"Happiness Index of Representative Residents\") +\n  xlab(\"Happiness Index (from 0 to 1)\") + ylab(\"Count\") + \n  scale_x_discrete(labels = c(\"0.2 and below\", \"0.21-0.4\", \n                              \"0.41-0.6\", \"0.61-0.8\", \"Above 0.8\")) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nExplanation From the chart above, most of the representative residents (66%) are not very happy (i.e. 0.6 and below), with only 300 out of the 880 representatives (34%) are happy (Happiness Index from 0.61 onwards).\n\n\n\n\n\n4.1.2. Total Income and Savings with reference to Education Level\nDesign Consideration Scatterplots are generally used to show the relationship between two continuous variables.\nThe original intent was to include more interactive interface by making use DT (data table) and crosstalk packages to allow users to click on a point on the scatterplot to view more information about the selected representative resident. However, the idea was aborted as the points were scattered widely across the x-axis. If we were to include a DT table beside it, the scatterplot will become very small and cramped up. As such, the additional details on the selected representative resident were presented as hovertext instead.\nFor the graph below, we will compare the Total Income against the Total Savings of the representative residents. Users can select/unselect the different education levels to show on the graph by clicking on the legend on the right side of the chart. The actual total savings and income figures will also be displayed as hovering text when the cursor moves over the points.\n\n\nShow the codes\nfig &lt;- plot_ly(data = single_merged_data,\n               type=\"scatter\",\n               x = ~Total_sumEarn,\n               y = ~Total_saving,\n               color = ~educationLevel,\n               # Hover text:\n               text = ~paste(\"&lt;br&gt;Age: \", age, \n                             \"&lt;br&gt;Have Kids?: \", haveKids, \n                             \"&lt;br&gt;Household Size: \", householdSize, \n                             \"&lt;br&gt;Total Income: $\", Total_sumEarn,\n                             \"&lt;br&gt;Total Expenses: $\", Total_sumExp,\n                             \"&lt;br&gt;Total Savings: $\", Total_saving))\n\nfig &lt;- fig %&gt;% \n  layout(title = \"\\nTotal Income and Savings of Representative Residents\\n\",\n         yaxis = list(zeroline = FALSE, title = \"Total Savings ($)\\n\",\n                      titlefont = list(weight = \"bold\")),\n         xaxis = list(zeroline = FALSE, title = \"\\nTotal Income ($)\\n\",\n                      titlefont = list(weight = \"bold\")))\n\nfig\n\n\n\n\n\n\nExplanation Based on the interactive chart above, we observed participants with higher levels of education tend to earn more and save more, as opposed to those with lower levels of education.\n\n\n4.1.3. Total Income, Expenditure and Savings for the period of study\nDesign Consideration Ridgeline plot is a set of overlapped density plots, which could help us compare distributions of the dataset. They are useful for visualizing changes in distributions over time. As such, we will make use of such plots to present 3 sets of information for the period of study (i.e. Mar 2022 to Feb 2023).\n\nTotal Income by Education LevelTotal Expenditures by Education Level\n\n\n\n\nShow the codes\nggplot(data = expanded_merged_data, \n       aes(x = sum_earning, y = educationLevel, fill = after_stat(x))) +\n  \n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +\n  \n  theme_minimal() +\n  \n    labs(title = 'Total Income by Education Level: {frame_time}',\n       y = \"Education Level\",\n       x = \"Total Income\") +\n  \n  theme(legend.position=\"none\",\n  text = element_text(family = \"Arial\"),\n  plot.title = element_text(face = \"bold\", size = 12),\n  \n  axis.title.x = element_text(size = 10, hjust = 1),\n  axis.title.y = element_text(size = 10, angle = 360, vjust=1),\n  axis.text = element_text(size = 8)) +\n  \n  scale_fill_viridis(name = \"sum_earning\", option = \"H\") +\n\n  transition_time(expanded_merged_data$YearMth) +\n  ease_aes('linear')\n\n\n\n\n\nExplanation\n\nRepresentative Residents with Graduates education have the highest income.\nThe income for all education levels remains consistent across the entire year.\nThere was a sharp increase in the income across all education levels in the month of March 2022.\n\n\n\n\n\nShow the codes\nggplot(data = expanded_merged_data, \n       aes(x = sum_expense, y = educationLevel, fill = after_stat(x))) +\n  \n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +\n  \n  theme_minimal() +\n  \n    labs(title = 'Total Expenditures by Education Level: {frame_time}',\n       y = \"Education Level\",\n       x = \"Total Expenditure\") +\n  \n  theme(legend.position=\"none\",\n  text = element_text(family = \"Arial\"),\n  plot.title = element_text(face = \"bold\", size = 12),\n  \n  axis.title.x = element_text(face = \"bold\", size = 10, hjust = 1),\n  axis.title.y = element_text(face = \"bold\", size = 10, angle = 360, vjust=1),\n  axis.text = element_text(size = 8)) +\n  \n  scale_fill_viridis(name = \"sum_expense\", option = \"C\") +\n\n  transition_time(expanded_merged_data$YearMth) +\n  ease_aes('linear')\n\n\n\n\n\nExplanation\n\nThere was no significant difference in the spending across the different education levels.\nThe Representative Residents generally spent &lt;$3000 per month.\nIt was observed that there was a sharp increase of expenditure across all education levels in the month of March and could reach as high as $8000.\n\n\n\n\n\n\n4.1.4. Analysis of Monthly Income and Spending\nDesign Consideration We use an interactive scatterplot to analyse the monthly income with the spending of the representative residents on a monthly basis.\nFor the graph below, the x-axis represents the monthly income, while the y-axis shows the monthly expense as a percentage (over the monthly income). The label ticks of y-axis was intentionally left to exceed 100% (anything over 100% will imply that the resident overspent for that particular month. Different colours of the bubbles are used to represent the different education levels. Additional information such as the Age, Household Size and whether the resident has kids will also be shown when the user mouseover the points.\n\n\nShow the codes\nbp &lt;- expanded_merged_data %&gt;%\n  plot_ly(x = ~sum_earning, \n          y = ~expense_percent, \n          sizes = c(2, 100),\n          color = ~educationLevel, \n          frame = ~YearMonth, \n          text = ~paste(\"&lt;br&gt;Representative Resident Id: \", participantId,\n                        \"&lt;br&gt;Age: \", age,\n                        \"&lt;br&gt;Education Level: \", educationLevel,\n                        \"&lt;br&gt;Have Kids: \", haveKids,\n                        \"&lt;br&gt;Household Size: \", householdSize,\n                        \"&lt;br&gt;Monthly Income: $\", sum_earning, \n                        \"&lt;br&gt;Monthly Expense: $\", sum_expense,\n                        \"&lt;br&gt;% of Monthly Income Spent: \", expense_percent, \"%\"),\n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(title = list(text= \"\\nMonthly Income vs % of Monthly Income Spent\\n\",\n                      weight=\"bold\"),\n         xaxis = list(title = \"\\nMonthly Income\", \n                      titlefont = list(weight = \"bold\")),\n         yaxis = list(title = \"% of Monthly Expenses over Income\\n\",\n                      titlefont = list(weight = \"bold\")),\n         showlegend = FALSE)\nbp\n\n\n\n\n\n\nExplanation From the graph, it appeared that there were some overspendings among the residents across the months. These residents typically belong to the lower education groups."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#one-sample-test",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#one-sample-test",
    "title": "Take-Home Exercise 01",
    "section": "4.2. One-sample test",
    "text": "4.2. One-sample test\n\nTotal EarningsTotal Earnings\n\n\n\n\nCode\nset.seed(1234)\n\ngghistostats(\n  data = single_merged_data,\n  x = Total_sumEarn,\n  type = \"bayes\",\n  test.value = 15000,\n  xlab = \"Total Earnings\"\n)\n\n\n\n\n\n\n\n\n\nCode\nset.seed(1234)\n\ngghistostats(\n  data = single_merged_data,\n  x = Total_sumExp,\n  type = \"bayes\",\n  test.value = 15000,\n  xlab = \"Total Expenses\"\n)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#two-sample-mean-test",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#two-sample-mean-test",
    "title": "Take-Home Exercise 01",
    "section": "4.2. Two-sample Mean Test",
    "text": "4.2. Two-sample Mean Test\nThe objective of a two-sample mean test is to compare the means of two independent samples and determine if they are significantly different from each other.\n\n\nShow the codes\nggbetweenstats(\n  data = single_merged_data,\n  x = haveKids, \n  y = Total_sumExp,\n  type = \"np\",\n  messages = FALSE\n) +\n  ylab(\"\\nTotal Expenditure\") +\n  xlab(\"\\nHave Kids?\") +\n  ggtitle(\"Total Expenditure of Representative Resides without and with Kids\\n\") +\n  theme(plot.title = element_text(hjust = 0.5)) \n\n\n\n\n\nExplanation The above violin plot shows that 247 representative residents with kids tend to spend more (approximately $2450) compared to the remaining 633 without kids.\nSince the p-value is very small (less than the significance level of 0.05), we can reject the null hypothesis that having kids will result in higher expenditure and conclude that there is a significant difference between the means of the two samples.\nHowever, we also noted that \\(\\hat{r}\\) value of -0.38 would indicate a moderate negative correlation between the two samples. This suggests that there may be a relationship between the samples, such that as one sample increases, the other sample decreases."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#the-correlation-between-wages-expenditures-and-age",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#the-correlation-between-wages-expenditures-and-age",
    "title": "Take-Home Exercise 01",
    "section": "4.5. The correlation between Wages, Expenditures and Age",
    "text": "4.5. The correlation between Wages, Expenditures and Age\n\nTotal Income and Total SavingAge and Wage Correlation\n\n\n\n\nCode\nggscatterstats(data = single_merged_data,\n               x = Total_sumEarn,\n               y = Total_saving,\n               marginal = FALSE) +\n  theme_minimal() +\n  labs(title=\"Correlation of Total Income and Savings\",\n       x = \"Total Income\",\n       y = \"Total Savings\") +\n  \n  theme(text = element_text(family = \"Arial\"),\n        plot.title = element_text(hjust = 0.2, size = 15, face = 'bold'),\n        plot.margin = margin(20, 20, 20, 20),\n        legend.position = \"bottom\",\n        axis.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\")) \n\n\n\n\n\nWith a $\\hat{r}$ value of 0.99, it indicates that there is a very strong positive linear relationship between the Total Income and Total Savings (more income, more savings).\n\n\n\n\nCode\nggscatterstats(data = single_merged_data,\n               x = age,\n               y = Total_Wage,\n               marginal = FALSE) +\n  theme_minimal() +\n  labs(title=\"Correlation of Total Wages and Age\",\n       x = \"Age\",\n       y = \"Total wage\") +\n  \n  theme(text = element_text(family = \"Arial\"),\n        plot.title = element_text(hjust = 0.2, size = 15, face = 'bold'),\n        plot.margin = margin(20, 20, 20, 20),\n        legend.position = \"bottom\",\n        axis.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\nGiven that the $\\hat{r}$ value is -0.03, there is weak negative correlation between the Age and Wage. This means that as the age increases, the wage tends to decrease, even though the relationship is not very strong.\n\n\n\n\n\nCode\n# create a data frame\n# plot the box plot\nggplot(single_merged_data, aes(x = jov_bin, y = Total_sumExp)) +\n  geom_boxplot() +\n  labs(x = \"Joviality\", y = \"Total Expenditure\", title = \"Box Plot of Joviality and Total Expenditure\")\n\n\n\n\n\n\n\nCode\n#bubble plot - is it useful???\nbp &lt;- expanded_merged_data %&gt;%\n  plot_ly(x = ~sum_expense, \n          y = ~sum_earning, \n          size = ~householdSize, \n          color = ~educationLevel, \n          frame = ~YearMonth, \n          text = ~age, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          )\nbp\n\n\n\n\n\n\n\n\nCode\n#bubble plot - is it useful???\n# bp &lt;- expanded_merge_data %&gt;%\n#   plot_ly(x = ~sum_expense, \n#           y = ~sum_earning, \n#           size = ~householdSize, \n#           color = ~educationLevel, \n#           frame = ~YearMonth, \n#           text = ~age, \n#           hoverinfo = \"text\",\n#           type = 'scatter',\n#           mode = 'markers'\n#           )\n# bp"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#the-correlation-between-total-earnings-expenditures-and-savings",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#the-correlation-between-total-earnings-expenditures-and-savings",
    "title": "Take-Home Exercise 01",
    "section": "4.5. The correlation between Total Earnings, Expenditures and Savings",
    "text": "4.5. The correlation between Total Earnings, Expenditures and Savings\nThe correlation charts show that there is weak negative relationship between the Total Earnings and Total Expenditure. In addition, there is a very strong relationship between the Total Earnings and Total Savings.\n\nTotal Earnings and Total SavingTotal Earnings and Total Spending\n\n\n\n\nCode\nggscatterstats(data = single_merged_data,\n               x = Total_sumEarn,\n               y = Total_saving,\n               marginal = FALSE) +\n  theme_minimal() +\n  labs(title=\"Correlation of Total Earnings and Savings\",\n       x = \"Total Earnings\",\n       y = \"Total Savings\") +\n  \n  theme(text = element_text(family = \"Arial\"),\n        plot.title = element_text(hjust = 0.2, size = 15, face = 'bold'),\n        plot.margin = margin(20, 20, 20, 20),\n        legend.position = \"bottom\",\n        axis.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nCode\nggscatterstats(data = single_merged_data,\n               x = Total_sumExp,\n               y = Total_sumEarn,\n               marginal = FALSE) +\n  theme_minimal() +\n  labs(title=\"Correlation of Total Expenditure and Total Earnings\",\n       x = \"Total Expenditure\",\n       y = \"Total Earnings\") +\n  \n  theme(text = element_text(family = \"Arial\"),\n        plot.title = element_text(hjust = 0.2, size = 15, face = 'bold'),\n        plot.margin = margin(20, 20, 20, 20),\n        legend.position = \"bottom\",\n        axis.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\nCode\n# create a data frame\n# plot the box plot\nggplot(single_merged_data, aes(x = jov_bin, y = Total_sumExp)) +\n  geom_boxplot() +\n  labs(x = \"Joviality\", y = \"Total Expenditure\", title = \"Box Plot of Joviality and Total Expenditure\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#plotting-ridgeline-graph",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#plotting-ridgeline-graph",
    "title": "Take-Home Exercise 01",
    "section": "4.6. Plotting ridgeline graph",
    "text": "4.6. Plotting ridgeline graph\n\n\nCode\nggplot(single_merged_data, \n       aes(x = Total_sumExp, \n           y = educationLevel)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    #fill = lighten(\"#7097BB\", .3),\n    fill = \"lightblue\",\n    color = \"grey30\"\n  ) +\n  scale_x_continuous(\n    name = \"Total Spending on Shelter\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nCode\nggplot(data = expanded_merge_data, aes(x = sum_expense, y = educationLevel, fill = after_stat(x))) +\n  \n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +\n  \n  theme_minimal() +\n  \n    labs(title = 'Total Expenditures by Education Level: {frame_time}',\n       y = \"Education Level\",\n       x = \"Total Expenditure\") +\n  \n  theme(legend.position=\"none\",\n  text = element_text(family = \"Arial\"),\n  plot.title = element_text(face = \"bold\", size = 12),\n  \n  axis.title.x = element_text(size = 10, hjust = 1),\n  axis.title.y = element_text(size = 10),\n  axis.text = element_text(size = 8)) +\n  \n  scale_fill_viridis(name = \"sum_expense\", option = \"D\") +\n\n  #transition_time(expanded_merge_data$YearMthDay) +\n  transition_time(expanded_merge_data$YearMth) +\n  ease_aes('linear')\n\n\n\n\n\n\n\nCode\nggplot(data = expanded_merge_data, aes(x = sum_earning, y = educationLevel, fill = after_stat(x))) +\n  \n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +\n  \n  theme_minimal() +\n  \n    labs(title = 'Total Income by Education Level: {frame_time}',\n       y = \"Education Level\",\n       x = \"Total Income\") +\n  \n  theme(legend.position=\"none\",\n  text = element_text(family = \"Arial\"),\n  plot.title = element_text(face = \"bold\", size = 12),\n  \n  axis.title.x = element_text(size = 10, hjust = 1),\n  axis.title.y = element_text(size = 10),\n  axis.text = element_text(size = 8)) +\n  \n  scale_fill_viridis(name = \"sum_earning\", option = \"D\") +\n\n  #transition_time(expanded_merge_data$YearMthDay) +\n  transition_time(expanded_merge_data$YearMth) +\n  ease_aes('linear')\n\n\n\n\n\n\n\nCode\nggplot(data = expanded_merge_data, aes(x = saving, y = educationLevel, fill = after_stat(x))) +\n  \n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +\n  \n  theme_minimal() +\n  \n    labs(title = 'Savings by Education Level: {frame_time}',\n       y = \"Education Level\",\n       x = \"Savings\") +\n  \n  theme(legend.position=\"none\",\n  text = element_text(family = \"Arial\"),\n  plot.title = element_text(face = \"bold\", size = 12),\n  \n  axis.title.x = element_text(size = 10, hjust = 1),\n  axis.title.y = element_text(size = 10),\n  axis.text = element_text(size = 8)) +\n  \n  scale_fill_viridis(name = \"saving\", option = \"G\") +\n\n  #transition_time(expanded_merge_data$YearMthDay) +\n  transition_time(expanded_merge_data$YearMth) +\n  ease_aes('linear')"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#one-sample-means-test",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#one-sample-means-test",
    "title": "Take-Home Exercise 01",
    "section": "4.2. One-sample Means Test",
    "text": "4.2. One-sample Means Test\nThe objective of a one-sample mean test is to determine if the mean of a sample is statistically significantly different from a known population mean. The null hypothesis is that there is no difference between the sample mean and the population mean, and the alternative hypothesis is that there is a difference. This type of test is commonly used in research to determine if a sample is representative of a larger population. By performing a one-sample mean test, researchers can determine if the sample is significantly different from the population and draw conclusions about the population based on the sample.\n\nTotal IncomeTotal Expenditure\n\n\n\n\nShow the codes\nset.seed(1234)\n\ngghistostats(\n  data = single_merged_data,\n  x = Total_sumEarn,\n  type = \"bayes\",\n  test.value = 15000,\n  xlab = \"Total Income\"\n)\n\n\n\n\n\n\n\n\n\nShow the codes\nset.seed(1234)\n\ngghistostats(\n  data = single_merged_data,\n  x = Total_sumExp,\n  type = \"bayes\",\n  test.value = 15000,\n  xlab = \"Total Expenses\"\n)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#one-way-anova-test",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#one-way-anova-test",
    "title": "Take-Home Exercise 01",
    "section": "4.3 One-Way ANOVA Test",
    "text": "4.3 One-Way ANOVA Test\nOne-Way ANOVA (Analysis of Variance) test is used to determine whether there is a statistically significant difference between the means of three or more independent groups. The purpose of conducting a One-Way ANOVA test is to determine whether the variation in the response variable (dependent variable) is due to the variation in the factor being tested (independent variable) or whether it is simply due to chance.\nWe will conduct a One-Way ANOVA test on the Total Expenditures (dependent variable) with different levels of education (independent variable).\n\n\nShow the codes\nggbetweenstats(\n  data = single_merged_data,\n  x = educationLevel, \n  y = Total_sumExp,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE) +\n\n  ggtitle(\"One-Way ANOVA Test for Total Expenditure by Education Level\\n\") + \n  theme(plot.title = element_text(hjust = 0.5)) +\n  xlab(\"\\nEducation Level\") + \n  ylab(\"Total Expenditure\\n\")\n\n\n\n\n\nExplanation A p-value of 0.03 means that there is a 3% probability of obtaining a test statistic as extreme or more extreme than the one observed, assuming the null hypothesis is true. This suggests that there is some evidence against the null hypothesis and that the observed differences in means between the groups may be statistically significant. It is also observed that the median values are slightly different across the different education levels."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#combine-the-2-data-sets",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#combine-the-2-data-sets",
    "title": "Take-Home Exercise 01",
    "section": "3.5. Combine the 2 data sets",
    "text": "3.5. Combine the 2 data sets\nWe will combine the finalised data sets of Participants and FinancialJournal into 1 single data frame for ease of analysis.\nFor the purpose of different analysis to be performed, we will create 2 sets of merged data:\n\nsingle_merged_data will have 1 row of record per participant\nexpanded_merged_data will comprise of a breakdown of the transactions by month for each participant\n\n\n\nShow the codes\n# Merge the two files based on the 'participantId' column\nsingle_merged_data &lt;- merge(participants, \n                            final_finjournal_single, \n                            by = \"participantId\")\n\n# expanded_merged_data contains a breakdown of the monthly transactions for each participant\nexpanded_merged_data &lt;- merge(participants, final_finjournal_wide, by = \"participantId\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#perform-data-wrangling-on-financial-journal-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#perform-data-wrangling-on-financial-journal-data",
    "title": "Take-Home Exercise 01",
    "section": "3.4. Perform Data Wrangling on financial journal data",
    "text": "3.4. Perform Data Wrangling on financial journal data\nAs part of the data wrangling process, we will also check the data set for duplicate records and remove them (if there is any).\n\n\nShow the codes\n# Load the FinancialJournal.csv into the environment\nfinjournal_raw &lt;- read_csv(\"data/FinancialJournal.csv\")\n\n# Check if 'finjournal' contains any duplicate rows\nhas_duplicates &lt;- any(duplicated(finjournal_raw))\n\nif (has_duplicates) {\n  # contains duplicate rows\n  \n  # Remove duplicate records from the 'finjournal_raw' data frame\n  finjournal &lt;- distinct(finjournal_raw)\n} else {\n  # No duplicate rows\n  # if there are no duplicate rows, we will just replicate finjournal_raw data for consistency\n  finjournal &lt;- finjournal_raw\n}\nfinjournal\n\n\n# A tibble: 1,512,523 × 4\n   participantId timestamp           category  amount\n           &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;      &lt;dbl&gt;\n 1             0 2022-03-01 00:00:00 Wage      2473. \n 2             0 2022-03-01 00:00:00 Shelter   -555. \n 3             0 2022-03-01 00:00:00 Education  -38.0\n 4             1 2022-03-01 00:00:00 Wage      2047. \n 5             1 2022-03-01 00:00:00 Shelter   -555. \n 6             1 2022-03-01 00:00:00 Education  -38.0\n 7             2 2022-03-01 00:00:00 Wage      2437. \n 8             2 2022-03-01 00:00:00 Shelter   -557. \n 9             2 2022-03-01 00:00:00 Education  -12.8\n10             3 2022-03-01 00:00:00 Wage      2367. \n# ℹ 1,512,513 more rows\n\n\nLooking at the finjournal data set above, we noticed that there are a few problems that we need to resolve before we can perform analysis on them\n\nparticipantId is in &lt;dbl&gt; format. Since this is used to identify each participant uniquely, we will convert it to a string factor\ntimestamp is in &lt;POSIXct&gt; format. For the purpose of analysis, we will extract the month and year from this data item.\ncategory is in &lt;chr&gt; format. We will convert it to a string factor\namount values are too granular. We will round it to 2 decimal places. There are also negative values, which we will convert all to absolute values instead.\n\n\n\nShow the codes\n# Convert participantId, category columns into string factor \nfinjournal$participantId &lt;- as.factor(as.character(finjournal$participantId))\nfinjournal$category &lt;- as.factor(as.character(finjournal$category))\n\n# Extract the date component from timestamp column\nfinjournal$date &lt;- as.Date(finjournal$timestamp)\n\n# Extract the month and year from the date component\nfinjournal$YearMonth &lt;- format(finjournal$date, \"%Y-%m\")\n\n# Convert the negative amount values to absolute values and round to 2 decimal places\nfinjournal$amount &lt;- round(abs(finjournal$amount), 2)\n\n\n\n3.4.1. Checking the quality of financial journal data\nThere are often dirty data that we will need to cleanse before performing any data analysis. This portion will check on the quality of the financial journal data.\n\n3.4.1.1. Identifying any possible outliers\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\n\n# group the finjournal data by participantId and get the transaction count\n# for each participant. We will also arrange the transaction count in \n# ascending order.\nfinjournal_grp &lt;- finjournal %&gt;%\n  group_by(participantId) %&gt;%\n  summarise(transaction_cnt = n()) %&gt;%\n  arrange(transaction_cnt)\n\n# Plotting a histogram with the data extracted for better visualisation\nplot &lt;- ggplot(data = finjournal_grp, \n               aes(x = transaction_cnt)) +\n  geom_histogram(color=\"gray30\", fill=\"deepskyblue3\") + \n  ylim(0, 150) +\n  ggtitle(\"Distribution of Transactions by Participants\") + \n  xlab(\"Transaction Count\") +\n  ylab(\"Number of Participants\") + \n  theme_light() +\n  theme(plot.title = element_text(hjust = 0.5, face=\"bold\"),\n        axis.title.x = element_text(face = \"bold\"),\n        axis.title.y = element_text(face = \"bold\"))\n\nplot &lt;- ggplotly(plot, tooltip = c(\"y\"))\n\nplot\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs you noticed from the chart above, there is a number of participants (131 of them in total), who have a significantly low number of transaction records, which is far off from the main distribution. As such, we will treat these as outliers and remove them from our analysis.\n\n\n\n\n3.4.1.2. Removing the Outliers\nWe will proceed to remove the 131 participants who have significantly low number of transaction records, compared to the rest.\n\n\nShow the codes\n# Since we have arrange the number of transactions in ascending order in the \n# earlier code chunks, we can just get the transaction count for the last \n# record of the 131 outliers. \ntxn_count &lt;- finjournal_grp[131, \"transaction_cnt\"]\n\n# Extract the participants without the 131 participants by the transaction count\nfinjournal_grp_filtered &lt;- finjournal_grp %&gt;%\n  filter(transaction_cnt &gt; as.integer(txn_count))\n\n# Remove the 131 participants from the finjournal\nfinjournal_filtered &lt;- merge(finjournal, finjournal_grp_filtered, by = \"participantId\")\n\n\n\n\n3.4.1.3. Transposing the Category Into Individual Columns\nIn this section, we will be performing a series of data wrangling activities for each participant:\n\nsum up all the daily transactions amount based on the respective categories\ntranspose the transaction categories into multiple columns (Education, Food, Recreation, Shelter, Wage, RentAdjustment)\nreplace all the NA values to 0\nderive total monthly expenses and total monthly earnings\nderive monthly savings (earnings - expenses)\n\n\n\nShow the codes\n# Group the new filtered dataset by ParticipantId, Category and YearMonth, then\n# sum up all the daily transactions amount based on the respective categories\nfinal_finjournal &lt;- finjournal_filtered %&gt;% \n  group_by(participantId, category, YearMonth) %&gt;% \n  summarise(Total = sum(amount))\n\n# Transpose the categories to columns \nfinal_finjournal_wide &lt;- pivot_wider(final_finjournal, \n                               names_from = category, \n                               values_from = Total)\n\n# Replace all the NA values to 0\nfinal_finjournal_wide[is.na(final_finjournal_wide)] &lt;- 0.0\n\n# Derive the total monthly expenses, total monthly earnings (income), \n# monthly savings, % of monthly expenses (over monthly income) \n# for each participant \nfinal_finjournal_wide$sum_expense &lt;- final_finjournal_wide$Education + \n                                     final_finjournal_wide$Food +\n                                     final_finjournal_wide$Recreation + \n                                     final_finjournal_wide$Shelter\n\nfinal_finjournal_wide$sum_earning &lt;- final_finjournal_wide$Wage + \n                                     final_finjournal_wide$RentAdjustment\n\nfinal_finjournal_wide$saving &lt;- final_finjournal_wide$sum_earning -\n                                final_finjournal_wide$sum_expense\n\nfinal_finjournal_wide$expense_percent &lt;-\n  round(final_finjournal_wide$sum_expense/final_finjournal_wide$sum_earning*100, 2)\n\nfinal_finjournal_wide$YearMthDay &lt;- \n  as.Date(paste0(final_finjournal_wide$YearMonth,\"-01\"))\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe will aggregate all the monthly transactions by participants, since final_finjournal will contain the detailed YearMonth for each participant.\n\n\n\n\nShow the codes\n# To have 1 financial journal record per participant \n\nfinal_finjournal_single &lt;- final_finjournal_wide %&gt;% \n  group_by(participantId) %&gt;% \n  summarise(Total_Edu = sum(Education), \n            Total_Food = sum(Food),\n            Total_Rec = sum(Recreation),\n            Total_Shelter = sum(Shelter),\n            Total_Wage = sum(Wage),\n            Total_RentAdj = sum(RentAdjustment),\n            Total_sumExp = sum(sum_expense),\n            Total_sumEarn = sum(sum_earning),\n            Total_saving = sum(saving))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#correlation-tests",
    "href": "Take-home_Ex/Take-home_Ex01/Take-Home_Ex01.html#correlation-tests",
    "title": "Take-Home Exercise 01",
    "section": "4.4. Correlation Tests",
    "text": "4.4. Correlation Tests\nCorrelation tests are performed to investigate the strength and direction of a relationship between two variables. It helps to determine if the variables are related and, if so, how strongly they are related. As such, we will conduct 2 correlation tests here. The first correlation test will look at the relationship between Total Income and Total Savings; the second correlation test will examine the relationship between Age and Wages.\n\nTotal Income and Total Saving Correlation TestAge and Wage Correlation Test\n\n\nAs we have seen in Section 4.1.2 (Total Income and Savings with reference to Education Level), it seems that the more income a representative resident earn, the more he/she saves. We will examine the correlation coefficient to see if this is indeed true.\n\n\nShow the codes\nggscatterstats(data = single_merged_data,\n               x = Total_sumEarn,\n               y = Total_saving,\n               marginal = FALSE) +\n  theme_minimal() +\n  labs(title=\"Correlation of Total Income and Savings\",\n       x = \"Total Income\",\n       y = \"Total Savings\") +\n  \n  theme(text = element_text(family = \"Arial\"),\n        plot.title = element_text(hjust = 0.2, size = 15, face = 'bold'),\n        plot.margin = margin(20, 20, 20, 20),\n        legend.position = \"bottom\",\n        axis.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\")) \n\n\n\n\n\nExplanation With a \\(\\hat{r}\\) value of 0.99, it indicates that there is a very strong positive linear relationship between the Total Income and Total Savings (more income, more savings).\n\n\n\n\nShow the codes\nggscatterstats(data = single_merged_data,\n               x = age,\n               y = Total_Wage,\n               marginal = FALSE) +\n  theme_minimal() +\n  labs(title=\"Correlation of Total Wages and Age\",\n       x = \"Age\",\n       y = \"Total wage\") +\n  \n  theme(text = element_text(family = \"Arial\"),\n        plot.title = element_text(hjust = 0.2, size = 15, face = 'bold'),\n        plot.margin = margin(20, 20, 20, 20),\n        legend.position = \"bottom\",\n        axis.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\nExplanation Given that the \\(\\hat{r}\\) value is -0.03, there is weak negative correlation between the Age and Wage. This means that as the age increases, the wage tends to decrease, even though the relationship is not very strong."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608 Visual Analytics & Applications",
    "section": "",
    "text": "Welcome to Ong Chae Hui’s web portal for the works done in ISSS608 Visual Analytics and Applications.\nIn this website, you will find all the coursework (Hands-On, In-Class and Take-Home Exercises) prepared in this course."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "pacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\n\nRows: 54 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): label, Department, Title\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\nRows: 9063 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): SentDate, Subject, MainSubject, sourceLabel, targetLabel\ndbl  (2): source, target\ntime (1): SentTime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#installing-and-launching-the-r-packages",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#installing-and-launching-the-r-packages",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "pacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#importing-network-data-from-files",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#importing-network-data-from-files",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "GAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\n\nRows: 54 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): label, Department, Title\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\nRows: 9063 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): SentDate, Subject, MainSubject, sourceLabel, targetLabel\ndbl  (2): source, target\ntime (1): SentTime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#reviewing-the-imported-data",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#reviewing-the-imported-data",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "glimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…"
  },
  {
    "objectID": "VAST-2023/MC1/MC1.html",
    "href": "VAST-2023/MC1/MC1.html",
    "title": "MC1",
    "section": "",
    "text": "pacman::p_load(jsonlite, tidygraph, ggraph, visNetwork, tidyverse)\n\n\nMC1 &lt;- fromJSON(\"data/MC1.json\")\n\n\nMC1_nodes &lt;- as_tibble(MC1$nodes) %&gt;%\n  select(id, type, country)\n\n\nMC1_edges &lt;- as_tibble(MC1$links) %&gt;%\n  select(source, target, type, weight, key)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website consists of a collection of the works done in ISSS608 Visual Analytics & Applications course."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, then they will be launched into R.\n\n\nCode\npacman::p_load(tidyverse)\n\n\n\n\n\n\n\nCode\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#getting-started",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, then they will be launched into R.\n\n\nCode\npacman::p_load(tidyverse)\n\n\n\n\n\n\n\nCode\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#r-graphics-vs-ggplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#r-graphics-vs-ggplot",
    "title": "Hands-on Exercise 1",
    "section": "2. R Graphics VS ggplot",
    "text": "2. R Graphics VS ggplot\n\n2.1. Plotting Graphics using R Graphics\n\n\nCode\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\n2.2. Plotting Graphics using ggplot\n\n\nCode\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10,\n                 boundary=100,\n                 color=\"black\",\n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\n\n2.2.1. Essential Grammatical Elements in ggplot2: data, showing empty canvas\n\n\nCode\nggplot(data=exam_data)\n\n\n\n\n\n\n\n2.2.2. Essential Grammatical Elements in ggplot2: Aesthetic mappings, showing x-axis and y-axis\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS))\n\n\n\n\n\n\n\n\n2.3. Essential Grammatical Elements in ggplot2: geom\nGeometric objects are the actual marks we put on a plot. Examples include: - geom_point for drawing individual points (e.g., a scatter plot) - geom_line for drawing lines (e.g., for a line charts) - geom_smooth for drawing smoothed lines (e.g., for simple trends or approximations) - geom_bar for drawing bars (e.g., for bar charts) - geom_histogram for drawing binned values (e.g. a histogram) - geom_polygon for drawing arbitrary shapes geom_map for drawing polygons in the shape of a map! (You can access the data to use for these maps by using the map_data() function).\n\n2.3.1. Essential Grammatical Elements in ggplot2: geom_bar, showing bar charts\n\n\nCode\nggplot(data=exam_data,\n       aes(x = RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\n2.3.2. Essential Grammatical Elements in ggplot2: geom_dotplot\nNote that the y-scale is not very useful and is very misleading\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS)) +\n  geom_dotplot(dotsize = 0.5)\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n2.3.3. Essential Grammatical Elements in ggplot2: geom_dotplot, without y-scale\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS)) +\n  geom_dotplot(binwidth = 2.5,\n               dotsize = 0.5) + \n  scale_y_continuous(NULL, breaks = NULL)\n\n\n\n\n\n\n\n2.3.4. Essential Grammatical Elements in ggplot2: geom_histogram\nDefault bin is 30\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n2.3.5. Essential Grammatical Elements in ggplot2: geom_dotplot, changing the defaults\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS)) +\n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\")\n\n\n\n\n\n\n\n2.3.6. Modifying a geometric object by changing aes()\nCan also be used to colour, fill and alpha of the geometric\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS,\n           fill = GENDER)) +\n  geom_histogram(bins=20,\n                 color=\"grey30\")\n\n\n\n\n\n\n\n2.3.7. Geometric Objects: geom-density()\ngeom-density() computes and plots kernel density estimate, which is a smoothed version of the histogram.\nIt is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\nThe code below plots the distribution of Maths scores in a kernel density estimate plot.\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS)) +\n  geom_density()\n\n\n\n\n\nUsing colour or fill arguments of aes()\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS,\n           color = GENDER)) +\n  geom_density()\n\n\n\n\n\n\n\n2.3.8. Geometric Objects: geom_boxplot\ngeom_boxplot() displays continuous value list. It visualises five summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually.\nThe code chunk below plots boxplots by using geom_boxplot().\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS,\n           colour = GENDER)) +\n  geom_boxplot()\n\n\n\n\n\nNotches are used in box plots to help visually assess whether the medians of distributions differ. If the notches do not overlap, this is evidence that the medians are different.\n\n\nCode\nggplot(data=exam_data,\n       aes(y = MATHS,\n           x= GENDER)) + \n  geom_boxplot(notch = TRUE)\n\n\n\n\n\n\n\n2.3.9. Geometric Objects: geom_violin()\ngeom_violin is designed for creating violin plot. Violin plots are a way of comparing multiple data distributions. With ordinary density curves, it is difficult to compare more than just a few distributions because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they’re placed side by side.\n\n\nCode\nggplot(data=exam_data,\n       aes(y = MATHS,\n           x = GENDER)) +\n  geom_violin()\n\n\n\n\n\n\n\n2.3.10. Geometric Objects: geom_point()\ngeom_point() is especially useful for creating scatterplot.\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS,\n           y = ENGLISH)) +\n  geom_point()\n\n\n\n\n\n\n\n2.3.11. geom objects can be combined\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)\n\n\n\n\n\n\n\n\n2.4. Essential Grammatical Elements in ggplot2: stat\nThe Statistics functions statistically transform data, usually as some form of summary. For example:\n\nfrequency of values of a variable (bar graph)\n\na mean\na confidence limit\n\nThere are two ways to use these functions:\n\nadd a stat_() function and override the default geom, or\nadd a geom_() function and override the default stat.\n\n\n\n2.4.1. Working with stat()\nThe boxplots below are incomplete because the positions of the means were not shown.\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n2.4.2. Working with stat - the stat_summary() method\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() + \n  stat_summary(geom = \"point\",\n               fun.y=\"mean\",\n               colour=\"red\",\n               size=4)\n\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\n\n\n2.4.3. Working with stat - the geom() method\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() + \n  geom_point(stat=\"summary\",\n               fun.y=\"mean\",\n               colour=\"red\",\n               size=4)\n\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\n\n\n2.4.4. Adding a best fit curve on a scatterplot\nThe scatterplot below shows the relationship of Maths and English grades of pupils. The interpretability of this graph can be improved by adding a best fit curve.\nBefore adding the best fit curve\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS,\n           y = ENGLISH)) +\n  geom_point()\n\n\n\n\n\nAfter adding the best fit curve\nNote that the default method used is loess.\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS,\n           y = ENGLISH)) +\n  geom_point() + \n  geom_smooth(size=0.5)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe default smoothing method can be overridden as shown below\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS,\n           y = ENGLISH)) +\n  geom_point() + \n  geom_smooth(method=lm,\n              size=0.5)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n2.5. Essential Grammatical Elements in ggplot2: Facets\nFacetting generates small multiples (sometimes also called trellis plot), each displaying a different subset of the data. They are an alternative to aesthetics for displaying additional discrete variables. ggplot2 supports two types of factes, namely: facet_grid() and facet_wrap.\n\n2.5.1. Working with facet_wrap()\nfacet_wrap wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid because most displays are roughly rectangular.\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS)) + \n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)\n\n\n\n\n\n\n\n2.5.2. facet_grid()\nfacet_grid() forms a matrix of panels defined by row and column facetting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.\n\n\nCode\nggplot(data=exam_data,\n       aes(x = MATHS)) + \n  geom_histogram(bins=20) +\n    facet_grid(~ CLASS)\n\n\n\n\n\n\n\n\n2.6. Essential Grammatical Elements in ggplot2: Coordinates\nThe Coordinates functions map the position of objects onto the plane of the plot. There are a number of different possible coordinate systems to use, they are:\n\ncoord_cartesian(): the default cartesian coordinate systems, where you specify x and y values (e.g. allows you to zoom in or out).\ncoord_flip(): a cartesian system with the x and y flipped.\ncoord_fixed(): a cartesian system with a “fixed” aspect ratio (e.g. 1.78 for a “widescreen” plot).\ncoord_quickmap(): a coordinate system that approximates a good aspect ratio for maps.\n\n\n2.6.1. Working with Coordinate\nBy the default, the bar chart of ggplot2 is in vertical form.\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\nFlipping the chart by using coord_flip().\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() + \n  coord_flip()\n\n\n\n\n\n\n\n2.6.2. Changing the y-axis and x-axis range\nThe scatterplot is slightly misleading because the y-axis and x-axis range are not equal.\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe code chunk below fixed both the y-axis and x-axis range from 0-100.\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n2.7. Essential Grammatical Elements in ggplot2: themes\nThemes control elements of the graph not related to the data. For example: - background colour - size of fonts - gridlines - colour of labels\nBuilt-in themes include: - theme_gray() (default) - theme_bw() - theme_classic()\nA list of theme can be found at this link. Each theme element can be conceived of as either a line (e.g. x-axis), a rectangle (e.g. graph background), or text (e.g. axis title).\n\n2.7.1. Working with theme\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\nA horizontal bar chart plotted using theme_classic().\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\nA horizontal bar chart plotted using theme_minimal().\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on Exercise 05 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, we will model, analyse and visualise network data using R.\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "title": "Hands-on Exercise 05 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, we will model, analyse and visualise network data using R.\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "title": "Hands-on Exercise 05 - Modelling, Visualising and Analysing Network Data with R",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n2.1.1. Installing and launching R packages\nFour network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-data",
    "title": "Hands-on Exercise 05 - Modelling, Visualising and Analysing Network Data with R",
    "section": "3. The Data",
    "text": "3. The Data\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also known as link) data.\n\n3.1. The Edges data\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n\n\n\n\n\n3.2. The Nodes data\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\n\n\n\n\n3.3. Importing the Network Data from Files\nImport GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n3.4. Reviewing the Imported Data\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n3.5. Wrangling Time\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n3.6. Reviewing the Revised Date Fields\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\n3.7. Wrangling Attributes\nA close examination of GAStech_edges data frame reveals that it consists of individual email flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\n3.8. Reviewing the Revised Edges File\nTable below shows the data structure of the formatted GAStech_edges data frame.\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Exercise 05 - Modelling, Visualising and Analysing Network Data with R",
    "section": "4. Creating Network Objects using tidygraph",
    "text": "4. Creating Network Objects using tidygraph\nIn this section, we will create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, read these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n4.1. The tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n4.2. The dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\n\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n4.3. Using tbl_graph() to build tidygraph data model\nIn this section, we will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\n\n\n\n\n\n\nNote\n\n\n\nRecommended to look at the reference guide of tbl_graph()\n\n\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n4.4. Reviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 54 × 4\n     id label               Department     Title                                \n  &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag…\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n# ℹ 48 more rows\n#\n# A tibble: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n4.5. Reviewing the output tidygraph’s graph object\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n4.6. Changing the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 1,372 × 4\n   from    to Weekday  Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;     &lt;int&gt;\n1    40    41 Saturday     13\n2    41    43 Monday       11\n3    35    31 Tuesday      10\n4    40    41 Monday       10\n5    40    43 Monday       10\n6    36    32 Sunday        9\n# ℹ 1,366 more rows\n#\n# A tibble: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on Exercise 05 - Modelling, Visualising and Analysing Network Data with R",
    "section": "5. Plotting Static Network Graphs with ggraph package",
    "text": "5. Plotting Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n5.1. Plotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n5.2. Changing the Default Network Graph Theme\nIn this section, we will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\n5.3. Changing the Coloring of the Plot\ntheme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n5.4. Working with ggraph’s layout\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n \n\n\n5.5. Fruchterman and Reingold layout\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\nlayout argument is used to define the layout to be used.\n\n\n\n\n5.6. Modifying Network Nodes\nIn this section, we will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\n5.7. Modifying Edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-facet-graphs",
    "title": "Hands-on Exercise 05 - Modelling, Visualising and Analysing Network Data with R",
    "section": "6. Creating Facet Graphs",
    "text": "6. Creating Facet Graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, we will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n6.1. Working with facet_edges()\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n6.2. Working with facet_edges()\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n6.3. A framed facet graph\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n6.4. Working with facet_nodes()\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#network-metrics-analysis",
    "title": "Hands-on Exercise 05 - Modelling, Visualising and Analysing Network Data with R",
    "section": "7. Network Metrics Analysis",
    "text": "7. Network Metrics Analysis\n\n7.1. Computing Centrality Indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector.\n\n\n\n\n\n\nNote\n\n\n\nIt is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\n\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\n\n7.2. Visualising Network Metrics\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n7.3. Visualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(\n    group_edge_betweenness(\n      weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on Exercise 05 - Modelling, Visualising and Analysing Network Data with R",
    "section": "8. Building Interactive Network Graph with visNetwork",
    "text": "8. Building Interactive Network Graph with visNetwork\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n8.1. Data Preparation\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n8.2. Plotting the first interactive network graph\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n8.3. Working with layout\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisit Igraphto find out more about visIgraphLayout’s argument.\n\n\n\n\n8.4. Working with visual attributes - Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n8.5. Working with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges.\n\nThe argument arrows is used to define where to place the arrow.\nThe smooth argument is used to plot the edges using a smooth curve.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisit Option to find out more about visEdges’s argument.\n\n\n\n\n8.6. Interactivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisit Option to find out more about visEdges’s argument."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html",
    "title": "Hands-on Exercise 06a - Building Ternary Plot with R",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#overview",
    "title": "Hands-on Exercise 06a - Building Ternary Plot with R",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 06a - Building Ternary Plot with R",
    "section": "2. Installing and launching R packages",
    "text": "2. Installing and launching R packages\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\n\nShow the codes\npacman::p_load('plotly', 'tidyverse')\n\n\n\n\n\n\n\n\nNote\n\n\n\nDue to some technical issue, ggtern is currently not available for downloading via cran. We need to download ggtern from the archive by using the code chunk below. The latest archive version is 3.4.1.\n\n\n\n\nShow the codes\n## Only need to run ONE-TIME\n\n#require(devtools)\n#install_version(\"ggtern\", version = \"3.4.1\", repos = \"http://cran.us.r-project.org\")\n\n\nNext, load ggtern package into R environment by using the code chunk below.\n\n\nShow the codes\nlibrary(ggtern)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#data-preparation",
    "title": "Hands-on Exercise 06a - Building Ternary Plot with R",
    "section": "3. Data Preparation",
    "text": "3. Data Preparation\n\n3.1. The Data\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n3.2. Importing Data\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n\nShow the codes\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\n3.3. Preparing the Data\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n\nShow the codes\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n#  spread(AG, Population) %&gt;%   ## spread is no longer supported\n  pivot_wider(names_from = AG, values_from = Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\n\n3.4. Plotting Ternary Diagram with R\n\n3.4.1. Plotting a static ternary diagram\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n\nShow the codes\n#Building the static ternary plot\nggtern(data=agpop_mutated,\n       aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\nShow the codes\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n3.4.2. Plotting an interative ternary diagram\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n\n\n\n\n\nNote\n\n\n\nggplot_ly does not work very well with ggtern. Hence we use plot_ly() instead.\n\n\n\n\nShow the codes\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html",
    "title": "Hands-on Exercise 06e - Treemap Visualisation with R",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html#overview",
    "title": "Hands-on Exercise 06e - Treemap Visualisation with R",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 06e - Treemap Visualisation with R",
    "section": "2. Installing and Launching R Packages",
    "text": "2. Installing and Launching R Packages\nBefore we get started, we need to make sure we have treemap, treemapify and tidyverse pacakges have been installed in you R.\n\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html#data-wrangling",
    "title": "Hands-on Exercise 06e - Treemap Visualisation with R",
    "section": "3. Data Wrangling",
    "text": "3. Data Wrangling\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\n3.1. Importing the Data Set\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nThe output tibble data.frame is called realis2018.\n\n\n3.2. Data Wrangling and Manipulation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n\n\n\n\nRecommendation\n\n\n\nStudents who are new to dplyr methods should consult Introduction to dplyr before moving on to the next section.\n\n\n\n\n3.3. Grouped Summaries Without using the Pipe\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\n\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = \n                            median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = \n                            median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\nNote\n\n\n\nAggregation functions such as sum() and median() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n3.4. Grouped Summaries with the Pipe\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\n\n\n\n\n\nRecommendation\n\n\n\nTo learn more about pipe, visit this excellent article: Pipes in R Tutorial For Beginners.\n\n\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = \n              median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = \n              median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html",
    "title": "Hands-on Exercise 06d - Building Parallel Coordinates Plot with R",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html#overview",
    "title": "Hands-on Exercise 06d - Building Parallel Coordinates Plot with R",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 06d - Building Parallel Coordinates Plot with R",
    "section": "2. Installing and Launching R Packages",
    "text": "2. Installing and Launching R Packages\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\nSeemed like this version of R studio (2023.03.1 Build 446) is unable to install parcoords. We will search for the package and install manually.\n\n## Only need to run ONE-TIME\nrequire(devtools)\ninstall_version(\"parcoords\", version = \"1.0.0\", repos = \"http://cran.us.r-project.org\")\n\nNext, load parcoords package into R environment by using the code chunk below.\n\nlibrary(parcoords)\n\n\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html#data-preparation",
    "title": "Hands-on Exercise 06d - Building Parallel Coordinates Plot with R",
    "section": "3. Data Preparation",
    "text": "3. Data Preparation\nIn this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on Exercise 06d - Building Parallel Coordinates Plot with R",
    "section": "4. Plotting Static Parallel Coordinates Plot",
    "text": "4. Plotting Static Parallel Coordinates Plot\nIn this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n4.1. Plotting a simple parallel coordinates\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n4.2. Plotting a parallel coordinates with boxplot\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\n\n4.3. Parallel Coordinates with Facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n\n\n4.4. Rotating x-axis text label\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\n\n4.5. Adjusting the rotated x-axis text label\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Exercise 06d - Building Parallel Coordinates Plot with R",
    "section": "5. Plotting Interactive Parallel Coordinates Plot: parallelPlot methods",
    "text": "5. Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n5.1. The Basic Plot\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that some of the axis labels are too long. We will learn how to overcome this problem in the next step.\n\n\n\n\n5.2. Rotate Axis Label\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n5.3. Changing the Colour Scheme\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunk below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n5.4. Parallel Coordinates Plot with Histogram\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06d.html#references",
    "title": "Hands-on Exercise 06d - Building Parallel Coordinates Plot with R",
    "section": "6. References",
    "text": "6. References\n\nggparcoord() of GGally package\nparcoords user guide\nparallelPlot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html#designing-treemap-with-treemap-package",
    "title": "Hands-on Exercise 06e - Treemap Visualisation with R",
    "section": "4. Designing Treemap with treemap Package",
    "text": "4. Designing Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n4.1. Designing a static treemap\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n4.2. Using the Basic Arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nThings to learn from the three arguments used\n\n\n\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s values will be used to map the sizes of the rectangles of the treemaps.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectangles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n\n\n4.3. Working with vColor and type arguments\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nThinking to learn from the code chunk above\n\n\n\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\n\n4.4. Colours in treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n4.5. The “value” type treemap\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nThinking to learn from the code chunk above\n\n\n\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\n\n4.6. The “manual” type treemap\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nThinking to learn from the code chunk above\n\n\n\nThe colour scheme used is very confusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative.\nTo overcome this problem, a single colour palette such as Blues should be used.\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n4.7. Treemap Layout\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n4.8. Working with algorithm argument\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n4.9. Using sortID\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Exercise 06e - Treemap Visualisation with R",
    "section": "5. Designing Treemap using treemapify Package",
    "text": "5. Designing Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\n5.1. Designing a Basic treemap\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n5.2. Defining Hierarchy\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06e.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Exercise 06e - Treemap Visualisation with R",
    "section": "6. Designing Interactive Treemap using d3treeR",
    "text": "6. Designing Interactive Treemap using d3treeR\n\n6.1. Installing d3treeR package\nThe following steps show you how to install a R package which is not available in cran.\n\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\n\n#install.packages(\"devtools\")\n\n\nNext, you will load the devtools library and install the package found in github by using the codes below.\n\n\n#library(devtools)\n#install_github(\"timelyportfolio/d3treeR\")\n\n\nNow you are ready to launch d3treeR package\n\n\nlibrary(d3treeR)\n\n\n\n6.2. Designing An Interactive Treemap\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html",
    "title": "Take-Home Exercise 02",
    "section": "",
    "text": "With reference to the Mini-Challenge 2 of VAST Challenge 2023 and by using appropriate static and interactive statistical graphics methods, we will help FishEye identify companies that may be engaged in illegal fishing.\n\n\nUse visual analytics to identify temporal patterns for individual entities and between entities in the knowledge graph FishEye created from trade records. Categorize the types of business relationship patterns you find.\n\n\n\nFor this task, we will make use of the mc2_challenge_graph.json provided for the data analysis and visualisation."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#the-task",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#the-task",
    "title": "Take-Home Exercise 02",
    "section": "",
    "text": "Use visual analytics to identify temporal patterns for individual entities and between entities in the knowledge graph FishEye created from trade records. Categorize the types of business relationship patterns you find."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#data-source",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#data-source",
    "title": "Take-Home Exercise 02",
    "section": "",
    "text": "For this task, we will make use of the mc2_challenge_graph.json provided for the data analysis and visualisation."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#loading-the-data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#loading-the-data",
    "title": "Take-Home Exercise 02",
    "section": "3.1. Loading the Data",
    "text": "3.1. Loading the Data\nThis section will show how the 13 JSON files are being loaded into the environment, starting with the main (mc2_challenge_graph.json), then the remaining JSON files for each type of fish.\nBased on the VAST 2023 data notes, column dataset will always be ‘MC2’, to represent this set of data belongs to mini challenge 2. As such, we will not import this column into the R environment.\n\n3.1.1. Load main file mc2_challenge_graph.json\nWe will first load in the main file, mc2_challenge_graph.json, then extract the nodes and edges (links) information out.\n\n\nShow the codes\nMC2 &lt;- fromJSON(\"data/mc2_challenge_graph.json\")\n\n\n\n3.1.1.1. Extracting the nodes data.frame from MC2\n\n\nShow the codes\nMC2_nodes &lt;- as_tibble(MC2$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry)\n\n\n\n\n3.1.1.2. Extracting the edges (links) data.frame from MC2\n\n\nShow the codes\nMC2_edges &lt;- as_tibble(MC2$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         weightkg, valueofgoods_omu, volumeteu,\n         valueofgoodsusd)\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\n# write_csv(x = MC2_nodes, \"data/generated/mc2_nodes.csv\")\n# write_csv(x = MC2_edges, \"data/generated/mc2_edges.csv\")\n\n\n\n\n3.1.1.3. Examining the structure of MC2_nodes and MC2_edges data.frames using glimpse() of dyplr.\n\n\nShow the codes\nglimpse(MC2_nodes)\n\n\nRows: 34,576\nColumns: 3\n$ id         &lt;chr&gt; \"AquaDelight Inc and Son's\", \"BaringoAmerica Marine Ges.m.b…\n$ shpcountry &lt;chr&gt; \"Polarinda\", NA, \"Oceanus\", NA, \"Oceanus\", \"Kondanovia\", NA…\n$ rcvcountry &lt;chr&gt; \"Oceanus\", NA, \"Oceanus\", NA, \"Oceanus\", \"Utoporiana\", NA, …\n\n\nShow the codes\nglimpse(MC2_edges)\n\n\nRows: 5,464,378\nColumns: 8\n$ source           &lt;chr&gt; \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son…\n$ target           &lt;chr&gt; \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica M…\n$ arrivaldate      &lt;chr&gt; \"2034-02-12\", \"2034-03-13\", \"2028-02-07\", \"2028-02-23…\n$ hscode           &lt;chr&gt; \"630630\", \"630630\", \"470710\", \"470710\", \"470710\", \"47…\n$ weightkg         &lt;int&gt; 4780, 6125, 10855, 11250, 11165, 11290, 9000, 19490, …\n$ valueofgoods_omu &lt;dbl&gt; 141015, 141015, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ valueofgoodsusd  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 87110, 188140, NA, 221110, 58…\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nMC2_nodes\n\nThe output of MC2_nodes above revealed that both shpcountry and rcvcountry contained missing values, represented as NA. In order not to have any missing values in the data, we will remove any records with NA.\nWe will also perform white space trimming for the columns, to remove any the leading and trailing white spaces.\n\nMC2_edges revealed a couple of issues that we should address before starting the analysis.\n\narrivaldate has the format of ‘YYYY-MM-DD’ and is treated as chr data type instead of date data type, and we should change the data type back to date data type.\nmissing values represented as NA for valueofgoods_omu and valueofgoodsusd. We should replace the NA to ‘0’.\nsince weightkg is supposed to be int data type, we should also ensure that there are not missing values (NA) in the data.\nWe will also perform white space trimming for the chr columns,to remove any the leading and trailing white spaces.\n\n\n\n\n\n\n3.1.1.4. Perform Data Wrangling for MC2_nodes and MC2_edges\n\n\nShow the codes\n## For MC2_nodes ##\n\n# Remove rows with NA values in any column\nMC2_nodes_noNA &lt;- MC2_nodes[!apply(is.na(MC2_nodes), 1, any), ]\n\n# Trim the leading and trailing white spaces\nMC2_nodes_noNA &lt;- MC2_nodes_noNA %&gt;%\n  mutate(id = trimws(id)) %&gt;%\n  mutate(shpcountry = trimws(shpcountry)) %&gt;%\n  mutate(rcvcountry = trimws(rcvcountry))\n\n\n## For MC2_edges ##\n# Converting to date data type (yyyy-mm-dd format) and add another new column, \n# Weekday, to indicate the day of week for visualisation later.\n# Also to trim the leading and trailing white spaces for source, target and\n# hscode columns\n\nMC2_edges &lt;- MC2_edges %&gt;%\n  mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n  mutate(Weekday = wday(arrivaldate,\n                        label = TRUE,\n                        abbr = FALSE)) %&gt;%\n  mutate(source = trimws(source)) %&gt;%\n  mutate(target = trimws(target)) %&gt;%\n  mutate(hscode = trimws(hscode))\n\n\n# Replace all the NA values to 0\nMC2_edges[is.na(MC2_edges)] &lt;- 0.0\n\n\n\n\nShow the codes\n# Printing the filtered data frame\nglimpse(MC2_nodes_noNA)\n\n\nRows: 9,332\nColumns: 3\n$ id         &lt;chr&gt; \"AquaDelight Inc and Son's\", \"Yu gan  Sea spray GmbH Indust…\n$ shpcountry &lt;chr&gt; \"Polarinda\", \"Oceanus\", \"Oceanus\", \"Kondanovia\", \"Vesperand…\n$ rcvcountry &lt;chr&gt; \"Oceanus\", \"Oceanus\", \"Oceanus\", \"Utoporiana\", \"Oceanus\", \"…\n\n\nShow the codes\nglimpse(MC2_edges)\n\n\nRows: 5,464,378\nColumns: 9\n$ source           &lt;chr&gt; \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son…\n$ target           &lt;chr&gt; \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica M…\n$ arrivaldate      &lt;date&gt; 2034-02-12, 2034-03-13, 2028-02-07, 2028-02-23, 2028…\n$ hscode           &lt;chr&gt; \"630630\", \"630630\", \"470710\", \"470710\", \"470710\", \"47…\n$ weightkg         &lt;int&gt; 4780, 6125, 10855, 11250, 11165, 11290, 9000, 19490, …\n$ valueofgoods_omu &lt;dbl&gt; 141015, 141015, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ valueofgoodsusd  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 87110, 188140, 0, 221110, 58645, 58…\n$ Weekday          &lt;ord&gt; Sunday, Monday, Monday, Wednesday, Monday, Monday, We…\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe number of records for MC2_nodes has tremendously dropped from 34,576 to 9,332, after removing those records with ‘NA’ any of the columns.\n\n\nAggregate MC2_edges records\nMC2_edges data frame showed that it consists of individual arrival records, and will not be very useful for visualisation. Hence, we will aggregate the records by arrivaldate, source, target, hscode and weekday.\n\n\nShow the codes\nMC2_edges_aggregated &lt;- MC2_edges %&gt;%\n  group_by(source, target, arrivaldate, hscode, weightkg,\n           valueofgoods_omu, volumeteu, valueofgoodsusd, Weekday) %&gt;%\n  summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\nglimpse(MC2_edges_aggregated)\n\n\nRows: 89,002\nColumns: 10\n$ source           &lt;chr&gt; \"-13\", \"-27\", \"-28\", \"-41\", \"-41\", \"-41\", \"1 AS Marin…\n$ target           &lt;chr&gt; \"BV Limited Liability Company\", \"Sea Breeze Sagl Ocea…\n$ arrivaldate      &lt;date&gt; 2033-09-23, 2032-09-01, 2033-11-30, 2028-11-18, 2028…\n$ hscode           &lt;chr&gt; \"640220\", \"940490\", \"200410\", \"844790\", \"844790\", \"84…\n$ weightkg         &lt;int&gt; 4295, 2940, 24570, 630, 455, 470, 1680, 7305, 9670, 1…\n$ valueofgoods_omu &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ valueofgoodsusd  &lt;dbl&gt; 0, 0, 56020, 0, 0, 0, 8900, 23615, 32830, 59850, 2461…\n$ Weekday          &lt;ord&gt; Friday, Wednesday, Wednesday, Saturday, Saturday, Tue…\n$ Weight           &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n\n\nVerify the node-edge relationships\nThis step is to ensure that all nodes will be connected to at least once, i.e. there will not be any isolated nodes.\n\n\nShow the codes\n# Verify node-edge relationships\nnode_ids &lt;- MC2_nodes_noNA$id\nvalid_nodes &lt;- union(unique(MC2_edges_aggregated$source),\n                     unique(MC2_edges_aggregated$target))\nvalid_nodes &lt;- intersect(valid_nodes, MC2_nodes_noNA$id)\n\n\n# Filter edges with valid nodes (removed isolated nodes)\nvalid_edges &lt;- MC2_edges_aggregated %&gt;%\n  filter(source %in% valid_nodes, target %in% valid_nodes)\n\n# Rename the columns in nodes column from shpcountry and rcvcountry to \n# source and target respectively in order to match the columns in edges\nfinal_MC2_nodes_noNA &lt;- MC2_nodes_noNA %&gt;%\n  rename(\n    source = shpcountry,\n    target = rcvcountry\n  )\n\n\nMaking use of `tbl_graph() to build tidygraph data model\n\n\nShow the codes\n# Build tbl_graph using the valid nodes and edges \ntbl_graph &lt;- tbl_graph(nodes = final_MC2_nodes_noNA,\n                       edges = valid_edges,\n                       directed = TRUE)\n\n# Print the tbl_graph\nprint(tbl_graph)\n\n\n# A tbl_graph: 9332 nodes and 74344 edges\n#\n# A directed multigraph with 3600 components\n#\n# A tibble: 9,332 × 3\n  id                                source     target    \n  &lt;chr&gt;                             &lt;chr&gt;      &lt;chr&gt;     \n1 AquaDelight Inc and Son's         Polarinda  Oceanus   \n2 Yu gan  Sea spray GmbH Industrial Oceanus    Oceanus   \n3 Olas del Mar Worldwide            Oceanus    Oceanus   \n4 French Crab S.p.A. Worldwide      Kondanovia Utoporiana\n5 Panope Limited Liability Company  Vesperanda Oceanus   \n6 hǎi dǎn Corporation Wharf         Marebak    Oceanus   \n# ℹ 9,326 more rows\n#\n# A tibble: 74,344 × 10\n   from    to arrivaldate hscode weightkg valueofgoods_omu volumeteu\n  &lt;int&gt; &lt;int&gt; &lt;date&gt;      &lt;chr&gt;     &lt;int&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1  8895  7099 2033-09-23  640220     4295                0         0\n2   628  1226 2028-11-18  844790      630                0         0\n3   628  1226 2028-12-09  844790      455                0         0\n# ℹ 74,341 more rows\n# ℹ 3 more variables: valueofgoodsusd &lt;dbl&gt;, Weekday &lt;ord&gt;, Weight &lt;int&gt;\n\n\n\n\nShow the codes\nggraph(tbl_graph, layout = \"nicely\") +\n  geom_edge_link2() +\n  geom_node_point() +\n  theme_void()\n\n\n\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\n# write_csv(x = final_MC2_nodes_noNA, \"data/generated/final_MC2_nodes_noNA.csv\")\n# write_csv(x = MC2_edges_aggregated, \"data/generated/mc2_edges_agg.csv\")\n# write_csv(x = valid_edges, \"data/generated/valid_edges.csv\")\n\n\n\n\n\n3.1.2. Load the individual fishes JSON files and extract the nodes and edges (links) information out.\nSimilarly to the main file above, we will extract the nodes and edges data from the respective JSON files, then we will examine the nodes and edges data.frames and perform the necessary data wrangling.\n\n3.1.2.1. carp.json\n\n\nShow the codes\n# Load carp.json into the environment\ncarp &lt;- fromJSON(\"data/bundles/carp.json\")\n\n\n\n\nShow the codes\n# Extract the nodes data.frame\ncarp_nodes &lt;- as_tibble(carp$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry)\n\n\n\n\nShow the codes\n# Extract the edges data.frame\ncarp_edges &lt;- as_tibble(carp$links) %&gt;%\n  select(source, target, arrivaldate, hscode, weightkg, \n         valueofgoods_omu, volumeteu, generated_by)\n\n\n\n\nShow the codes\nglimpse(carp_nodes)\n\n\nRows: 245\nColumns: 3\n$ id         &lt;chr&gt; \"Tshimbua GmbH & Co. KG\", \"Caracola del Sol Services\", \"Mar…\n$ shpcountry &lt;chr&gt; \"Sirenareef\", NA, \"Marebak\", NA, \"Coral Solis\", NA, \"Mareba…\n$ rcvcountry &lt;chr&gt; NA, \"Azurionix\", NA, \"Azurionix\", NA, \"Azurionix\", NA, NA, …\n\n\nShow the codes\nglimpse(carp_edges)\n\n\nRows: 165\nColumns: 8\n$ source           &lt;chr&gt; \"Tshimbua GmbH & Co. KG\", \"Marine Masterminds Dry doc…\n$ target           &lt;chr&gt; \"Caracola del Sol Services\", \"Playa de la Luna Incorp…\n$ arrivaldate      &lt;chr&gt; \"2034-03-20\", \"2034-08-01\", \"2034-11-27\", \"2034-10-10…\n$ hscode           &lt;int&gt; 80440, 940179, 940161, 940161, 40729, 700711, 701090,…\n$ weightkg         &lt;int&gt; 15720, 10795, 6555, 6675, 54775, 22430, 32890, 20775,…\n$ valueofgoods_omu &lt;dbl&gt; 15915, NA, NA, NA, NA, NA, NA, 63125, NA, 57020, NA, …\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 0, 15, 0, 5, 0, 0, 0, NA, 5, 5, NA, 0, NA, N…\n$ generated_by     &lt;chr&gt; \"carp\", \"carp\", \"carp\", \"carp\", \"carp\", \"carp\", \"carp…\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\ncarp_nodes\n\nThe output of carp_nodes above revealed that both shpcountry and rcvcountry contained missing values, represented as NA. In order not to have any missing values in the data, we will remove any records with NA.\nWe will also perform white space trimming for the columns, to remove any the leading and trailing white spaces.\n\ncarp_edges revealed a couple of issues that we should address before starting the analysis.\n\narrivaldate has the format of ‘YYYY-MM-DD’ and is treated as chr data type instead of date data type, and we should change the data type back to date data type.\nhscode is treated as int data type instead of chr data type. we will change the data type back to chr.\nmissing values represented as NA for valueofgoods_omu and valueofgoodsusd. We should replace the NA to ‘0’.\nsince weightkg is int data type, we should also ensure that there are not missing values (NA) in the data.\nWe will also perform white space trimming for the chr columns to remove any the leading and trailing white spaces.\n\n\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\n# write_csv(x = carp_nodes, \"data/generated/carp_nodes.csv\")\n# write_csv(x = carp_edges, \"data/generated/carp_edges.csv\")\n\n\n\n\nShow the codes\n### Data Wrangling \n## For carp_nodes ##\n\n# Remove rows with NA values in any column\ncleaned_carp_nodes &lt;- carp_nodes[!apply(is.na(carp_nodes), 1, any), ]\n\n# Trim the leading and trailing white spaces\ncarp_nodes &lt;- carp_nodes %&gt;%\n  mutate(id = trimws(id)) %&gt;%\n  mutate(shpcountry = trimws(shpcountry)) %&gt;%\n  mutate(rcvcountry = trimws(rcvcountry))\n\n\n## For carp_edges ##\n# Converting to date data type (yyyy-mm-dd format) and add another new column, \n# Weekday, to indicate the day of week for visualisation later.\n# Also to trim the leading and trailing white spaces for source, target and\n# hscode columns\n\ncarp_edges &lt;- carp_edges %&gt;%\n  mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n  mutate(Weekday = wday(arrivaldate,\n                        label = TRUE,\n                        abbr = FALSE)) %&gt;%\n  mutate(source = trimws(source)) %&gt;%\n  mutate(target = trimws(target)) %&gt;%\n  mutate(hscode = trimws(as.character(hscode))) \n\n# Replace all the NA values to 0\ncarp_edges[is.na(carp_edges)] &lt;- 0.0\n\n\n\n\nShow the codes\nglimpse(cleaned_carp_nodes)\n\n\nRows: 5\nColumns: 3\n$ id         &lt;chr&gt; \"Estrella del Mar Seafarer\", \"Caracola Marina - Plc Enterpr…\n$ shpcountry &lt;chr&gt; \"Oceanus\", \"Kondanovia\", \"Alverossia\", \"Puerto Sol\", \"Ocean…\n$ rcvcountry &lt;chr&gt; \"Oceanus\", \"Oceanus\", \"Oceanus\", \"Azurionix\", \"Oceanus\"\n\n\nShow the codes\nglimpse(carp_edges)\n\n\nRows: 165\nColumns: 9\n$ source           &lt;chr&gt; \"Tshimbua GmbH & Co. KG\", \"Marine Masterminds Dry doc…\n$ target           &lt;chr&gt; \"Caracola del Sol Services\", \"Playa de la Luna Incorp…\n$ arrivaldate      &lt;date&gt; 2034-03-20, 2034-08-01, 2034-11-27, 2034-10-10, 2034…\n$ hscode           &lt;chr&gt; \"80440\", \"940179\", \"940161\", \"940161\", \"40729\", \"7007…\n$ weightkg         &lt;int&gt; 15720, 10795, 6555, 6675, 54775, 22430, 32890, 20775,…\n$ valueofgoods_omu &lt;dbl&gt; 15915, 0, 0, 0, 0, 0, 0, 63125, 0, 57020, 0, 0, 0, 0,…\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 0, 15, 0, 5, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0…\n$ generated_by     &lt;chr&gt; \"carp\", \"carp\", \"carp\", \"carp\", \"carp\", \"carp\", \"carp…\n$ Weekday          &lt;ord&gt; Monday, Tuesday, Monday, Tuesday, Sunday, Thursday, S…\n\n\nMaking use of `tbl_graph() to build tidygraph data model\n\n\nShow the codes\n# \n# \n# carp_edges_aggregated &lt;- carp_edges %&gt;%\n#   group_by(source, target, arrivaldate, hscode, weightkg, \n#            valueofgoods_omu, volumeteu, Weekday) %&gt;%\n#     summarise(Weight = n()) %&gt;%\n#   filter(source!=target) %&gt;%\n#   filter(Weight&gt;0) %&gt;%\n#   ungroup()\n# \n# glimpse(carp_edges_aggregated)\n# \n# \n# carp_graph &lt;- as_tbl_graph(nodes = carp_nodes) %&gt;%\n#   activate(nodes) %&gt;%\n#   activate(carp_edges_aggregated) %&gt;%\n#   bind_edges(carp_edges_aggregated) %&gt;%\n#   arrange(desc(Weight))\n\n\n# carp_graph &lt;- tbl_graph(nodes = cleaned_carp_nodes,\n#                        edges = carp_edges,\n#                        directed = TRUE)\n\n# carp_graph %&gt;%\n#   activate(edges) %&gt;%\n#   arrange(desc(Weight))\n\n# carp_graph\n\n\n\n\nShow the codes\n# ggraph(carp_graph) +\n#   geom_edge_link() +\n#   geom_node_point()\n\n\n\n\n3.1.2.2. catfish.json\n\n\nShow the codes\n# Load catfish.json into the environment\ncatfish &lt;- fromJSON(\"data/bundles/catfish.json\")\n\n\n\n\nShow the codes\n# Extract the nodes data.frame\ncatfish_nodes &lt;- as_tibble(catfish$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry, dataset)\n\n\n\n\nShow the codes\nglimpse(catfish_nodes)\n\n\nRows: 368\nColumns: 4\n$ id         &lt;chr&gt; \"Line S.A. de C.V.\", \"Fisherman's Best Marine conservation\"…\n$ shpcountry &lt;chr&gt; \"Lumindoria\", NA, \"-27\", NA, \"Rio Isla\", NA, \"-27\", NA, \"Me…\n$ rcvcountry &lt;chr&gt; NA, \"-27\", NA, \"-27\", NA, \"-27\", NA, \"-27\", NA, \"Azurionix\"…\n$ dataset    &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC…\n\n\n\n\nShow the codes\n# Extract the edges data.frame\ncatfish_edges &lt;- as_tibble(catfish$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         valueofgoods_omu, generated_by, dataset)\n\n\n\n\nShow the codes\nglimpse(catfish_edges)\n\n\nRows: 277\nColumns: 7\n$ source           &lt;chr&gt; \"Line S.A. de C.V.\", \"Line S.A. de C.V.\", \"Nazhivka S…\n$ target           &lt;chr&gt; \"Fisherman's Best Marine conservation\", \"Mar de la Vi…\n$ arrivaldate      &lt;chr&gt; \"2034-11-08\", \"2035-01-19\", \"2034-04-27\", \"2035-01-05…\n$ hscode           &lt;int&gt; 300590, 961900, 848180, 250510, 640419, 340600, 22084…\n$ valueofgoods_omu &lt;dbl&gt; 1491470, NA, NA, NA, NA, 78415, NA, NA, 27200, NA, 23…\n$ generated_by     &lt;chr&gt; \"catfish\", \"catfish\", \"catfish\", \"catfish\", \"catfish\"…\n$ dataset          &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2…\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\nwrite_csv(x = catfish_nodes, \"data/generated/catfish_nodes.csv\")\nwrite_csv(x = catfish_edges, \"data/generated/catfish_edges.csv\")\n\n\n\n\nShow the codes\n### Data Wrangling \n\n## For catfish_nodes ##\n# Replace all the NA values to \"\"\n# catfish_nodes[is.na(catfish_nodes)] &lt;- \"\"\n# \n# \n# ## For carp_edges ## --&gt; ONLY 7 variables &lt;--\n# # Converting to date data type (yyyy-mm-dd format) and add another new column, \n# # Weekday, to indicate the day of week for visualisation later.\n# \n# catfish_edges &lt;- catfish_edges %&gt;%\n#   mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n#   mutate(Weekday = wday(arrivaldate,\n#                         label = TRUE,\n#                         abbr = FALSE))\n# \n# # Replace all the NA values to 0\n# catfish_edges[is.na(catfish_edges)] &lt;- 0.0\n\n\n\n\n3.1.2.3. chub_mackerel.json\n\n\nShow the codes\n# Load chub_mackerel.json into the environment\nchub &lt;- fromJSON(\"data/bundles/chub_mackerel.json\")\n\n\n\n\nShow the codes\n# Extract the nodes data.frame\nchub_nodes &lt;- as_tibble(chub$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry, dataset)\n\n\n\n\nShow the codes\nglimpse(chub_nodes)\n\n\nRows: 193\nColumns: 4\n$ id         &lt;chr&gt; \"Ocean Quest A/S Logistics\", \"Estrella del Mar Tilapia Oyj …\n$ shpcountry &lt;chr&gt; \"Vesperanda\", NA, NA, NA, NA, NA, NA, \"Marebak\", NA, \"Calab…\n$ rcvcountry &lt;chr&gt; NA, \"Coralmarica\", \"Oceanus\", \"Azurionix\", \"Azurionix\", \"Az…\n$ dataset    &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC…\n\n\n\n\nShow the codes\n# Extract the edges data.frame\nchub_edges &lt;- as_tibble(chub$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         weightkg, valueofgoods_omu, volumeteu,\n         generated_by, dataset)\n\n\n\n\nShow the codes\nglimpse(chub_edges)\n\n\nRows: 320\nColumns: 9\n$ source           &lt;chr&gt; \"Ocean Quest A/S Logistics\", \"Ocean Quest A/S Logisti…\n$ target           &lt;chr&gt; \"Estrella del Mar Tilapia Oyj Marine\", \"Estrella del …\n$ arrivaldate      &lt;chr&gt; \"2034-06-06\", \"2034-06-06\", \"2034-06-06\", \"2034-06-06…\n$ hscode           &lt;int&gt; 160414, 160414, 160414, 160414, 160414, 160414, 16041…\n$ weightkg         &lt;int&gt; 24030, 24030, 24020, 24030, 24035, 24020, 24025, 2403…\n$ valueofgoods_omu &lt;dbl&gt; 119085, 119085, 119110, 119085, 119110, 119110, 11902…\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ generated_by     &lt;chr&gt; \"chub_mackerel\", \"chub_mackerel\", \"chub_mackerel\", \"c…\n$ dataset          &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2…\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\nwrite_csv(x = chub_nodes, \"data/generated/chub_nodes.csv\")\nwrite_csv(x = chub_edges, \"data/generated/chub_edges.csv\")\n\n\n\n\nShow the codes\n### Data Wrangling \n\n## For chub_nodes ##\n# Replace all the NA values to \"\"\n# chub_nodes[is.na(chub_nodes)] &lt;- \"\"\n# \n# \n# ## For chub_edges ##\n# # Converting to date data type (yyyy-mm-dd format) and add another new column, \n# # Weekday, to indicate the day of week for visualisation later.\n# \n# chub_edges &lt;- chub_edges %&gt;%\n#   mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n#   mutate(Weekday = wday(arrivaldate,\n#                         label = TRUE,\n#                         abbr = FALSE))\n# \n# # Replace all the NA values to 0\n# chub_edges[is.na(chub_edges)] &lt;- 0.0\n\n\n\n\n3.1.2.4. cod2.json\n\n\nShow the codes\n# Load cod2.json into the environment\ncod &lt;- fromJSON(\"data/bundles/cod2.json\")\n\n\n\n\nShow the codes\n# Extract the nodes data.frame\ncod_nodes &lt;- as_tibble(cod$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry, dataset)\n\n\n\n\nShow the codes\nglimpse(cod_nodes)\n\n\nRows: 6\nColumns: 4\n$ id         &lt;chr&gt; \"Kariba Dam   Ges.m.b.H. Delivery\", \"Oceanic Opportunities …\n$ shpcountry &lt;chr&gt; \"Puerto del Mar\", NA, \"Nalakond\", NA, \"Marebak\", NA\n$ rcvcountry &lt;chr&gt; NA, \"Azurionix\", NA, \"Oceanus\", NA, \"Azurionix\"\n$ dataset    &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\"\n\n\n\n\nShow the codes\n# Extract the edges data.frame\ncod_edges &lt;- as_tibble(cod$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         weightkg, valueofgoods_omu, volumeteu,\n         generated_by, dataset)\n\n\n\n\nShow the codes\nglimpse(cod_edges)\n\n\nRows: 193\nColumns: 9\n$ source           &lt;chr&gt; \"Kariba Dam   Ges.m.b.H. Delivery\", \"Kariba Dam   Ges…\n$ target           &lt;chr&gt; \"Oceanic Opportunities CJSC Marine\", \"Oceanic Opportu…\n$ arrivaldate      &lt;chr&gt; \"2034-01-27\", \"2034-01-27\", \"2034-01-27\", \"2034-01-27…\n$ hscode           &lt;int&gt; 30729, 30729, 30729, 30729, 30729, 30729, 30729, 3072…\n$ weightkg         &lt;int&gt; 13175, 13175, 13175, 13175, 13175, 13175, 13175, 1317…\n$ valueofgoods_omu &lt;dbl&gt; 148085, 148085, 148085, 148085, 148085, 148085, 14808…\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ generated_by     &lt;chr&gt; \"cod2\", \"cod2\", \"cod2\", \"cod2\", \"cod2\", \"cod2\", \"cod2…\n$ dataset          &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2…\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\nwrite_csv(x = cod_nodes, \"data/generated/cod_nodes.csv\")\nwrite_csv(x = cod_edges, \"data/generated/cod_edges.csv\")\n\n\n\n\nShow the codes\n### Data Wrangling \n\n## For cod_nodes ##\n# Replace all the NA values to \"\"\n# cod_nodes[is.na(cod_nodes)] &lt;- \"\"\n# \n# \n# ## For cod_edges ##\n# # Converting to date data type (yyyy-mm-dd format) and add another new column, \n# # Weekday, to indicate the day of week for visualisation later.\n# \n# cod_edges &lt;- cod_edges %&gt;%\n#   mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n#   mutate(Weekday = wday(arrivaldate,\n#                         label = TRUE,\n#                         abbr = FALSE))\n# \n# # Replace all the NA values to 0\n# cod_edges[is.na(cod_edges)] &lt;- 0.0\n\n\n\n\n3.1.2.5. herring.json\n\n\nShow the codes\n# Load herring.json into the environment\nherring &lt;- fromJSON(\"data/bundles/herring.json\")\n\n\n\n\nShow the codes\n# Extract the nodes data.frame\nherring_nodes &lt;- as_tibble(herring$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry, dataset)\n\n\n\n\nShow the codes\nglimpse(herring_nodes)\n\n\nRows: 199\nColumns: 4\n$ id         &lt;chr&gt; \"Bahía del Sol ОАО\", \"Océano de Coral Corporation Express\",…\n$ shpcountry &lt;chr&gt; \"Rio Isla\", NA, \"Oceanus\", NA, NA, \"Marebak\", NA, \"Kondanov…\n$ rcvcountry &lt;chr&gt; NA, \"Azurionix\", NA, \"Azurionix\", \"Azurionix\", NA, \"Azurion…\n$ dataset    &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC…\n\n\n\n\nShow the codes\n# Extract the edges data.frame\nherring_edges &lt;- as_tibble(herring$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         weightkg, valueofgoods_omu, volumeteu,\n         generated_by, dataset)\n\n\n\n\nShow the codes\nglimpse(herring_edges)\n\n\nRows: 154\nColumns: 9\n$ source           &lt;chr&gt; \"Bahía del Sol ОАО\", \"Oceanic Oracle GmbH & Co. KG\", …\n$ target           &lt;chr&gt; \"Océano de Coral Corporation Express\", \"SeaLion Marin…\n$ arrivaldate      &lt;chr&gt; \"2034-10-20\", \"2034-07-28\", \"2034-06-23\", \"2034-05-27…\n$ hscode           &lt;int&gt; 841869, 320990, 392690, 701090, 870829, 210390, 95051…\n$ weightkg         &lt;int&gt; 15935, 17530, 2090, 29355, 4095, 19390, 8060, 4960, 4…\n$ valueofgoods_omu &lt;dbl&gt; NA, 489215, NA, NA, NA, 54550, NA, NA, 93015, 38025, …\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ generated_by     &lt;chr&gt; \"herring\", \"herring\", \"herring\", \"herring\", \"herring\"…\n$ dataset          &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2…\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\nwrite_csv(x = herring_nodes, \"data/generated/herring_nodes.csv\")\nwrite_csv(x = herring_edges, \"data/generated/herring_edges.csv\")\n\n\n\n\nShow the codes\n### Data Wrangling \n\n## For herring_nodes ##\n# Replace all the NA values to \"\"\n# herring_nodes[is.na(herring_nodes)] &lt;- \"\"\n# \n# \n# ## For herring_edges ##\n# # Converting to date data type (yyyy-mm-dd format) and add another new column, \n# # Weekday, to indicate the day of week for visualisation later.\n# \n# herring_edges &lt;- herring_edges %&gt;%\n#   mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n#   mutate(Weekday = wday(arrivaldate,\n#                         label = TRUE,\n#                         abbr = FALSE))\n# \n# # Replace all the NA values to 0\n# herring_edges[is.na(herring_edges)] &lt;- 0.0\n\n\n\n\n3.1.2.6. lichen.json\n\n\nShow the codes\n# Load lichen.json into the environment\nlichen &lt;- fromJSON(\"data/bundles/lichen.json\")\n\n\n\n\nShow the codes\n# Extract the nodes data.frame\nlichen_nodes &lt;- as_tibble(lichen$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry, dataset)\n\n\n\n\nShow the codes\nglimpse(lichen_nodes)\n\n\nRows: 18\nColumns: 4\n$ id         &lt;chr&gt; \"Jammu Limited Liability Company\", \"Coral del Sur Ltd. Corp…\n$ shpcountry &lt;chr&gt; \"Utoporiana\", NA, \"Marebak\", \"Playa Solis\", NA, \"Vesperanda…\n$ rcvcountry &lt;chr&gt; NA, \"Oceanus\", \"Oceanus\", NA, \"Azurionix\", NA, \"Azurionix\",…\n$ dataset    &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC…\n\n\n\n\nShow the codes\n# Extract the edges data.frame\nlichen_edges &lt;- as_tibble(lichen$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         weightkg, valueofgoods_omu, volumeteu,\n         generated_by, dataset)\n\n\n\n\nShow the codes\nglimpse(lichen_edges)\n\n\nRows: 184\nColumns: 9\n$ source           &lt;chr&gt; \"Jammu Limited Liability Company\", \"Jammu Limited Lia…\n$ target           &lt;chr&gt; \"Coral del Sur Ltd. Corporation Investment\", \"Coral d…\n$ arrivaldate      &lt;chr&gt; \"2034-05-19\", \"2034-11-04\", \"2034-09-23\", \"2034-05-19…\n$ hscode           &lt;int&gt; 920590, 920590, 920590, 920590, 920590, 920590, 92071…\n$ weightkg         &lt;int&gt; 2710, 1560, 3995, 2710, 2675, 2710, 2435, 2675, 2710,…\n$ valueofgoods_omu &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0,…\n$ generated_by     &lt;chr&gt; \"lichen\", \"lichen\", \"lichen\", \"lichen\", \"lichen\", \"li…\n$ dataset          &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2…\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\nwrite_csv(x = lichen_nodes, \"data/generated/lichen_nodes.csv\")\nwrite_csv(x = lichen_edges, \"data/generated/lichen_edges.csv\")\n\n\n\n\nShow the codes\n### Data Wrangling \n\n## For lichen_nodes ##\n# Replace all the NA values to \"\"\n# lichen_nodes[is.na(lichen_nodes)] &lt;- \"\"\n# \n# \n# ## For lichen_edges ##\n# # Converting to date data type (yyyy-mm-dd format) and add another new column, \n# # Weekday, to indicate the day of week for visualisation later.\n# \n# lichen_edges &lt;- lichen_edges %&gt;%\n#   mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n#   mutate(Weekday = wday(arrivaldate,\n#                         label = TRUE,\n#                         abbr = FALSE))\n# \n# # Replace all the NA values to 0\n# lichen_edges[is.na(lichen_edges)] &lt;- 0.0\n\n\n\n\n3.1.2.7. mackerel.json\n\n\nShow the codes\n# Load mackerel.json into the environment\nmac &lt;- fromJSON(\"data/bundles/mackerel.json\")\n\n\n\n\nShow the codes\n# Extract the nodes data.frame\nmac_nodes &lt;- as_tibble(mac$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry, dataset)\n\n\n\n\nShow the codes\nglimpse(mac_nodes)\n\n\nRows: 243\nColumns: 4\n$ id         &lt;chr&gt; \"Manipur  Market Charter Boat \", \"Playa de la Luna Incorpor…\n$ shpcountry &lt;chr&gt; \"Coralmarica\", NA, \"-27\", NA, \"Oceanus\", \"Mawazam\", NA, \"Ri…\n$ rcvcountry &lt;chr&gt; NA, \"Azurionix\", NA, \"-27\", \"Oceanus\", NA, \"-27\", NA, \"-27\"…\n$ dataset    &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC…\n\n\n\n\nShow the codes\n# Extract the edges data.frame\nmac_edges &lt;- as_tibble(mac$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         valueofgoods_omu, generated_by, dataset)\n\n\n\n\nShow the codes\nglimpse(mac_edges)\n\n\nRows: 198\nColumns: 7\n$ source           &lt;chr&gt; \"Manipur  Market Charter Boat \", \"Chhattisgarh  A/S &…\n$ target           &lt;chr&gt; \"Playa de la Luna Incorporated\", \"Sea Breezes GmbH & …\n$ arrivaldate      &lt;chr&gt; \"2034-10-09\", \"2035-02-11\", \"2035-02-17\", \"2035-01-03…\n$ hscode           &lt;int&gt; 381190, 320417, 860900, 848180, 870899, 851629, 84149…\n$ valueofgoods_omu &lt;dbl&gt; 23808010, 242150, NA, NA, NA, NA, NA, NA, NA, 213170,…\n$ generated_by     &lt;chr&gt; \"mackerel\", \"mackerel\", \"mackerel\", \"mackerel\", \"mack…\n$ dataset          &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2…\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\nwrite_csv(x = mac_nodes, \"data/generated/mac_nodes.csv\")\nwrite_csv(x = mac_edges, \"data/generated/mac_edges.csv\")\n\n\n\n\nShow the codes\n### Data Wrangling \n\n## For mac_nodes ##\n# Replace all the NA values to \"\"\n# mac_nodes[is.na(mac_nodes)] &lt;- \"\"\n# \n# \n# ## For mac_edges ##  --&gt; ONLY 7 variables &lt;--\n# # Converting to date data type (yyyy-mm-dd format) and add another new column, \n# # Weekday, to indicate the day of week for visualisation later.\n# \n# mac_edges &lt;- mac_edges %&gt;%\n#   mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n#   mutate(Weekday = wday(arrivaldate,\n#                         label = TRUE,\n#                         abbr = FALSE))\n# \n# # Replace all the NA values to 0\n# mac_edges[is.na(mac_edges)] &lt;- 0.0\n\n\n\n\n3.1.2.8. pollock.json\n\n\nShow the codes\n# Load pollock.json into the environment\npollock &lt;- fromJSON(\"data/bundles/pollock.json\")\n\n\n\n\nShow the codes\n# Extract the nodes data.frame\npollock_nodes &lt;- as_tibble(pollock$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry, dataset)\n\n\n\n\nShow the codes\nglimpse(pollock_nodes)\n\n\nRows: 110\nColumns: 4\n$ id         &lt;chr&gt; \"Ocean Quest A/S Logistics\", \"Volga River LLC Enterprises\",…\n$ shpcountry &lt;chr&gt; \"Mawazam\", NA, NA, NA, NA, NA, NA, \"Mawazam\", NA, \"Marebak\"…\n$ rcvcountry &lt;chr&gt; NA, \"Azurionix\", \"Azurionix\", \"Azurionix\", \"Coralmarica\", \"…\n$ dataset    &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC…\n\n\n\n\nShow the codes\n# Extract the edges data.frame\npollock_edges &lt;- as_tibble(pollock$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         weightkg, valueofgoods_omu, volumeteu,\n         generated_by, dataset)\n\n\n\n\nShow the codes\nglimpse(pollock_edges)\n\n\nRows: 160\nColumns: 9\n$ source           &lt;chr&gt; \"Ocean Quest A/S Logistics\", \"Ocean Quest A/S Logisti…\n$ target           &lt;chr&gt; \"Volga River LLC Enterprises\", \"Volga River LLC Enter…\n$ arrivaldate      &lt;chr&gt; \"2034-09-19\", \"2034-08-14\", \"2034-09-19\", \"2034-08-14…\n$ hscode           &lt;int&gt; 160414, 160414, 160414, 160414, 160414, 160414, 16041…\n$ weightkg         &lt;int&gt; 34675, 77035, 34675, 77035, 40245, 41840, 77035, 6214…\n$ valueofgoods_omu &lt;dbl&gt; 132055, 354030, 132055, 354030, 204180, 230160, 35403…\n$ volumeteu        &lt;dbl&gt; 0, 5, 0, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0,…\n$ generated_by     &lt;chr&gt; \"pollock\", \"pollock\", \"pollock\", \"pollock\", \"pollock\"…\n$ dataset          &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2…\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\nwrite_csv(x = pollock_nodes, \"data/generated/pollock_nodes.csv\")\nwrite_csv(x = pollock_edges, \"data/generated/pollock_edges.csv\")\n\n\n\n\nShow the codes\n### Data Wrangling \n\n## For pollock_nodes ##\n# Replace all the NA values to \"\"\n# pollock_nodes[is.na(pollock_nodes)] &lt;- \"\"\n# \n# \n# ## For pollock_edges ##\n# # Converting to date data type (yyyy-mm-dd format) and add another new column, \n# # Weekday, to indicate the day of week for visualisation later.\n# \n# pollock_edges &lt;- pollock_edges %&gt;%\n#   mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n#   mutate(Weekday = wday(arrivaldate,\n#                         label = TRUE,\n#                         abbr = FALSE))\n# \n# # Replace all the NA values to 0\n# pollock_edges[is.na(pollock_edges)] &lt;- 0.0\n\n\n\n\n3.1.2.9. salmon_wgl.json\n\n\nShow the codes\n# Load salmon_wgl.json into the environment\nsal_wgl &lt;- fromJSON(\"data/bundles/salmon_wgl.json\")\n\n\n\n\nShow the codes\n# Extract the nodes data.frame\nsal_wgl_nodes &lt;- as_tibble(sal_wgl$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry, dataset)\n\n\n\n\nShow the codes\nglimpse(sal_wgl_nodes)\n\n\nRows: 120\nColumns: 4\n$ id         &lt;chr&gt; \"Harbah Motorboat AG Shipping\", \"Playa del Tesoro Ges.m.b.H…\n$ shpcountry &lt;chr&gt; \"Utoporiana\", NA, \"Icarnia\", NA, \"Kethilim\", NA, NA, \"Khams…\n$ rcvcountry &lt;chr&gt; NA, \"Oceanus\", NA, \"Azurionix\", NA, \"Azurionix\", \"Oceanus\",…\n$ dataset    &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC…\n\n\n\n\nShow the codes\n# Extract the edges data.frame\nsal_wgl_edges &lt;- as_tibble(sal_wgl$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         weightkg, valueofgoods_omu, volumeteu,\n         generated_by, dataset)\n\n\n\n\nShow the codes\nglimpse(sal_wgl_edges)\n\n\nRows: 216\nColumns: 9\n$ source           &lt;chr&gt; \"Harbah Motorboat AG Shipping\", \"Harbah Motorboat AG …\n$ target           &lt;chr&gt; \"Playa del Tesoro Ges.m.b.H.\", \"Playa del Tesoro Ges.…\n$ arrivaldate      &lt;chr&gt; \"2034-08-13\", \"2034-08-13\", \"2034-08-13\", \"2034-08-13…\n$ hscode           &lt;int&gt; 845490, 845490, 845490, 845490, 845490, 845490, 84549…\n$ weightkg         &lt;int&gt; 6805, 6805, 6805, 6805, 6805, 6805, 6805, 6805, 6805,…\n$ valueofgoods_omu &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ generated_by     &lt;chr&gt; \"salmon_wgl\", \"salmon_wgl\", \"salmon_wgl\", \"salmon_wgl…\n$ dataset          &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2…\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\nwrite_csv(x = sal_wgl_nodes, \"data/generated/sal_wgl_nodes.csv\")\nwrite_csv(x = sal_wgl_edges, \"data/generated/sal_wgl_edges.csv\")\n\n\n\n\nShow the codes\n### Data Wrangling \n\n## For sal_wgl_nodes ##\n# Replace all the NA values to \"\"\n# sal_wgl_nodes[is.na(sal_wgl_nodes)] &lt;- \"\"\n# \n# \n# ## For sal_wgl_edges ##\n# # Converting to date data type (yyyy-mm-dd format) and add another new column, \n# # Weekday, to indicate the day of week for visualisation later.\n# \n# sal_wgl_edges &lt;- sal_wgl_edges %&gt;%\n#   mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n#   mutate(Weekday = wday(arrivaldate,\n#                         label = TRUE,\n#                         abbr = FALSE))\n# \n# # Replace all the NA values to 0\n# sal_wgl_edges[is.na(sal_wgl_edges)] &lt;- 0.0\n\n\n\n\n3.1.2.10. salmon.json\n\n\nShow the codes\n# Load salmon.json into the environment\nsalmon &lt;- fromJSON(\"data/bundles/salmon.json\")\n\n\n\n\nShow the codes\n# Extract the nodes data.frame\nsalmon_nodes &lt;- as_tibble(salmon$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry, dataset)\n\n\n\n\nShow the codes\nglimpse(salmon_nodes)\n\n\nRows: 8\nColumns: 4\n$ id         &lt;chr&gt; \"Samaka Chart ОАО Delivery\", \"-2\", \"Kallianassa Ltd. Corpor…\n$ shpcountry &lt;chr&gt; \"Nalakond\", NA, \"Marebak\", NA, NA, NA, NA, NA\n$ rcvcountry &lt;chr&gt; NA, \"Azurionix\", NA, \"Azurionix\", \"Azurionix\", \"Azurionix\",…\n$ dataset    &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\"\n\n\n\n\nShow the codes\n# Extract the edges data.frame\nsalmon_edges &lt;- as_tibble(salmon$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         weightkg, valueofgoods_omu, volumeteu,\n         generated_by, dataset)\n\n\n\n\nShow the codes\nglimpse(salmon_edges)\n\n\nRows: 132\nColumns: 9\n$ source           &lt;chr&gt; \"Samaka Chart ОАО Delivery\", \"Samaka Chart ОАО Delive…\n$ target           &lt;chr&gt; \"-2\", \"-2\", \"-2\", \"-2\", \"-2\", \"-2\", \"-2\", \"-2\", \"-2\",…\n$ arrivaldate      &lt;chr&gt; \"2034-10-23\", \"2034-10-23\", \"2034-10-14\", \"2034-10-14…\n$ hscode           &lt;int&gt; 30619, 30619, 30619, 30619, 30619, 30619, 30619, 3061…\n$ weightkg         &lt;int&gt; 20980, 20980, 20980, 20980, 20980, 20980, 20985, 2098…\n$ valueofgoods_omu &lt;dbl&gt; 94150, 94150, 94170, 94170, 94170, 94170, 94175, 9415…\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ generated_by     &lt;chr&gt; \"salmon\", \"salmon\", \"salmon\", \"salmon\", \"salmon\", \"sa…\n$ dataset          &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2…\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\nwrite_csv(x = salmon_nodes, \"data/generated/salmon_nodes.csv\")\nwrite_csv(x = salmon_edges, \"data/generated/salmon_edges.csv\")\n\n\n\n\nShow the codes\n### Data Wrangling \n\n## For salmon_nodes ##\n# Replace all the NA values to \"\"\n# salmon_nodes[is.na(salmon_nodes)] &lt;- \"\"\n# \n# \n# ## For salmon_edges ##\n# # Converting to date data type (yyyy-mm-dd format) and add another new column, \n# # Weekday, to indicate the day of week for visualisation later.\n# \n# salmon_edges &lt;- salmon_edges %&gt;%\n#   mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n#   mutate(Weekday = wday(arrivaldate,\n#                         label = TRUE,\n#                         abbr = FALSE))\n# \n# # Replace all the NA values to 0\n# salmon_edges[is.na(salmon_edges)] &lt;- 0.0\n\n\n\n\n3.1.2.11. shark.json\n\n\nShow the codes\n# Load shark.json into the environment\nshark &lt;- fromJSON(\"data/bundles/shark.json\")\n\n\n\n\nShow the codes\n# Extract the nodes data.frame\nshark_nodes &lt;- as_tibble(shark$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry, dataset)\n\n\n\n\nShow the codes\nglimpse(shark_nodes)\n\n\nRows: 11\nColumns: 4\n$ id         &lt;chr&gt; \"Jammu Limited Liability Company\", \"Coral del Sur Ltd. Corp…\n$ shpcountry &lt;chr&gt; \"Utoporiana\", NA, \"Playa Solis\", NA, \"Vesperanda\", NA, NA, …\n$ rcvcountry &lt;chr&gt; NA, \"Oceanus\", NA, \"Oceanus\", NA, \"Azurionix\", \"Azurionix\",…\n$ dataset    &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC…\n\n\n\n\nShow the codes\n# Extract the edges data.frame\nshark_edges &lt;- as_tibble(shark$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         weightkg, valueofgoods_omu, volumeteu,\n         generated_by, dataset)\n\n\n\n\nShow the codes\nglimpse(shark_edges)\n\n\nRows: 256\nColumns: 9\n$ source           &lt;chr&gt; \"Jammu Limited Liability Company\", \"Jammu Limited Lia…\n$ target           &lt;chr&gt; \"Coral del Sur Ltd. Corporation Investment\", \"Coral d…\n$ arrivaldate      &lt;chr&gt; \"2034-07-15\", \"2034-04-16\", \"2034-05-19\", \"2034-09-23…\n$ hscode           &lt;int&gt; 920710, 920590, 920590, 920590, 920790, 920590, 92059…\n$ weightkg         &lt;int&gt; 2435, 2675, 2710, 3995, 7085, 3995, 1560, 2435, 2710,…\n$ valueofgoods_omu &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ generated_by     &lt;chr&gt; \"shark\", \"shark\", \"shark\", \"shark\", \"shark\", \"shark\",…\n$ dataset          &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2…\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\nwrite_csv(x = shark_nodes, \"data/generated/shark_nodes.csv\")\nwrite_csv(x = shark_edges, \"data/generated/shark_edges.csv\")\n\n\n\n\nShow the codes\n### Data Wrangling \n\n## For shark_nodes ##\n# Replace all the NA values to \"\"\n# shark_nodes[is.na(shark_nodes)] &lt;- \"\"\n# \n# \n# ## For carp_edges ##\n# # Converting to date data type (yyyy-mm-dd format) and add another new column, \n# # Weekday, to indicate the day of week for visualisation later.\n# \n# carp_edges &lt;- carp_edges %&gt;%\n#   mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n#   mutate(Weekday = wday(arrivaldate,\n#                         label = TRUE,\n#                         abbr = FALSE))\n# \n# # Replace all the NA values to 0\n# carp_edges[is.na(carp_edges)] &lt;- 0.0\n\n\n\n\n3.1.2.12. tuna.json\n\n\nShow the codes\n# Load tuna.json into the environment\ntuna &lt;- fromJSON(\"data/bundles/tuna.json\")\n\n\n\n\nShow the codes\n# Extract the nodes data.frame\ntuna_nodes &lt;- as_tibble(tuna$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry, dataset)\n\n\n\n\nShow the codes\nglimpse(tuna_nodes)\n\n\nRows: 29\nColumns: 4\n$ id         &lt;chr&gt; \"Andhra Pradesh  Sp Worldwide\", \"Jharkhand Sea  S.p.A. Unit…\n$ shpcountry &lt;chr&gt; \"Utoporiana\", NA, \"Rio Isla\", NA, \"-27\", NA, \"-27\", NA, NA,…\n$ rcvcountry &lt;chr&gt; NA, \"-27\", \"-27\", \"Azurionix\", NA, \"-27\", NA, \"Oceanus\", \"O…\n$ dataset    &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC…\n\n\n\n\nShow the codes\n# Extract the edges data.frame\ntuna_edges &lt;- as_tibble(tuna$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         valueofgoods_omu, generated_by, dataset)\n\n\n\n\nShow the codes\nglimpse(tuna_edges)\n\n\nRows: 314\nColumns: 7\n$ source           &lt;chr&gt; \"Andhra Pradesh  Sp Worldwide\", \"Andhra Pradesh  Sp W…\n$ target           &lt;chr&gt; \"Jharkhand Sea  S.p.A. United\", \"Jharkhand Sea  S.p.A…\n$ arrivaldate      &lt;chr&gt; \"2034-09-21\", \"2034-08-30\", \"2034-08-19\", \"2034-09-21…\n$ hscode           &lt;int&gt; 840999, 846820, 220300, 220300, 847989, 441600, 85044…\n$ valueofgoods_omu &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ generated_by     &lt;chr&gt; \"tuna\", \"tuna\", \"tuna\", \"tuna\", \"tuna\", \"tuna\", \"tuna…\n$ dataset          &lt;chr&gt; \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2\", \"MC2…\n\n\n\n\nShow the codes\n### Exporting to CSV for closer examination\nwrite_csv(x = tuna_nodes, \"data/generated/tuna_nodes.csv\")\nwrite_csv(x = tuna_edges, \"data/generated/tuna_edges.csv\")\n\n\n\n\nShow the codes\n### Data Wrangling \n\n## For tuna_nodes ##\n# Replace all the NA values to \"\"\n# tuna_nodes[is.na(tuna_nodes)] &lt;- \"\"\n# \n# \n# ## For tuna_edges ## --&gt; ONLY 7 variables &lt;--\n# # Converting to date data type (yyyy-mm-dd format) and add another new column, \n# # Weekday, to indicate the day of week for visualisation later.\n# \n# tuna_edges &lt;- tuna_edges %&gt;%\n#   mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n#   mutate(Weekday = wday(arrivaldate,\n#                         label = TRUE,\n#                         abbr = FALSE))\n# \n# # Replace all the NA values to 0\n# tuna_edges[is.na(tuna_edges)] &lt;- 0.0"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#loading-and-extracting-the-data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#loading-and-extracting-the-data",
    "title": "Take-Home Exercise 02",
    "section": "3.1. Loading and Extracting the Data",
    "text": "3.1. Loading and Extracting the Data\nBased on the VAST 2023 data notes, column dataset will always be ‘mc2’, to represent this set of data belongs to mini challenge 2. As such, we will not import this column into the R environment.\n\n3.1.1. Load main file mc2_challenge_graph.json\nWe will first load in the main file, mc2_challenge_graph.json, then extract the nodes and edges (links) information out.\n\nmc2 &lt;- fromJSON(\"data/mc2_challenge_graph.json\")\n\n\n3.1.1.1. Extracting the nodes data.frame from mc2\n\nmc2_nodes &lt;- as_tibble(mc2$nodes) %&gt;%\n  select(id, shpcountry, rcvcountry)\n\n\n\n3.1.1.2. Extracting the edges (links) data.frame from mc2\n\nmc2_edges &lt;- as_tibble(mc2$links) %&gt;%\n  select(source, target, arrivaldate, hscode,\n         weightkg, valueofgoods_omu, volumeteu,\n         valueofgoodsusd)\n\n\n\n3.1.1.3. Examining the structure of mc2_nodes and mc2_edges data.frames using glimpse() of dyplr.\n\nglimpse(mc2_nodes)\n\nRows: 34,576\nColumns: 3\n$ id         &lt;chr&gt; \"AquaDelight Inc and Son's\", \"BaringoAmerica Marine Ges.m.b…\n$ shpcountry &lt;chr&gt; \"Polarinda\", NA, \"Oceanus\", NA, \"Oceanus\", \"Kondanovia\", NA…\n$ rcvcountry &lt;chr&gt; \"Oceanus\", NA, \"Oceanus\", NA, \"Oceanus\", \"Utoporiana\", NA, …\n\nglimpse(mc2_edges)\n\nRows: 5,464,378\nColumns: 8\n$ source           &lt;chr&gt; \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son…\n$ target           &lt;chr&gt; \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica M…\n$ arrivaldate      &lt;chr&gt; \"2034-02-12\", \"2034-03-13\", \"2028-02-07\", \"2028-02-23…\n$ hscode           &lt;chr&gt; \"630630\", \"630630\", \"470710\", \"470710\", \"470710\", \"47…\n$ weightkg         &lt;int&gt; 4780, 6125, 10855, 11250, 11165, 11290, 9000, 19490, …\n$ valueofgoods_omu &lt;dbl&gt; 141015, 141015, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ volumeteu        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ valueofgoodsusd  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 87110, 188140, NA, 221110, 58…\n\n\n\n\n\n\n\n\nExamination of the data structure\n\n\n\n\nThere are a number of chr data type columns in both mc2_nodes and mc2_edges, as a good practice, we will trim away any possible leading and trailing white spaces of the data, before we perform any analysis.\nThe arrivaldate has the format of ‘YYYY-MM-DD’ and is treated as chr data type instead of date data type.\nA closer examination on the columns valueofgoods_omu, volumeteu, valueofgoodsusd columns revealed that there are also a large number of NA (missing values) and are deemed as as incomplete. We will drop these columns from analysis.\nWe will also need to filter out any possible duplicate records by using the distinct() function.\n\n\n\n\n\n3.1.1.4. Perform Data Wrangling for mc2_edges\n\n\n\n\n\n\nData Preparation for mc2_edges\n\n\n\n\nDrop columns valueofgoods_omu, volumeteu and valueofgoodsusd since they are deemed as as incomplete for analysis.\nConvert arrivaldate from chr data type to date data type by using ymd().\nExtract the year component out from arrivaldate field with year().\nPerform white space trimming for the chr columns,to remove any the leading and trailing white spaces with trimws().\nRemove any possible duplicate records by using distinct()\nBased on the data notes provided in the VAST Challenge, hscode refers to the Harmonized Commodity Description and Coding System Nomenclature (HS) developed by World Customs Organization (WCO). We will filter all the records based on the hscodes related to the context of the challenge. The relevant hscodes are prefixed with: 301, 302, 303, 304, 305, 306, 307, 308, 309, 1504, 1603, 1604, 1605 and 2301).\nAggregate the records, by deriving a weight column based on the number of records by grouping the source, target, hscode and year.\n\n\n\n\n# For the edges data frame, we will make use of select() to extract \n# and rearrange the columns.\nmc2_edges &lt;- mc2_edges %&gt;%\n  select(arrivaldate, source, target, hscode, weightkg) %&gt;%\n  mutate(arrivaldate = ymd(arrivaldate)) %&gt;%\n  mutate(year = year(arrivaldate)) %&gt;%\n  mutate(source = trimws(source)) %&gt;%\n  mutate(target = trimws(target)) %&gt;%\n  mutate(hscode = trimws(hscode)) %&gt;%\n  distinct()\n\n\n# List of relevant hscodes related to fishing\nrel_hscodes_3 &lt;- c(\"301\", \"302\", \"303\", \"304\", \"305\",\n                   \"306\", \"307\", \"308\", \"309\")\n\nrel_hscodes_4 &lt;- c(\"1504\", \"1603\", \"1604\", \"1605\", \"2301\")\n\n# Extract the records with the above 1st 3 or 4 characters of hscodes\n# using substr().\nmc2_edges_aggregated &lt;- mc2_edges %&gt;%\n  filter((substr(hscode, 1, 3) %in% rel_hscodes_3) |\n         (substr(hscode, 1, 4) %in% rel_hscodes_4)) %&gt;%\n  group_by(source, target, hscode, year) %&gt;%\n  summarise(weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  arrange(weight) %&gt;%\n  ungroup()\n\n\n\n3.1.1.5. Perform Data Wrangling for mc2_nodes\n\n\n\n\n\n\nData Preparation for mc2_nodes\n\n\n\nThe nodes within the node file must be unique and since we have filtered and cleaned the data in mc2_edges_aggregated, we will make use of this to extract the nodes that are used here.\nrbind() is used to combine the data in both source and target columns of mc2_edges_aggregated. We will then extract the unique records to form the nodes data frame.\n\n\n\nid1 &lt;- mc2_edges_aggregated %&gt;%\n  select(source) %&gt;%\n  rename(id = source)\nid2 &lt;- mc2_edges_aggregated %&gt;%\n  select(target) %&gt;%\n  rename(id = target)\n\n# create a new nodes data table derived from the source and target of edge data. This would ensure that only nodes with connections will be included.\n\nmc2_nodes_extracted &lt;- rbind(id1, id2) %&gt;%\n  distinct()\n\n\n# Build tbl_graph using the valid nodes and edges \nmc2_graph &lt;- tbl_graph(nodes = mc2_nodes_extracted,\n                       edges = mc2_edges_aggregated,\n                       directed = TRUE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#making-use-of-tbl_graph-to-build-tidygraph-data-model",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#making-use-of-tbl_graph-to-build-tidygraph-data-model",
    "title": "Take-Home Exercise 02",
    "section": "4.1. Making use of tbl_graph() to build tidygraph data model",
    "text": "4.1. Making use of tbl_graph() to build tidygraph data model\n\n# Build tbl_graph using the valid nodes and edges \n# mc2_graph &lt;- tbl_graph(nodes = mc2_nodes_extracted,\n#                        edges = mc2_edges_aggregated,\n#                        directed = TRUE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#visualising-the-overview-of-the-network-graph",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#visualising-the-overview-of-the-network-graph",
    "title": "Take-Home Exercise 02",
    "section": "4.2. Visualising the Overview of the Network Graph",
    "text": "4.2. Visualising the Overview of the Network Graph\nData Preparation for using visNetwork\n\n# Renaming the 'source' and 'target' columns to 'from' and 'to' respectively for visNetwork\n# mc2_edges_aggregated_vis &lt;- mc2_edges_aggregated %&gt;%\n#   rename(from = source) %&gt;%\n#   rename(to = target) %&gt;%\n#   filter(from!=to) %&gt;%\n#   ungroup()\n\nFrom the network graph below, each node represents an individual business entity (id), which you could select from the dropdown list, to examine the connectivity of this entity with other entities.\nFor the easy of searching of a particular entity, the dropdown list has been sorted in ascending order.\nIn addition, directional arrows have also been included on the edges, to help identify “high degree” nodes that are highly connected to other nodes in the network.\nHover action is also enabled for the graph so that the user can hover the mouse pointer over the graph to look at the different ‘groups’ of connectivity.\n\n# edges &lt;- mc2_edges_aggregated_vis %&gt;%\n#   filter(weight &gt; 50)\n# \n# nodes &lt;- mc2_nodes_extracted %&gt;%\n#   filter(id %in% c(\"id\", edges$from, edges$to)) %&gt;%\n#   mutate(centrality = centrality_authority()) %&gt;%\n#   arrange(id)\n# \n# \n# vis_nw &lt;- visNetwork(nodes, edges,\n#                      main = \"An Overview of the Network Graph\") %&gt;%\n#   visIgraphLayout(layout = \"layout_with_kk\") %&gt;%\n#   visLayout(randomSeed = 1234) %&gt;%\n#   visNodes(aes(size = centrality, colour = centrality)) %&gt;% \n#   visOptions(highlightNearest = list(enabled = T, degree = 2, hover = T),\n#              nodesIdSelection = TRUE) %&gt;%\n#   visEdges(arrows = \"to\", smooth = list(enabled = TRUE,\n#                          type = \"curvedCW\"))\n# \n# vis_nw\n\nObservations:\nFrom the above network graph, we noticed that some of the nodes in the middle cluster seemed to have a thicker border compared to the rest, indicating that these nodes have more connections to the other nodes. With the help of the hover function, we were able to identify the ids of these nodes.\nCAN WE MAKE USE OF THE DEGREE OF CENTRALITY OR IN-DEGREE TO COLLATE THESE NODES????\n\n4.2.1. Identifying the high in-degree connectivity nodes (based on the above network graph)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#computing-the-centrality-indices---using-centrality_betweenness",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#computing-the-centrality-indices---using-centrality_betweenness",
    "title": "Take-Home Exercise 02",
    "section": "4.3. Computing the Centrality Indices - using Centrality_Betweenness",
    "text": "4.3. Computing the Centrality Indices - using Centrality_Betweenness\n\n# g &lt;- mc2_graph %&gt;%\n#   mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n#   ggraph(layout = \"kk\") +\n#   geom_edge_link(aes(width=weight),\n#                  alpha=0.2) +\n#   scale_edge_width(range = c(0.1, 5)) +\n#   geom_node_point(aes(size=1, color=centrality_betweenness()))\n# g + theme_graph() + labs(title = \"Betweenness Centrality Graph\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#eigen-vector-centrality",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#eigen-vector-centrality",
    "title": "Take-Home Exercise 02",
    "section": "4.3. Eigen Vector Centrality",
    "text": "4.3. Eigen Vector Centrality\nEigen centrality is a measure to indicate the importance or influence of a node to the other nodes. It considers both the number of connections a node has and the importance of those connections. Nodes with high eigen centrality are not only connected to many other nodes but are also connected to nodes that have high eigen centrality.\n\nquantile_graph &lt;- quantile(eigen_centrality(mc2_graph)$vector,\n                           probs = seq(0, 1, 1/10))\n\nV(mc2_graph)$size = eigen_centrality(mc2_graph)$vector\n\nmc2_graph_aggregated &lt;- delete_vertices(mc2_graph, \n                                        V(mc2_graph)[size &lt; \n                                                       quantile_graph[10]])\n\nset.seed (1234)\n\nlayout1 &lt;- layout_with_kk(mc2_graph_aggregated)\n\n#identify top 10% of the new vertices\nquantile_graph_aggregated &lt;- quantile(V(mc2_graph_aggregated)$size, \n                                      probs = seq(0, 1, 1/10))\n\n#color yellow if vertices is top 10%\nV(mc2_graph_aggregated)$color &lt;- ifelse (V(mc2_graph_aggregated)$size &gt;\n                                           quantile_graph_aggregated[10], \n                                         \"darkgoldenrod3\", \"azure3\") \n\nE(mc2_graph_aggregated)$color &lt;- \"grey\"\n\n#Increase the size of nodes based on their centrality score, only those with high score will be visible\nV(mc2_graph_aggregated)$size &lt;- V(mc2_graph_aggregated)$size/0.065 \n\nplot(mc2_graph_aggregated, edge.arrow.size=0.25,edge.arrow.mode = \"-\", \n     main = \"The important/influencial Entities\")\n\n\n\n# Create the ggraph plot\n# g &lt;- ggraph(mc2_graph_aggregated, layout = layout1) +\n#   geom_edge_link(aes(width = weight), \n#                  alpha = 0.2, \n#                  arrow = arrow(length = unit(0.25, \"cm\"), \n#                                type = \"closed\")) +\n#   scale_edge_width(range = c(0.1, 5)) +\n#   geom_node_point(aes(size = V(mc2_graph_aggregated)$size, \n#                       color = V(mc2_graph_aggregated)$color)) +\n#   labs(title = \"Which Entity has the most influence\")\n\n# Print the plot\n# print(g)\n\nComputing the degree of centrality for each node\n\n# Calculate the degree of each node\nnode_degrees &lt;- degree(graph)\n\n# Add node degrees as node attributes\nmc2_graph2 &lt;- mc2_graph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(degree = node_degrees)\n\n# Plot the network graph with node degrees\nggraph(mc2_graph2, layout = \"kk\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = \"id\")) +\n  geom_node_label(aes(label = degree), nudge_y = 0.2) +\n  theme_graph()\n\n\n\n\n\nIn-degreeOut-degree\n\n\nComputing the in-degree measures for each node\n\n# Convert tbl_graph to igraph object\nmc2_igraph &lt;- igraph::as.igraph(mc2_graph)\n\n# Calculate the in-degree and out-degree of each node\nin_degree &lt;- degree(mc2_igraph, mode = \"in\")\nout_degree &lt;- degree(mc2_igraph, mode = \"out\")\n\n# Add in-degree and out-degree as node attributes\nV(mc2_igraph)$in_degree &lt;- in_degree\nV(mc2_igraph)$out_degree &lt;- out_degree\n\n# Convert back to tbl_graph\nmc2_graph_with_degrees &lt;- as_tbl_graph(mc2_igraph)\n\n# Plot the network graph with node degrees\nggraph(mc2_graph_with_degrees, layout = \"kk\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = \"\"), vjust = 2) +\n  geom_node_label(aes(label = in_degree), nudge_y = 0.2, color = \"blue\") \n\n\n\n\n\n\nComputing the in-degree measures for each node\n\n# Convert tbl_graph to igraph object\nmc2_igraph &lt;- igraph::as.igraph(mc2_graph)\n\n# Calculate the in-degree and out-degree of each node\nin_degree &lt;- degree(mc2_igraph, mode = \"in\")\nout_degree &lt;- degree(mc2_igraph, mode = \"out\")\n\n# Add in-degree and out-degree as node attributes\nV(mc2_igraph)$in_degree &lt;- in_degree\nV(mc2_igraph)$out_degree &lt;- out_degree\n\n# Convert back to tbl_graph\nmc2_graph_with_degrees &lt;- as_tbl_graph(mc2_igraph)\n\n# Plot the network graph with node degrees\nggraph(mc2_graph_with_degrees, layout = \"kk\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = \"\"), vjust = 2) +\n  geom_node_label(aes(label = out_degree), nudge_y = 0.2, color = \"red\") \n\n\n\n\n\n\n\n\nggraph(mc2_graph, layout = \"kk\") +\n  geom_edge_link(aes(colour = factor(year), size = 2)) +\n  geom_node_point(aes()) +\n  theme_graph()\n\n\n\n\n\nggraph(mc2_graph, layout = \"kk\") +\n  geom_edge_link(aes(colour = factor(year))) +\n  geom_node_point(aes()) +\n  facet_edges(~year) +\n  theme_graph()\n\n\n\n\n\n### Exporting to CSV for closer examination\n# write_csv(x = final_MC2_nodes_noNA, \"data/generated/final_MC2_nodes_noNA.csv\")\n# write_csv(x = MC2_edges_aggregated, \"data/generated/mc2_edges_agg.csv\")\n# write_csv(x = valid_edges, \"data/generated/valid_edges.csv\")\n\n\n### Exporting to CSV for closer examination\n# write_csv(x = mc2_nodes, \"data/generated/mc2_nodes.csv\")\n# write_csv(x = mc2_edges, \"data/generated/mc2_edges.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#eigenvector-centrality",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#eigenvector-centrality",
    "title": "Take-Home Exercise 02",
    "section": "4.6. Eigenvector Centrality",
    "text": "4.6. Eigenvector Centrality\nEigenvector centrality is a measure to indicate the importance or influence of a node to the other nodes. It considers both the number of connections a node has and the importance of those connections. Nodes with high eigen centrality are not only connected to many other nodes but are also connected to nodes that have high eigen centrality.\n\n4.6.1. Data Preparation\n\nmc2_edges_aggregated_eigen &lt;- mc2_edges_aggregated %&gt;%\n  filter(weight &gt;80) %&gt;%\n  ungroup()\n\nid1_eigen &lt;- mc2_edges_aggregated_eigen %&gt;%\n  select(source) %&gt;%\n  rename(id = source)\nid2_eigen &lt;- mc2_edges_aggregated_eigen %&gt;%\n  select(target) %&gt;%\n  rename(id = target)\n\n# create a new nodes data table derived from the source and target of edge data. This would ensure that only nodes with connections will be included.\n\nmc2_nodes_extracted_eigen &lt;- rbind(id1_eigen, id2_eigen) %&gt;%\n  distinct()\n\nmc2_graph_eigen &lt;- tbl_graph(nodes = mc2_nodes_extracted_eigen,\n                       edges = mc2_edges_aggregated_eigen,\n                       directed = TRUE)\n\n\n\n4.6.2. Plotting the Eigenvector Network Graph\n\nquantile_graph &lt;- quantile(eigen_centrality(mc2_graph_eigen)$vector,\n                           probs = seq(0, 1, 1/10))\n\nV(mc2_graph_eigen)$size = eigen_centrality(mc2_graph_eigen)$vector\n\nmc2_graph_aggregated_eigen &lt;- delete_vertices(mc2_graph_eigen,\n                                        V(mc2_graph_eigen)[size &lt;                                                        quantile_graph[10]])\n\nset.seed (1234)\n\nlayout1 &lt;- layout_with_kk(mc2_graph_aggregated_eigen)\n\n#identify top 10% of the new vertices\nquantile_graph_aggregated &lt;- quantile(V(mc2_graph_aggregated_eigen)$size,\n                                      probs = seq(0, 1, 1/10))\n\n# color yellow if vertices is top 10%\nV(mc2_graph_aggregated_eigen)$color &lt;- ifelse (V(mc2_graph_aggregated_eigen)$size &gt;\n                                           quantile_graph_aggregated[10],\n                                         \"darkgoldenrod3\", \"azure3\")\n\n# Greying out the edges so that the yellow nodes will stand out in the graph\nE(mc2_graph_aggregated_eigen)$color &lt;- \"grey\"\n\n#Increase the size of nodes based on their centrality score, only those with high score will be visible\nV(mc2_graph_aggregated_eigen)$size &lt;- V(mc2_graph_aggregated_eigen)$size/0.065\n\nplot(mc2_graph_aggregated_eigen, edge.arrow.size=0.25,edge.arrow.mode = \"-\",\n     main = \"Most Important/Influential Business Entities in the Network\",\n     width=30, height=12)\n\n\n\n\n\n  vertex_attr(mc2_graph_aggregated_eigen,\n              index = V(mc2_graph_aggregated_eigen)$size*0.065 &gt;\n                quantile_graph_aggregated[10])\n\n$id\n[1] \"Sea Breezes S.A. de C.V. Freight\"                     \n[2] \"nián yú Ltd. Corporation\"                             \n[3] \"Caracola del Sol Services\"                            \n[4] \"Niger Bend   Limited Liability Company Marine ecology\"\n\n$size\n[1] 13.95522 15.38462 10.87785 13.93994\n\n$color\n[1] \"darkgoldenrod3\" \"darkgoldenrod3\" \"darkgoldenrod3\" \"darkgoldenrod3\"\n\n\nObservations:\nBased on the EigenVector Centrality, there are a total of 4 business entities with its eigenvalue above the 90th percentile. These business entities are considered the most important/influential business entities in the network.\n\n\n4.7 Visualising Community\n\n# Data Preparation\n# mc2_edges_aggregated_comm &lt;- mc2_edges_aggregated %&gt;%\n#   filter(weight &gt;700) %&gt;%\n#   ungroup()\n# \n# id1_comm &lt;- mc2_edges_aggregated_comm %&gt;%\n#   select(source) %&gt;%\n#   rename(id = source)\n# id2_comm &lt;- mc2_edges_aggregated_comm %&gt;%\n#   select(target) %&gt;%\n#   rename(id = target)\n\n# create a new nodes data table derived from the source and target of edge data. This would ensure that only nodes with connections will be included.\n\n# mc2_nodes_extracted_comm &lt;- rbind(id1_comm, id2_comm) %&gt;%\n#   distinct()\n# \n# mc2_graph_comm &lt;- tbl_graph(nodes = mc2_nodes_extracted_comm,\n#                        edges = mc2_edges_aggregated_comm,\n#                        directed = TRUE)\n# \n\n\n# Create tbl_graph object for the community analysis\n# g &lt;- mc2_graph_comm %&gt;%\n#   mutate(community = as.factor(group_optimal(weights = weight))) %&gt;%\n#   ggraph(layout = \"fr\") +\n#   geom_edge_link(aes(width=weight),\n#                  alpha=0.2) +\n#   scale_edge_width(range = c(0.1, 5)) +\n#   geom_node_point(aes(colour = community))\n# \n# g + theme_graph()\n\n\n### Exporting to CSV for closer examination\n# write_csv(x = final_MC2_nodes_noNA, \"data/generated/final_MC2_nodes_noNA.csv\")\n# write_csv(x = MC2_edges_aggregated, \"data/generated/mc2_edges_agg.csv\")\n# write_csv(x = valid_edges, \"data/generated/valid_edges.csv\")\n\n\n### Exporting to CSV for closer examination\n# write_csv(x = mc2_nodes, \"data/generated/mc2_nodes.csv\")\n# write_csv(x = mc2_edges, \"data/generated/mc2_edges.csv\")\n\n\n# mc2_graph &lt;- read_rds(\"data/generated/mc2_graph.rds\")\n# mc2_nodes &lt;- read_rds(\"data/generated/mc2_nodes.rds\")\n# mc2_edges &lt;- read_rds(\"data/generated/mc2_edges.rds\")\n\n\n# write_rds(mc2_nodes, \"data/generated/mc2_nodes.rds\")\n# write_rds(mc2_edges, \"data/generated/mc2_edges.rds\")\n# write_rds(mc2_nodes, \"data/generated/mc2_nodes_extracted.rds\")\n# write_rds(mc2_edges, \"data/generated/mc2_edges_aggregated.rds\")\n# write_rds(mc2_graph, \"data/generated/mc2_graph.rds\")\n\n\n# write_csv(x = nodes_df, \"data/generated/nodes_df.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#computing-the-centrality-indices---using-centrality_closeness",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#computing-the-centrality-indices---using-centrality_closeness",
    "title": "Take-Home Exercise 02",
    "section": "4.4. Computing the Centrality Indices - using centrality_closeness",
    "text": "4.4. Computing the Centrality Indices - using centrality_closeness\n\n# g &lt;- mc2_graph %&gt;%\n#   mutate(closeness_centrality = centrality_closeness()) %&gt;%\n#   ggraph(layout = \"kk\") + \n#   geom_edge_link(aes(width=weight), alpha=0.1) +\n#   scale_edge_width(range = c(0.1, 5)) +\n#   geom_node_point(aes(size=0.5, color=centrality_closeness()))\n# g + theme_graph() + labs(title = \"Closeness Centrality between the Nodes\")\n\n\n### Exporting to CSV for closer examination\n# write_csv(x = final_MC2_nodes_noNA, \"data/generated/final_MC2_nodes_noNA.csv\")\n# write_csv(x = MC2_edges_aggregated, \"data/generated/mc2_edges_agg.csv\")\n# write_csv(x = valid_edges, \"data/generated/valid_edges.csv\")\n\n\n### Exporting to CSV for closer examination\n# write_csv(x = mc2_nodes, \"data/generated/mc2_nodes.csv\")\n# write_csv(x = mc2_edges, \"data/generated/mc2_edges.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#computing-the-degree-of-centrality",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#computing-the-degree-of-centrality",
    "title": "Take-Home Exercise 02",
    "section": "4.4 Computing the Degree of Centrality",
    "text": "4.4 Computing the Degree of Centrality\nThe degree centrality of a node is calculated by counting the number of direct connections (edges) that a node has with other nodes. Nodes with a higher degree centrality have more connections, indicating that they are more central or influential within the network.\nFor this exercise, we will identify nodes will high degree of centrality by extracting those nodes with degree &gt; 20. These are the nodes that are potentially the fishery warehouses or distribution.\n\n# Calculate the degree of each node\n# node_degrees &lt;- degree(mc2_graph)\n\n# Add node degrees as node attributes\n# mc2_graph_deg &lt;- mc2_graph %&gt;%\n#   activate(nodes) %&gt;%\n#   mutate(degree = node_degrees) #%&gt;% filter(degree &gt; 25)\n\n\n# Plot the network graph with node degrees\n# ggraph(mc2_graph_deg, layout = \"kk\") +\n#   geom_edge_link() +\n#   geom_node_point() +\n#   geom_node_text(aes(label = \"\")) +\n#   geom_node_label(aes(label = degree), nudge_x = 0.1, nudge_y = 0.1, repel = TRUE) +\n#   theme_graph() +\n#   labs(title = \"Degree of Centrality Graph\")\n\n\n### Exporting to CSV for closer examination\n# write_csv(x = final_MC2_nodes_noNA, \"data/generated/final_MC2_nodes_noNA.csv\")\n# write_csv(x = MC2_edges_aggregated, \"data/generated/mc2_edges_agg.csv\")\n# write_csv(x = valid_edges, \"data/generated/valid_edges.csv\")\n\n\n### Exporting to CSV for closer examination\n# write_csv(x = mc2_nodes, \"data/generated/mc2_nodes.csv\")\n# write_csv(x = mc2_edges, \"data/generated/mc2_edges.csv\")\n\n\n# mc2_graph &lt;- read_rds(\"data/generated/mc2_graph.rds\")\n# mc2_nodes &lt;- read_rds(\"data/generated/mc2_nodes.rds\")\n# mc2_edges &lt;- read_rds(\"data/generated/mc2_edges.rds\")\n\n\n# write_rds(mc2_nodes, \"data/generated/mc2_nodes.rds\")\n# write_rds(mc2_edges, \"data/generated/mc2_edges.rds\")\n# write_rds(mc2_nodes, \"data/generated/mc2_nodes_extracted.rds\")\n# write_rds(mc2_edges, \"data/generated/mc2_edges_aggregated.rds\")\n# write_rds(mc2_graph, \"data/generated/mc2_graph.rds\")\n\n\n# write_csv(x = nodes_df, \"data/generated/nodes_df.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#betweenness-centrality",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#betweenness-centrality",
    "title": "Take-Home Exercise 02",
    "section": "4.3.Betweenness Centrality",
    "text": "4.3.Betweenness Centrality\nBetweenness centrality is a measure of the importance or influence of a node in a network based on its ability to connect other nodes. Nodes with high betweenness centrality act as bridges or intermediaries, connecting different parts of the network and have the potential to control the flow of information or influence within the network.\nIn the context of this exercise, nodes with high betweenness centrality might potentially be impact the network.\n\nDistribution of Betweeness Centrality IndexCode Chunk\n\n\n\n\n\n\n\n\n\n\n# Get the count of the records in each bin\nnodes_df_bet_gp &lt;- nodes_df %&gt;%\n  group_by(bet_bin) %&gt;%\n  summarise(cnt = n())\n\n# Create a distribution with bar chart using ggplot2\nplot_bet &lt;- ggplot(nodes_df_bet_gp, aes(x = bet_bin, y = cnt)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue3\") +\n  geom_text(aes(label = cnt), vjust = -0.5, color = \"black\") +\n  labs(x = \"\\nBetweeenness Centrality\", y = \"Count\\n\", \n       title = \"Distribution of Betweenness Centrality of the Nodes\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) \n\nplot_bet\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations from the Distribution of Betweeness Centrality Index\n\n\n\nThe betweenness centrality distribution revealed that a large number of nodes has a low value and might potentially be the fishing vessels or small clusters without much influence to the entire network.\nHowever, it is noted that there are 11 nodes with very high betweenness centrality index (&gt;=500K), and we should pay attention to them. The datatable below shows the list of business entities and their respective betweenness centrality index.\n\n\n\n# Extract the Top 11 nodes with extremely high betweenness centrality index\n\nnodes_df_bet_top11 &lt;- nodes_df %&gt;%\n  filter(betweenness_score &gt; 500000) %&gt;%\n  select(id, label, betweenness_score, bet_bin, cent_bin, \n         deg_bin, in_bin, out_bin, close_bin) %&gt;%  \n  arrange(desc(betweenness_score))\n\ndatatable(head(nodes_df_bet_top11, 11), \n          colnames = c(\"ID\", \"Entity\", \"Betweenness Centrality Index\", \n                       \"Betweenness Centrality Category\", \"Centrality Category\",\n                       \"Degree Centrality Category\", \"In-Degree Centrality Category\",\n                       \"Out-Degree Centrality Cateogy\", \"Closeness Index Category\"),\n          options = list(\n            columnDefs = list(list(className = 'dt-center', targets = 1:5),\n                    list(targets = 5, visible = FALSE))))\n\n\n\n\n\n\n\n# get the company names for the edges\nedges_df_bet &lt;- edges_df %&gt;%\n  filter(from %in% nodes_df_bet_top11$id |\n         to %in% nodes_df_bet_top11$id) %&gt;%\n  filter(weight &gt; 10)\n\n# build the nodes table\nnodes_bet &lt;- nodes_df %&gt;%\n  filter(id %in% c(edges_df_bet$from, edges_df_bet$to)) %&gt;%\n  rename(group = bet_bin)\n\nvisNetwork(nodes_bet,\n           edges_df_bet,\n           main = \"Top 11 Entities with High Betweenness Centrality Index\") %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLayout(randomSeed = 1234) %&gt;%\n  visOptions(highlightNearest = list(enabled = T, degree = 2, hover = T),\n             nodesIdSelection = TRUE) %&gt;%\n  visNodes(id = nodes_bet$id, size=10) %&gt;%\n  visLegend() %&gt;%  \n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\"))\n\n\n\n\n\n::: callout-note title=“Observations” ???TBA??? Now that we have the top 11 nodes with high betweenness centrality index, we will proceed to find its neighbouring nodes to build up the network graph. :::"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#looking-at-the-degree-of-centrality",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#looking-at-the-degree-of-centrality",
    "title": "Take-Home Exercise 02",
    "section": "4.3. Looking at the Degree of Centrality",
    "text": "4.3. Looking at the Degree of Centrality\nDegree centrality is a measure of the importance or popularity of a node in a network based on the number of direct connections it has with other nodes in the network.\n\nDistribution of Degree Centrality IndexCode Chunk\n\n\n\n\n\n\n\n\n\n\n# Plotting the Distribution for Degree Centrality Index\nnodes_df_deg_gp &lt;- nodes_df %&gt;%\n  group_by(deg_bin) %&gt;%\n  summarise(cnt = n())\n\n# Create a bar chart with hover text using ggplot2\nplot &lt;- ggplot(nodes_df_deg_gp, aes(x = deg_bin, y = cnt)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue3\") +\n  geom_text(aes(label = cnt), vjust = -0.5, color = \"black\") +\n  labs(x = \"\\nDegree Centrality\", y = \"Count\\n\", \n       title = \"Distribution of Degree Centrality of the Nodes\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) \n\nplot\n\n\n\n\n\n\n\n\n\n\nObservations from the Distribution of Degree Centrality Index\n\n\n\nThe distribution chart above revealed that more than 99% of the nodes have low degree centrality index. There are 18 nodes with high degree centrality index of &gt;= 300. This means that these nodes have significantly high number of direct connections to other nodes within the network.\n\n\n\n4.3.1. Comparing the Different Centrality Indices\n\n# Extract the nodes with high degree centrality index\n\nnodes_df_deg &lt;- nodes_df %&gt;%\n  filter(degree_score &gt;= 300) %&gt;%\n  select(id, label, degree_score, deg_bin, eigen_score, eigen_bin,\n         in_degree, in_bin, out_degree, out_bin,\n         betweenness_score, bet_bin, closeness_score, close_bin) %&gt;%  \n  arrange(desc(degree_score))\n\ndatatable(nodes_df_deg, \n          class=\"compact\",\n          caption = \"\\nTable 2: Business Entities with High Degree Centrality Index\\n\",\n          colnames = c(\"ID\", \"Entity\", \n                       \"Degree Centrality Index\",\n                       \"Degree Centrality Category\", \n                       \"Eigenvector Centrality Index\", \n                       \"Eigenvector Centrality Category\", \n                       \"In-Degree Centrality Index\",\n                       \"In-Degree Centrality Category\",\n                       \"Out-Degree Centrality Index\",\n                       \"Out-Degree Centrality Cateogy\",\n                       \"Betweenness Centrality Index\",\n                       \"Betweenness Centrality Category\",\n                       \"Closeness Centrality Index\",\n                       \"Closeness Centrality Category\"),\n          options = list(\n            columnDefs = list(list(className = 'dt-center', \n                                   targets = 14))))\n\n\n\n\n\n\n\n\n4.3.2. Degree Centrality Network Graph\n\n# get the entity names for the edges\nedges_df_deg &lt;- edges_df %&gt;%\n  filter(from %in% nodes_df_deg$id |\n         to %in% nodes_df_deg$id) %&gt;%\n  filter(weight &gt; 30)\n\n# build the nodes table\nnodes_deg &lt;- nodes_df %&gt;%\n  filter(id %in% c(edges_df_deg$from, edges_df_deg$to)) %&gt;%\n  rename(group = deg_bin) %&gt;%\n  arrange(label)\n\nvisNetwork(nodes_deg,\n           edges_df_deg,\n           main = \"Entities with High Degree Centrality Index\") %&gt;%\n  visIgraphLayout(layout = \"layout_with_kk\") %&gt;%\n  visLayout(randomSeed = 1234) %&gt;%\n  visOptions(highlightNearest = list(enabled = T, degree = 2, hover = T),\n             nodesIdSelection = TRUE) %&gt;%\n  visNodes(id = nodes_deg$id, size=10) %&gt;%\n  visLegend() %&gt;%  \n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE))\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nBased on the information presented in the DataTable and Network Graph, we noticed that these nodes generally have a high number of outgoing connections (&gt;300 each), and very low number of incoming connections (highest at 24 incoming connections). This suggest these nodes are likely to be the bigger fishing vessels or distributors."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#looking-at-the-in-degree-centrality",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#looking-at-the-in-degree-centrality",
    "title": "Take-Home Exercise 02",
    "section": "4.4. Looking at the In-Degree Centrality",
    "text": "4.4. Looking at the In-Degree Centrality\nIn-degree centrality is a measure of centrality in a network that focuses on incoming connections to a node. Nodes with high in-degree centrality have a larger number of connections pointing towards them, indicating that they receive a lot of resources from other nodes.\n\nDistribution for In-Degree Centrality IndexCode Chunk\n\n\n\n\n\n\n\n\n\n\n# Plotting the Distribution for In-Degree centrality Index\nnodes_df_in &lt;- nodes_df %&gt;%\n  group_by(in_bin) %&gt;%\n  summarise(cnt = n())\n\n# Create a bar chart with hover text using ggplot2\nplot_in &lt;- ggplot(nodes_df_in, aes(x = in_bin, y = cnt)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue3\") +\n  geom_text(aes(label = cnt), vjust = -0.5, color = \"black\") +\n  labs(x = \"\\nIn-Degree Centrality\", y = \"Count\\n\", \n       title = \"Distribution of In-Degree Centrality of the Nodes\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) \n\nplot_in\n\n\n\n\n\n\n\n\n\n\nObservations from the Distribution of In-Degree Centrality Index\n\n\n\nIt is observed that there are 27 nodes with significantly high in-degree centrality index (&gt;=600), which are likely to be the main resource collection points.\n\n\n\n4.4.1. Comparing the Different Centrality Indices\n\n# Extracting the top 10 entities with highest in-degree centrality\nnodes_df_in_top &lt;- nodes_df %&gt;%\n  filter(in_degree &gt;= 600) %&gt;%\n  select(id, label, in_degree, in_bin, eigen_score, eigen_bin,\n         degree_score, deg_bin, betweenness_score, bet_bin, \n         closeness_score, close_bin, out_degree, out_bin) %&gt;%\n  arrange(desc(in_degree))\n\ntop_10_in &lt;- head(nodes_df_in_top,10)\n  \ndatatable(top_10_in, \n          class=\"compact\",\n          caption = \"\\nTable 3: Top 10 Business Entities with In-Degree Centrality Index\\n\",\n          colnames = c(\"ID\", \"Entity\", \"In-Degree Centrality Index\",\n                       \"In-Degree Centrality Category\",\n                       \"Eigenvector Centrality Index\", \n                       \"Eigenvector Centrality Category\", \n                       \"Degree Centrality Index\",\n                       \"Degree Centrality Category\", \n                       \"Betweenness Centrality Index\",\n                       \"Betweenness Centrality Category\",\n                       \"Closeness Centrality Index\",\n                       \"Closeness Centrality Category\",\n                       \"Out-Degree Centrality Index\",\n                       \"Out-Degree Centrality Cateogy\"),\n          options = list(\n            columnDefs = list(list(className = 'dt-center', \n                                   targets = 14))))\n\n\n\n\n\n\n\n\n4.4.2. In-Degree Centrality Network Graph\n\n# get the entity names for the edges\nedges_df_in &lt;- edges_df %&gt;%\n  filter(from %in% top_10_in$id |\n         to %in% top_10_in$id) %&gt;%\n  filter(weight &gt; 80)\n\n# build the nodes table\nnodes_in &lt;- nodes_df %&gt;%\n  filter(id %in% c(edges_df_in$from, edges_df_in$to)) %&gt;%\n  rename(group = in_bin) %&gt;%\n  arrange(label)\n\nvisNetwork(nodes_in,\n           edges_df_in,\n           main = \"Entities with High In-Degree Centrality Index\") %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLayout(randomSeed = 1234) %&gt;%\n  visOptions(highlightNearest = list(enabled = T, degree = 2, hover = T),\n             nodesIdSelection = TRUE) %&gt;%\n  visNodes(id = nodes_in$id, size=10) %&gt;%\n  visLegend() %&gt;%  \n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\"))\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nHigh in-degree centrality index suggests that these nodes are likely to be the fishery wharves/collection centres for the fishing vessels. It is noted that some of these nodes also have high eigenvector centrality index, indicating tha they are one of the key nodes within the network.\nMost of these nodes have low values for degree centrality, out-degree centrality and closeness centrality indices."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#looking-at-the-out-degree-centrality",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#looking-at-the-out-degree-centrality",
    "title": "Take-Home Exercise 02",
    "section": "4.5. Looking at the Out-Degree Centrality",
    "text": "4.5. Looking at the Out-Degree Centrality\nOut-degree centrality is a measure of centrality in a network that focuses on outgoing connections from a node. Nodes with high out-degree centrality have a larger number of connections pointing away from them, indicating that they have a greater reach and influence over other nodes.\n\nDistribution for Out-Degree Centrality IndexCode Chunk\n\n\n\n\n\n\n\n\n\n\n# Plotting the Distribution for Out-Degree Centrality Index\nnodes_df_out &lt;- nodes_df %&gt;%\n  group_by(out_bin) %&gt;%\n  summarise(cnt = n())\n\n# Create a bar chart with hover text using ggplot2\nplot_in &lt;- ggplot(nodes_df_out, aes(x = out_bin, y = cnt)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue3\") +\n  geom_text(aes(label = cnt), vjust = -0.5, color = \"black\") +\n  labs(x = \"\\nOut-Degree Centrality\", y = \"Count\\n\", \n       title = \"Distribution of Out-Degree Centrality of the Nodes\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) \n\nplot_in\n\n\n\n\n\n\n\n\n\n\nObservations from the Distribution of Out-Degree Centrality Index\n\n\n\nMajority of the entities in the network have low number of out-degree centrality index, which suggest that they are likely to be the fishing vessels within the network, contributing to one of the major fishery wharves/collection centres.\nIt is observed that there are 18 nodes with significantly high out-degree centrality index (&gt;300), which we will study them in more details.\n\n\n\n4.5.1. Comparing the Different Centrality Indices\n\n# Extracting the top companies with highest out-degree centrality\nnodes_df_out &lt;- nodes_df %&gt;%\n  filter(out_degree &gt; 300) %&gt;%\n  select(id, label, out_degree, out_bin, eigen_score, eigen_bin, \n         degree_score, deg_bin, betweenness_score, bet_bin, \n         closeness_score, close_bin, in_degree, in_bin) %&gt;%\n  arrange(desc(out_degree))\n\n\ndatatable(nodes_df_out, \n          colnames = c(\"ID\", \"Entity\", \"Out-Degree Centrality Index\",\n                       \"Out-Degree Centrality Cateogy\",\n                       \"Eigenvector Centrality Index\", \n                       \"Eigenvector Centrality Category\", \n                       \"Degree Centrality Index\",\n                       \"Degree Centrality Category\", \n                       \"Betweenness Centrality Index\",\n                       \"Betweenness Centrality Category\",\n                       \"Closeness Centrality Index\",\n                       \"Closeness Centrality Category\",\n                       \"In-Degree Centrality Index\",\n                       \"In-Degree Centrality Category\"),\n          options = list(\n            columnDefs = list(list(className = 'dt-center', \n                                   targets = 14))))\n\n\n\n\n\n\n\n\n4.5.2. Out-Degree Centrality Network Graph\n\n# get the company names for the edges\nedges_df_out &lt;- edges_df %&gt;%\n  filter(from %in% nodes_df_out$id |\n         to %in% nodes_df_out$id) %&gt;%\n  filter(weight &gt; 30)\n\n# build the nodes table\nnodes_out &lt;- nodes_df %&gt;%\n  filter(id %in% c(edges_df_out$from, edges_df_out$to)) %&gt;%\n  rename(group = out_bin) %&gt;%\n  arrange(label)\n\nvisNetwork(nodes_out,\n           edges_df_out,\n           main = \"Entities with High Out-Degree Centrality Index\") %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLayout(randomSeed = 1234) %&gt;%\n  visOptions(highlightNearest = list(enabled = T, degree = 2, hover = T),\n             nodesIdSelection = TRUE) %&gt;%\n  visNodes(id = nodes_out$id, size=10) %&gt;%\n  visLegend() %&gt;%  \n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\"))\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe entity with the highest out-degree centrality index, The Salty Dog Limited Liability Company, provides resources to other nodes with low value of out-degree centrality index."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#identifying-the-clusters",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#identifying-the-clusters",
    "title": "Take-Home Exercise 02",
    "section": "4.6. Identifying the Clusters",
    "text": "4.6. Identifying the Clusters"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#overview-of-the-network",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#overview-of-the-network",
    "title": "Take-Home Exercise 02",
    "section": "4.1. Overview of the Network",
    "text": "4.1. Overview of the Network\nFrom the network graph below, each node represents an individual business entity (label), which you could select from the dropdown list, to examine the connectivity of this entity with other entities.\nDesign Considerations\n\nIn order not to over clutter the network graph, all edges with weight &lt;= 50 will be filtered out and not included.\nFor the ease of selecting a particular entity, a dropdown list with all the entities (nodes) present in the graph is provided. The list is also sorted in ascending order by using arrange().\nIn addition, directional arrows have also been included on the edges, to identify the in-flow and out-flow edges.\nMouse pointer hover action is also included on the graph so that the user can hover the mouse pointer over the graph to look at the possible different ‘groups’ of connectivity.\n\n\n# Preparing the data for visualisation\nedges &lt;- edges_df %&gt;%\n  filter(weight &gt; 50)\n\nnodes &lt;- nodes_df %&gt;%\n  filter(id %in% c(edges$from, edges$to)) %&gt;%\n  arrange(label)\n\n# Building the network graph using visNetwork package\nvis_nw &lt;- visNetwork(nodes, edges,\n                     main = \"An Overview of the Network Graph\") %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLayout(randomSeed = 1234) %&gt;%\n  visNodes() %&gt;%\n  visOptions(highlightNearest = list(enabled = T, degree = 2, hover = T),\n             nodesIdSelection = TRUE) %&gt;%\n  visEdges(arrows = \"to\", smooth = list(enabled = TRUE, type = \"curvedCW\"))\n\nvis_nw\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom the overview network graph, we noticed that some of the nodes in the middle seemed to be denser compared to the rest, indicating that these nodes probably have more connections to the other nodes. We were able to identify the ids of these nodes when we zoom into the graph. We can also look at individual node’s connectivity to other nodes by clicking on it or hover it with the mouse pointer. However, this requires the user to maneuver around the graph with great efforts.\nWe will make use of the different centrality indices computed earlier to uncover more insights in the subsequent sections."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#the-high-centrality-index-nodes",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#the-high-centrality-index-nodes",
    "title": "Take-Home Exercise 02",
    "section": "4.2. The High Centrality Index Nodes",
    "text": "4.2. The High Centrality Index Nodes\nWe will attempt to identify the nodes with high centrality index, that would most likely be the main hubs within the network.\nFirstly, we will take a look at the distribution of the centrality index across all the nodes to gain an overview picture of the network.\nIn view that there are some extrememly low count figure, the count for each bin is displayed at the top of each bar for ease of reference.\n\nDistribution of Centrality IndexCode Chunk\n\n\n\n\n\n\n\n\n\n\n# Plotting the Distribution for Centrality \n\n# Get the count of the records in each bin\nnodes_df_cent_gp &lt;- nodes_df %&gt;%\n  group_by(cent_bin) %&gt;%\n  summarise(cnt = n())\n\n# Create a distribution with bar chart using ggplot2\nplot_cent &lt;- ggplot(nodes_df_cent_gp, aes(x = cent_bin, y = cnt)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue3\") +\n  geom_text(aes(label = cnt), vjust = -0.5, color = \"black\") +\n  labs(x = \"\\nCentrality Index\", y = \"Count\\n\", \n       title = \"Distribution of Centrality Index of the Nodes\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) \n\nplot_cent\n\n\n\n\n\n\n\n\n\n\nObservations from the Distribution of Centrality Index\n\n\n\nThe graph above revealed that there are 3 nodes that have a centrality index &gt; 0.5. This means that these nodes are well-connected to other nodes in the network. These nodes have very high number of in-degree centrality index, indicating that they are most likely to be the fishery wharves/central collection centers.\nWith that information, we will study these 3 entities in greater details.\n\n\n\n# Extracting the top 3 entities with highest centrality indices\nnodes_df_cent_top3 &lt;- nodes_df %&gt;%\n  filter(centrality &gt;= 0.5) %&gt;%\n  select(id, label, centrality, cent_bin, deg_bin, \n         in_bin, out_bin, bet_bin, close_bin) %&gt;%\n  arrange(desc(centrality))\n\ntop_3_cent &lt;- head(nodes_df_cent_top3, 3)\n\ndatatable(head(top_3_cent, 3), \n          colnames = c(\"ID\", \"Entity\", \"Centrality Index\", \"Centrality Category\",\n                       \"Degree Centrality Category\", \"In-Degree Centrality Category\",\n                       \"Out-Degree Centrality Cateogy\", \"Betweenness Index Category\",\n                       \"Closeness Index Category\"),\n          options = list(\n            columnDefs = list(list(className = 'dt-center', targets = 1:5),\n                    list(targets = 5, visible = FALSE))))\n\n\n\n\n\n\n\n# get the company names for the edges\nedges_df_cent &lt;- edges_df %&gt;%\n  filter(from %in% top_3_cent$id |\n         to %in% top_3_cent$id) %&gt;%\n  filter(weight &gt; 10)\n\n# build the nodes table\nnodes_cent &lt;- nodes_df %&gt;%\n  filter(id %in% c(edges_df_cent$from, edges_df_cent$to)) %&gt;%\n  rename(group = cent_bin)\n\nvisNetwork(nodes_cent,\n           edges_df_cent,\n           main = \"Network Graph of the Top 3 Entities with High Centrality\") %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLayout(randomSeed = 1234) %&gt;%\n  visOptions(highlightNearest = list(enabled = T, degree = 2, hover = T),\n             nodesIdSelection = TRUE) %&gt;%\n  visNodes(id = nodes_cent$id, size=10) %&gt;%\n visLegend() %&gt;%  \n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\"))\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nBased on the DataTable and network graph, these 3 entities are likely to be the primary fishery wharves/collection centers within the network, as they have high in-degree centrality indices. However,the low out-degree centrality index, betweenness centrality index and closeness centrality index suggested that these nodes have limited impact/reach within the network."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#looking-at-the-closeness-centrality",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#looking-at-the-closeness-centrality",
    "title": "Take-Home Exercise 02",
    "section": "4.3. Looking at the Closeness Centrality",
    "text": "4.3. Looking at the Closeness Centrality\nCloseness centrality is a measure of how quickly or easily a node can reach other nodes in a network. Nodes with high closeness centrality are able to reach other nodes in the network more quickly, indicating that they have a shorter average distance to other nodes. They are considered as central or influential because they can efficiently transmit information, resources, or influence to other nodes.\n\nDistribution of Closeness Centrality IndexCode Chunk\n\n\n\n\n\n\n\n\n\n\n# Plotting the Distribution for closeness centrality \nnodes_df_close_gp &lt;- nodes_df %&gt;%\n  group_by(close_bin) %&gt;%\n  summarise(cnt = n())\n\n\n# Create a bar chart with hover text using ggplot2\nplot_close &lt;- ggplot(nodes_df_close_gp, aes(x = close_bin, y = cnt)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue3\") +\n  geom_text(aes(label = cnt), vjust = -0.5, color = \"black\") +\n  labs(x = \"\\nCloseness Centrality\", y = \"Count\\n\", \n       title = \"Distribution of Closeness Centrality of the Nodes\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) \n\nplot_close\n\n\n\n\n\n\n\n\n\n\nObservations from Distribution for closeness centrality\n\n\n\nInterestingly, there are 4,828 nodes without any closeness centrality score (NA). These nodes are likely to be disconnected or isolated from the rest of the network.\nCould these nodes potentially be the illegal fishing vessels? No connection to any of the wharves nor distribution centres.\n\n\n\n4.3.1. Look at those with high closeness centrality index (== 1.0)\n\n# Extract the nodes with closeness centrality index == 1.0\n\nnodes_df_high_close &lt;- nodes_df %&gt;%\n  filter(closeness_score == 1.0) %&gt;%\n  select(id, label, closeness_score, close_bin, bet_bin, cent_bin, \n         deg_bin, in_bin, out_bin) %&gt;%  \n  arrange(desc(closeness_score))\n\n#datatable(nodes_df_high_close, class= \"compact\")\n\ndatatable(nodes_df_high_close, \n          colnames = c(\"ID\", \"Entity\", \"Closeness Centrality Index\", \n                       \"Closeness Centrality Index Category\",\n                       \"Betweenness Centrality Category\", \"Centrality Category\",\n                       \"Degree Centrality Category\", \"In-Degree Centrality Category\",\n                       \"Out-Degree Centrality Cateogy\"),\n          options = list(\n            columnDefs = list(list(className = 'dt-center', targets = 1:5),\n                    list(targets = 5, visible = TRUE))))\n\n\n\n\n\n\n\n# get the company names for the edges\nedges_df_high_close &lt;- edges_df %&gt;%\n  filter(from %in% nodes_df_high_close$id |\n         to %in% nodes_df_high_close$id) %&gt;%\n  filter(weight &gt; 10)\n\n# build the nodes table\nnodes_high_close &lt;- nodes_df %&gt;%\n  filter(id %in% c(edges_df_high_close$from, edges_df_high_close$to)) %&gt;%\n  rename(group = close_bin)\n\nvisNetwork(nodes_high_close,\n           edges_df_high_close,\n           main = \"Entities with High Closeness Centrality Index\") %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLayout(randomSeed = 1234) %&gt;%\n  visOptions(highlightNearest = list(enabled = T, degree = 2, hover = T),\n             nodesIdSelection = TRUE) %&gt;%\n  visNodes(id = nodes_high_close$id, size=10) %&gt;%\n  visLegend() %&gt;%  \n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\"))\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n???TBA??? \n\n\n\n\n4.3.2. Look at those with closeness centrality index == NA\n\n# Extract the nodes with closeness centrality index == NA\n\nnodes_df_na_close &lt;- nodes_df %&gt;%\n  filter(is.na(closeness_score)) %&gt;%\n  select(id, label, closeness_score, close_bin, \n         betweenness_score, bet_bin, centrality, \n         in_degree, in_bin, out_degree, out_bin) %&gt;%  \n  arrange(desc(closeness_score))\n\ndatatable(nodes_df_na_close, class= \"compact\")\n\n\n\n\n\n\n\n# get the company names for the edges\nedges_df_na_close &lt;- edges_df %&gt;%\n  filter(from %in% nodes_df_na_close$id |\n         to %in% nodes_df_na_close$id) %&gt;%  filter(weight &gt; 20)\n\n# build the nodes table\nnodes_na_close &lt;- nodes_df %&gt;%\n  filter(id %in% c(edges_df_na_close$from, edges_df_na_close$to)) %&gt;%\n  rename(group = close_bin)\n\nvisNetwork(nodes_na_close,\n           edges_df_na_close,\n           main = \"Entities with Closeness Centrality Index == NA\") %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLayout(randomSeed = 1234) %&gt;%\n  visOptions(highlightNearest = list(enabled = T, degree = 2, hover = T),\n             nodesIdSelection = TRUE) %&gt;%\n  visNodes(id = nodes_na_close$id, size=10) %&gt;%\n  visLegend() %&gt;%  \n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\"))\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n???TBA???"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#the-important-nodes-of-the-network---eigenvector-centrality",
    "href": "Take-home_Ex/Take-home_Ex02/Take-Home_Ex02.html#the-important-nodes-of-the-network---eigenvector-centrality",
    "title": "Take-Home Exercise 02",
    "section": "4.2. The Important Nodes of the Network - Eigenvector Centrality",
    "text": "4.2. The Important Nodes of the Network - Eigenvector Centrality\nEigenvector centrality index captures the overall importance of a node based on its connections and the importance of those connections in the network. Nodes with high eigenvector centrality are connected to other nodes that are also highly central or influential.\nWe will attempt to identify the main hub nodes within the network by looking at the nodes with high eigenvector centrality index.\nFirstly, we will take a look at the distribution of this centrality index across all the nodes to gain an overview picture of the network.\nDesign Considerations\nIn view of the big differentiation in the count for each bin, the count for each bin is displayed at the top of each bar for easy reference.\n\nDistribution of Eigenvector Centrality IndexCode Chunk\n\n\n\n\n\n\n\n\n\n\n# Plotting the Distribution for Eigenvector Centrality Index\n\n# Get the count of the records in each bin\nnodes_df_eigen_gp &lt;- nodes_df %&gt;%\n  group_by(eigen_bin) %&gt;%\n  summarise(cnt = n())\n\n# Create a distribution with bar chart using ggplot2\nplot_eigen &lt;- ggplot(nodes_df_eigen_gp, aes(x = eigen_bin, y = cnt)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue3\") +\n  geom_text(aes(label = cnt), vjust = -0.5, color = \"black\") +\n  labs(x = \"\\nEigenvector Centrality Index\", y = \"Count\\n\", \n       title = \"Distribution of Eigenvector Centrality Index\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) \n\nplot_eigen\n\n\n\n\n\n\n\n\n\n\nObservations from the Distribution of Eigenvector Centrality Index\n\n\n\nThe graph above revealed that there are 4 nodes with eigenvector centrality index of &gt; 0.5. This means that these nodes are connected to other important nodes in the network.\nA quick examination of the eigenvector centrality index of the nodes revealed that 11,657 out of 11,688 nodes (&gt;99%) have an eigenvector centrality index of &lt;0.1.\nFor the purpose of visual analysis, we will focus on the 4 nodes with high eigenvector centrality index.\n\n\n\n4.2.1. Comparing the Different Centrality Indices\n\n# Extracting the top 4 entities with highest centrality indices\nnodes_df_eigen_top4 &lt;- nodes_df %&gt;%\n  filter(eigen_score &gt;= 0.5) %&gt;%\n  select(id, label, eigen_score, eigen_bin, degree_score, deg_bin, \n         in_degree, in_bin, out_degree, out_bin, \n         betweenness_score, bet_bin, closeness_score, close_bin) %&gt;%\n  arrange(desc(eigen_score))\n\ndatatable(nodes_df_eigen_top4, \n          class=\"compact\",\n          caption = \"\\nTable 1: Business Entities with High Eigenvector Centrality Index\\n\",\n          colnames = c(\"ID\", \"Entity\", \"Eigenvector Centrality Index\", \n                       \"Eigenvector Centrality Category\", \n                       \"Degree Centrality Index\",\n                       \"Degree Centrality Category\", \n                       \"In-Degree Centrality Index\",\n                       \"In-Degree Centrality Category\",\n                       \"Out-Degree Centrality Index\",\n                       \"Out-Degree Centrality Cateogy\",\n                       \"Betweenness Centrality Index\",\n                       \"Betweenness Centrality Category\",\n                       \"Closeness Centrality Index\",\n                       \"Closeness Centrality Category\"),\n          options = list(\n            columnDefs = list(list(className = 'dt-center', \n                                   targets = 14))))\n\n\n\n\n\n\n\n\n4.2.2. Eigenvector Centrality Network Graph\n\n# get the company names for the edges\nedges_df_eigen &lt;- edges_df %&gt;%\n  filter(from %in% nodes_df_eigen_top4$id |\n         to %in% nodes_df_eigen_top4$id) %&gt;%\n  filter(weight &gt; 50)\n\n# build the nodes table\nnodes_eigen &lt;- nodes_df %&gt;%\n  filter(id %in% c(edges_df_eigen$from, edges_df_eigen$to)) %&gt;%\n  rename(group = eigen_bin) %&gt;%\n  arrange(label)\n\nvisNetwork(nodes_eigen,\n           edges_df_eigen,\n           main = \"Network Graph of the Top 4 Entities with High Eigenvector Centrality\") %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLayout(randomSeed = 1234) %&gt;%\n  visOptions(highlightNearest = list(enabled = T, degree = 2, hover = T),\n             nodesIdSelection = TRUE) %&gt;%\n  visNodes(id = nodes_eigen$id, size=10) %&gt;%\n visLegend() %&gt;%  \n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\"))\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nBased on the information presented in the DataTable and the visualisation presented in the Network Graph above, we observed that these 4 nodes also have very high number of in-degree centrality index and very low number of out-degree centrality index, suggesting that they are likely to be the fishery wharves/central collection centers within the network."
  }
]